[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"first set 13 labs roughly tracks “Thinking Data”1 “Answering questions data”;2 second set labs roughly tracks “Experimental Design Analysis Psychology.”3Although primary aim create lab exercises reinforce stats concepts also train basic R coding skills data-analysis, many side goals, including showing students advantages using R markdown Github creating communicating research products. example, aside tutorials, developing R package called vertical,4 highlights advantages learning R researchers psychology. , possible, hope inject broader discussion awesome R tools use labs (time, deep-dive requires separate course…maybe coming soon browser near ).","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Right now ’s Dec 4th, 2020; Anjali Krishnan & co-teaching stats course graduate students Master’s Experimental Psychology program. semester almost , next semester moving stats II. first time tried coordinate efforts graduate level, ’s fun.years ago, put heads together create free undergraduate textbook (Answering questions data) lab manual, covered exercises across four software environments, R, Excel, SPSS, Jamovi (Anjali’s verion ). resources exist Github repos, licensed CC 4.0, can copied, remixed, re-used license.Anjali teaching stats sequence graduate level several years, program decided expand statistics offering include additional lab hours. , year, ’m joining club writing lab curriculum every week course. now, decided gather labs written far put “fancy” new bs4_book() bookdown format.Anjali using two statistics textbooks lecture portion course. first semester mostly follows “Thinking Data,”5 second semester mostly follows “Experimental Design Analysis Psychology.”6 Coincidentally, trained John Vokey & Scott Allen University Lethbridge (worked textbook undergrad), Anjali trained Hérve Abdi UT Austin. , seems academically inherited impulse create statistics curriculum.","code":""},{"path":"preface.html","id":"for-students","chapter":"Preface","heading":"0.1 For Students","text":"student class questions ’s going class, please ask email .student interested using materials, class, don’t know begin, hope little overview helpful. think enough breadcrumbs work way course material .creating new chapters almost week Fall 2020 Spring 2021. chapter series lab exercises corresponding closely chapters textbooks use stats II.7 labs use R, RStudio, Github complete assignments. See instructions next Getting Started section tips installing necessary software.lab similar structure, written chapter corresponding lecture video followed generalization assignment. Lab content split conceptual sections (reinforcing statistical concept R), practical sections (showing specific tasks analyses R). start basic R coding, discuss new coding concepts needed. generalization problems end lab designed solved R code, challenge students independently apply coding concepts lab solving similar new assignment problems. time, assigned problems walkthrough videos showing example solutions case get stuck.follow materials, gain experience using R Rstudio, well writing powerful amazing R Markdown language (e.g., allowed make web-book), also become familiar using Github sharing research assets like statistical analyses produce R. Also, hope find materials useful enriching understanding statistics. Best luck!","code":""},{"path":"preface.html","id":"for-instructors","chapter":"Preface","heading":"0.2 For Instructors","text":"materials currently development, also released CC -SA 4.0 license. result, materials free remix re-use license.Feel free fork repo use materials see fit. like contribute development materials, feel free submit pull-request suggested changes (leave comment github issues).","code":""},{"path":"preface.html","id":"cc-by-sa-4.0-license","chapter":"Preface","heading":"0.3 CC BY-SA 4.0 license","text":"license means free :Share: copy redistribute material medium formatAdapt: remix, transform, build upon material purpose, even commercially.licensor revoke freedoms long follow license terms.following terms:Attribution: must give appropriate credit, provide link license, indicate changes made. may reasonable manner, way suggests licensor endorses use.ShareAlike: remix, transform, build upon material, must distribute contributions license original.additional restrictions: may apply legal terms technological measures legally restrict others anything license permits.","code":""},{"path":"preface.html","id":"copying-the-textbook","chapter":"Preface","heading":"0.4 Copying the textbook","text":"textbook written R-Studio, using R Markdown, compiled web-book format using bookdown package. general, thank larger R community amazing tools made.source code compiling book available GitHub repository book:https://github.com/CrumpLab/rstatsforpsychIn principle, anybody fork otherwise download repository. Load .Rproj file R-studio compile entire book. individual .rmd files chapter edited content style better suit needs.want contribute version textbook, make pull requests GitHub, discuss issues make requests issues tab.Note self come back quick video rundown process.","code":""},{"path":"preface.html","id":"citation","chapter":"Preface","heading":"0.5 Citation","text":"Note, date reflects latest compilation book. DOI minted 12/10/2020.Crump, M. J. C. (2022, April 26). Reproducible statistics psychologists R: Lab Tutorials. https://doi.org/10.17605/OSF.IO/KBHGA","code":""},{"path":"r-rstudio-github.html","id":"r-rstudio-github","chapter":"R, RStudio, & Github","heading":"R, RStudio, & Github","text":"meet first class things try home get started.","code":""},{"path":"r-rstudio-github.html","id":"install-r-and-r-studio","chapter":"R, RStudio, & Github","heading":"0.6 Install R and R Studio","text":"Download install R computer. R website https://www.r-project.orgDownload install R-studio. must download install R first installing R-studio. R-studio website https://www.rstudio.com","code":""},{"path":"r-rstudio-github.html","id":"github.com-and-github-desktop","chapter":"R, RStudio, & Github","heading":"0.7 Github.com and Github Desktop","text":"Create free github account. github website https://github.comDownload install github desktop https://desktop.github.com","code":""},{"path":"r-rstudio-github.html","id":"test-the-pipeline","chapter":"R, RStudio, & Github","heading":"0.8 Test the pipeline","text":"semester students submitting work github repositories. Follow steps test github pipeline make sure working:Create new R project (initialize git)Create new R Markdown documentPublish R project folder Github.comMake commits demonstrate local changes reflected github.com","code":""},{"path":"r-rstudio-github.html","id":"why-are-we-using-r-for-this-statistics-lab","chapter":"R, RStudio, & Github","heading":"0.9 Why are we using R for this statistics lab?","text":"quick attempt explain think totally worth learn R data-analysis, psychologists general.","code":""},{"path":"r-rstudio-github.html","id":"rstudio-run-through","chapter":"R, RStudio, & Github","heading":"0.10 RStudio run through","text":"look features RStudio.","code":""},{"path":"basic-r-programming.html","id":"basic-r-programming","chapter":"Basic R programming","heading":"Basic R programming","text":"take number different approaches using R learn statistics semester. One approaches learn basic coding/scripting R, can become entire course . video gives impression coding looks feels like R Studio, introduces basic coding concepts variables, logic, loops, functions.","code":""},{"path":"practice-problems.html","id":"practice-problems","chapter":"Practice problems","heading":"Practice problems","text":"“8/27/2020 | Last Compiled: 2022-04-26”chapter supplement students looking exercises work coding skills outside class. labs course designed develop practical data analysis skills R, conceptual knowledge statistics using R way interact statistical phenomena. assume students may new coding. Don’t worry, student course, gradually introduce coding concepts throughout course.Learning code takes time effort, can intensely frustrating beginning. learning programming first time undergraduate, advisor (John Vokey) showed computer desk told teach program…pointed large hole drywall underneath desk said, “kick wall”.Learning teach coding also hard. ’ve trying years. However, stumble across methods seem promising, /immensely helpful , like advertise . example years ago came across completely different approach learning programming website Project Euler, highly recommend. website presents series problems, usually mathematical ones, like sum first 1000 prime numbers? challenge use programming language find correct answer. submit correct answers, unlock forum post working code (many different languages).idea Project Euler learn basic programming skills trying solve concrete problems. might know syntax make computer accomplish particular goal , particular goal mind, use search figure make programming language something.problems inspired Project Euler. past classes sometimes assigned problems little guidance anything R, class sat around puzzling things couple weeks. time, listing problems, along tips videos, eventually example code problems. Although problems involve statistics, . , intended concrete enough know question asking accomplish.think worthwhile try solve problems . time, can really helpful example solutions get stuck. P.S. favorite problem snakes ladders simulation, think can learn solve problem R, well way able solve sorts data-analysis problems, give problem try solve , congrats, think ’s pretty impressive .","code":""},{"path":"practice-problems.html","id":"programming-challenges-i","chapter":"Practice problems","heading":"0.11 Programming Challenges I","text":"purpose problems, try solve ? important overarching goal learning code become justifiably confident ability write scripts solve problems. end day applying skills new problems without textbook answers, ability solve problems rests learning write new code works. abstract problem, instead requires practice writing code solve new problems. following problem sets designed primarily aim mind. Solving problems simultaneously develop ability write scripts solve new problems, well give hands exposure learning syntax R language. problems written specifically R language, solving problems another language useful strategy learning syntax another language.problems roughly ordered terms difficulty, easier problems first harder problems second. problems can solve combining foundational programming concepts already discussed. , can solved declaring variables, using logic statements, loops create algorithms solve problem. problems require writing functions, formal general way writing algorithms. Many problems can solved quickly efficiently writing lines code, using intrinsic functions already supplied R programming language. problems might consider writing different solutions explore different syntax options.","code":""},{"path":"practice-problems.html","id":"easier-problems","chapter":"Practice problems","heading":"0.12 Easier Problems","text":"simple math numbers, addition, subtraction, multiplication, divisionPut numbers variables, simple math variablesWrite code place numbers 1 100 separately variable using loop. , using seq function.Find sum integer numbers 1 100.can use sum() function vector numbersHow without using sum function? example, use loop accomplish task?Write function find sum integers two values.List odd numbers 1 100.use seq() functionHow without using seq() function? Consider using mod function %%, evaluates whether remainder dividing one number another.List prime numbers 1 1000.Generate 100 random numberscheck runif functionto look help file run ?runif console. general ?function_name show help file function R.Generate 100 random numbers within specific rangerunif can thisWrite functions give descriptive statistics vector variable storing multiple numbers. Write functions following without using R intrinsics: mean, mode, median, range, standard deviationIt’s ok use sum() length()creative see can find multiple solutions. example two ways compute mean.Count number characters string variableuse strsplit() split character vectorCount number words string variableuse strsplitCount number sentences string variableconsider splitting . characterCount number times specific character occurs string variabletable() function can help count individual occurencesHow without table function?logical test see one word found within text another string variable.example given word hello, can run test see contained test sentence?consider using %%Put current computer time milliseconds variableMeasure long piece code takes run measuring time code run, code run, taking difference find total timeMeasure long piece code takes run measuring time code run, code run, taking difference find total timeRead .txt file .csv file variableRead .txt file .csv file variablescan() general purpose text input functionread.csv() read .csv filesOutput contents variable .txt filewrite.csv()Create variable stores 20x20 matrix random numbershere’s make matrix full 0sOutput matrix txt file using commas tabs separate column values, new lines separate row valueswrite.csv()","code":"\n# syntax for writing a function\n\nfunction_name <- function(input_name){\n  #body where you modify input\n  return(name_of_output)\n}\n\n# running the function\nfunction_name(some_input)\n# four divided by two gives no remainder\n# the mod function shows 0\n4%%2\n#> [1] 0\n\n# 5 divided by two gives a remainder\n# the mod function shows 1\n5%%2\n#> [1] 1\n# using sum and length\nmean_A <- function(x){\n  return(sum(x)/length(x))\n}\n\nsome_numbers <- c(1,2,3,4,5)\nmean_A(some_numbers)\n#> [1] 3\n\n# no intrinsics\nmean_B <- function(x){\n  counter <- 0\n  total_sum <-0\n  for(i in x){\n    total_sum <- total_sum+i\n    counter<-counter+1\n  }\n  return(total_sum/counter)\n}\n\nmean_B(some_numbers)\n#> [1] 3\na <- \"adskfjhkadsjfh\"\nstrsplit(a,split=\"\")\n#> [[1]]\n#>  [1] \"a\" \"d\" \"s\" \"k\" \"f\" \"j\" \"h\" \"k\" \"a\" \"d\" \"s\" \"j\" \"f\" \"h\"\n\n# note that strsplit returns its result in a list\nb <-strsplit(a,split=\"\")\nb[[1]] # access all elements in list 1\n#>  [1] \"a\" \"d\" \"s\" \"k\" \"f\" \"j\" \"h\" \"k\" \"a\" \"d\" \"s\" \"j\" \"f\" \"h\"\nb[[1]][1] # access first element of list 1\n#> [1] \"a\"\n\n# lists can be unlisted\nd <- unlist(strsplit(a,split=\"\"))\nd  # all elements in character vector\n#>  [1] \"a\" \"d\" \"s\" \"k\" \"f\" \"j\" \"h\" \"k\" \"a\" \"d\" \"s\" \"j\" \"f\" \"h\"\nd[1] #first element\n#> [1] \"a\"\na <- \"this is a sentence\"\nstrsplit(a,split=\" \") # use a space as the splitting character\n#> [[1]]\n#> [1] \"this\"     \"is\"       \"a\"        \"sentence\"\na <- c(1,3,2,3,2,3,2,3,4,5,4,3,4,3,4,5,6,7)\ntable(a)\n#> a\n#> 1 2 3 4 5 6 7 \n#> 1 3 6 4 2 1 1\ntest_word <- \"hello\"\ntest_sentence <-\"is the word hello in this sentence\"\na <- c(1,2,3,4,5)\nb <- 5\nd <- 8\n\n# question is b in a?\nb%in%a\n#> [1] TRUE\n\n# is d in a?\nd%in%a\n#> [1] FALSE\nprint(as.numeric(Sys.time())*1000, digits=15)\n#> [1] 1650801583875.96\na <- matrix(0, ncol=20,nrow=20)"},{"path":"practice-problems.html","id":"harder-problems","chapter":"Practice problems","heading":"0.13 Harder Problems","text":"","code":""},{"path":"practice-problems.html","id":"fizzbuzz","chapter":"Practice problems","heading":"0.13.1 FizzBuzz","text":"List numbers 1 100 following constraints. number can divided three evenly, print Fizz instead number. number can divided five evenly, print Buzz instead number. Finally, number can divided three five evenly, print FizzBuzz instead number. answer look something like :1, 2, Fizz, 4, Buzz, Fizz, 7, 8, Fizz, Buzz, 11, Fizz, 13, 14, FizzBuzz, 16, 17, Fizz, 19, Buzz, Fizz, 22, 23, Fizz, Buzz, 26, Fizz, 28, 29, FizzBuzz, 31, 32, Fizz, 34, Buzz, Fizz, 37, 38, Fizz, Buzz, 41, Fizz, 43, 44, FizzBuzz, 46, 47, Fizz, 49, Buzz, Fizz, 52, 53, Fizz, Buzz, 56, Fizz, 58, 59, FizzBuzz, 61, 62, Fizz, 64, Buzz, Fizz, 67, 68, Fizz, Buzz, 71, Fizz, 73, 74, FizzBuzz, 76, 77, Fizz, 79, Buzz, Fizz, 82, 83, Fizz, Buzz, 86, Fizz, 88, 89, FizzBuzz, 91, 92, Fizz, 94, Buzz, Fizz, 97, 98, Fizz, BuzzHere bits might useful","code":"\n# a number mod three will return 0 if it divides evenly\n6%%3\n#> [1] 0\n# a number mod five will return 0 if it divides evenly\n10%%5\n#> [1] 0\n\n# examples of replacing elements of a vector\na<-c(1,2,3,4,5)\na[3]<-\"Fizz\"\na\n#> [1] \"1\"    \"2\"    \"Fizz\" \"4\"    \"5\"\n\n# notice that a starts as a numeric vector\n# but changes to an all character vector after \"Fizz\" is added"},{"path":"practice-problems.html","id":"frequency-counts","chapter":"Practice problems","heading":"0.13.2 Frequency Counts","text":"Take text input, able produce table shows counts character text. problem related earlier easy problem asking count number times single letter appears text. slightly harder problem general version: count frequencies unique characters text.’s easy way thisCan without using table? Attempt problem using data.frame. tips","code":"\na<-\"some text that has some letters\"\ntable(unlist(strsplit(a,split=\"\")))\n#> \n#>   a e h l m o r s t x \n#> 5 2 5 2 1 2 2 1 4 6 1\n# data.frame is data format that produces named columns of data\n\n# creates two vectors\nnumbers <-c(1,2,3,4,5)\nletters <-c(\"a\",\"b\",\"c\",\"d\",\"e\")\n\n# make a data.frame from two vectors\nnew_df <- data.frame(numbers,letters)\nprint(new_df)\n#>   numbers letters\n#> 1       1       a\n#> 2       2       b\n#> 3       3       c\n#> 4       4       d\n#> 5       5       e\n\n# access individual columns of dataframe\nnew_df$numbers\n#> [1] 1 2 3 4 5\nnew_df$letters\n#> [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n# get names of data.frame\nnames(new_df)\n#> [1] \"numbers\" \"letters\"\n\n# break the problem into steps\n# first part of problem is to identify all unique character in the string\na<-c(1,2,3,4,5,2,2,3,2,3)\nunique(a)\n#> [1] 1 2 3 4 5\nb<-\"a string with some letters\"\nunique(unlist(strsplit(b,split=\"\")))\n#>  [1] \"a\" \" \" \"s\" \"t\" \"r\" \"i\" \"n\" \"g\" \"w\" \"h\" \"o\" \"m\" \"e\" \"l\"\n\n# second part is to go through each of the unique letters in the list of unique letters, and for each count the number of times they appear in the original text\n# store the results in a data.frame with two columns, one with the letter names, and another with the counts"},{"path":"practice-problems.html","id":"test-the-random-number-generator","chapter":"Practice problems","heading":"0.13.3 Test the Random Number Generator","text":"Test random number generator flat distribution. Generate million random numbers 0 100. Count number 0s, 1s, 2s, 3s, etc. way 100. Look counts numbers determine relatively equal. example, plot counts Excel make histogram. bars close flat, number equal chance selected, random number generator working without bias.","code":"\na<-runif(100,0,100)\nhist(a)"},{"path":"practice-problems.html","id":"create-a-multiplication-table","chapter":"Practice problems","heading":"0.13.4 Create a multiplication table","text":"Generate matrix multiplication table. example, labels columns numbers 1 10, labels rows numbers 1 10. contents cells matrix correct answer multiplying column value row value.","code":"\n# you can multiply all numbers in a vector in one go\na<-c(1,2,3,4,5,6,7,8,9,10)\na*2\n#>  [1]  2  4  6  8 10 12 14 16 18 20\n\n# you can nest loops\nfor(i in 1:3){\n  for(j in 1:3){\n    print(i*j)\n  }\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 2\n#> [1] 4\n#> [1] 6\n#> [1] 3\n#> [1] 6\n#> [1] 9"},{"path":"practice-problems.html","id":"encrypt-and-decrypt-the-alphabet","chapter":"Practice problems","heading":"0.13.5 Encrypt and Decrypt the Alphabet","text":"Turn normal english text encrypted version text, able turn decrypted text back normal english text. simple encryption scramble alphabet letter corresponds new randomly chosen (unique) letter.\n- following code shows example using numbershere different approach making use factor() function","code":"\noriginal_sequence <- c(1,2,3,4,5,2,2,3,2,4,5,2)\nnumbers <- unique(original_sequence)\nscrambled_numbers <- sample(numbers)\nencryption_key <- data.frame(numbers,scrambled_numbers)\n\nencrypt_numbers <-function(input_sequence,key){\n  encrypted_sequence<-c()\n  for(i in 1:length(input_sequence)){\n    original_number <- input_sequence[i]\n    new_number <- key[key$numbers==original_number,]$scrambled_numbers\n    encrypted_sequence[i] <- new_number\n  }\n  return(encrypted_sequence)\n}\n\nencrypt_numbers(original_sequence,encryption_key)\n#>  [1] 5 2 1 4 3 2 2 1 2 4 3 2\noriginal_sequence <- c(1,2,3,4,5,2,2,3,2,4,5,2)\noriginal_sequence <- as.factor(original_sequence)\nlevels(original_sequence) # show names of levels in factor\n#> [1] \"1\" \"2\" \"3\" \"4\" \"5\"\nnew_sequence <- original_sequence # copy\nlevels(new_sequence)<-c(5,4,3,2,1) # rename the levels\nnew_sequence # all elements are now changed\n#>  [1] 5 4 3 2 1 4 4 3 4 2 1 4\n#> Levels: 5 4 3 2 1"},{"path":"practice-problems.html","id":"snakes-and-ladders","chapter":"Practice problems","heading":"0.13.6 Snakes and Ladders","text":"task write algorithm can simulate playing depicted Snakes Ladders board. assume roll dice produces random number 1 6. able simulate one played game, write loop simulate 1000 games, estimate average number dice rolls needed successfully complete game.-tip: consider simpler version problem. many times need roll dice dice rolls add 25 greater?add representaion board, change square player depending whether land ladder snake.","code":"\n# rolling a dice with sample\nsample(c(1,2,3,4,5,6),1)\n#> [1] 5\nsample(c(1,2,3,4,5,6),1)\n#> [1] 4\nsample(c(1,2,3,4,5,6),1)\n#> [1] 4\n\n# try one simulation\ntotal_sum<-0\nnumber_of_rolls<-0\nwhile(total_sum < 25){\n  number_of_rolls <- number_of_rolls+1\n  total_sum <-total_sum+sample(c(1,2,3,4,5,6),1)\n}\nnumber_of_rolls\n#> [1] 8\n\n# record the results from multiple simulations\n\nsave_rolls <- c()\nfor(sims in 1:100){\n  total_sum<-0\n  number_of_rolls<-0\n  while(total_sum < 25){\n    number_of_rolls <- number_of_rolls+1\n    total_sum <-total_sum+sample(c(1,2,3,4,5,6),1)\n  }\n  save_rolls[sims] <- number_of_rolls\n}\nmean(save_rolls)\n#> [1] 7.84"},{"path":"practice-problems.html","id":"dice-rolling-simulations","chapter":"Practice problems","heading":"0.13.7 Dice-rolling simulations","text":"Assume pair dice rolled. Using monte carlo-simulation, compute probabilities rolling 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, respectively.","code":""},{"path":"practice-problems.html","id":"monte-hall-problem","chapter":"Practice problems","heading":"0.13.8 Monte Hall problem","text":"monte-hall problem follows. contestant game show presented three closed doors. told prize behind one door, two goats behind two doors. asked choose door contains prize. making choice game show host opens one remaining two doors (chosen contestant), reveals goat. now two door remaining. contestant asked like switch choice door, keep initial choice. correct answer participant switch initial choice, choose door. increase odds winning. Demonstrate monte-carlo simulation odds winning higher participant switches participants keeps original choice.","code":""},{"path":"practice-problems.html","id":"doors-problem","chapter":"Practice problems","heading":"0.13.9 100 doors problem","text":"Problem: 100 doors row initially closed. make 100 passes doors. first time , visit every door toggle door (door closed, open ; open, close ). second time visit every 2nd door (door 2, 4, 6, etc.). third time, every 3rd door (door 3, 6, 9, etc.), etc, visit 100th door.Question: state doors last pass? open, closed?","code":""},{"path":"practice-problems.html","id":"bottles-of-beer-problem","chapter":"Practice problems","heading":"0.13.10 99 Bottles of Beer Problem","text":"puzzle, write code print entire “99 bottles beer wall”” song. know song, lyrics follow form:X bottles beer wall X bottles beer Take one , pass around X-1 bottles beer wallWhere X X-1 replaced numbers course, 99 way 0.","code":""},{"path":"practice-problems.html","id":"random-tic-tac-toe","chapter":"Practice problems","heading":"0.13.11 Random Tic-Tac-Toe","text":"Imagine two players make completely random choices playing tic-tac-toe. game either end draw one two players win. Create monte-carlo simulation “random” version tic-tac-toe. 10,000 simulations, proportion time game won versus drawn?","code":""},{"path":"coding-reference.html","id":"coding-reference","chapter":"Coding Reference","heading":"Coding Reference","text":"page contains minimal explanations examples common coding patterns base R tidyverse. Students can make content requests contribute reference page, just leave message github issues course repository.","code":""},{"path":"coding-reference.html","id":"base-r","chapter":"Coding Reference","heading":"0.14 Base R","text":"Base R refers intrinsics capabilities R come fresh installation R. additional libraries needed.","code":""},{"path":"coding-reference.html","id":"variables","chapter":"Coding Reference","heading":"0.15 Variables","text":"variable name. <- assignment operator. example, 1 assigned object named .Variables classes describe contents.Classes allow disallow commands. example, can’t add numeric character:Classes can converted:","code":"\na <- 1\nx <- 1\nclass(x)\n#> [1] \"numeric\"\n\ny <- \"1\"\nclass(y)\n#> [1] \"character\"\n\nz <- TRUE\nclass(z)\n#> [1] \"logical\"\nx+y\n#> Error in x + y: non-numeric argument to binary operator\ny <- as.numeric(y)\nx+y\n#> [1] 2"},{"path":"coding-reference.html","id":"vectors","chapter":"Coding Reference","heading":"0.15.1 Vectors","text":"Vectors 1-dimensional objects name, can hold multiple elements class. number elements vector vector length. Manipulating vectors involves creating , storing, retrieving, changing elements inside vector.","code":""},{"path":"coding-reference.html","id":"vector-creation","chapter":"Coding Reference","heading":"0.15.1.1 Vector Creation","text":"multiple ways create vectorlength() returns number elements vector","code":"\na <- c() # create a NULL vector\na\n#> NULL\n\na <- 1:5 # assign a sequence to a name\na\n#> [1] 1 2 3 4 5\n\na <- c(1,2,3,4,5) # assign a vector made with combine c()\na\n#> [1] 1 2 3 4 5\n\n#pre-assign an empty vector\na <- vector(mode = \"numeric\", length=10)\na\n#>  [1] 0 0 0 0 0 0 0 0 0 0\n\na <- vector(mode = \"integer\", length=10)\na\n#>  [1] 0 0 0 0 0 0 0 0 0 0\n\na <- vector(mode = \"logical\", length=10)\na\n#>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\na <- vector(mode = \"character\", length=10)\na\n#>  [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\na < c(1,4,5)\n#>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nlength(a)\n#> [1] 10"},{"path":"coding-reference.html","id":"vector-combination","chapter":"Coding Reference","heading":"0.15.1.2 Vector Combination","text":"possible combine existing vectors together make new vector using c().However, attempt combine vectors different classes, R throw error, coerce (convert) one vectors class .","code":"\nx <- 1:5\ny <- 6:10\n\nx\n#> [1] 1 2 3 4 5\ny\n#> [1]  6  7  8  9 10\n\nz <- c(x,y)\nz\n#>  [1]  1  2  3  4  5  6  7  8  9 10\nx <- 1:5\ny <- c(\"a\",\"b\",\"c\",\"d\",\"e\")\n\nx\n#> [1] 1 2 3 4 5\ny\n#> [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\nz <- c(x,y)\nz\n#>  [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"a\" \"b\" \"c\" \"d\" \"e\""},{"path":"coding-reference.html","id":"vector-indexing","chapter":"Coding Reference","heading":"0.15.1.3 Vector Indexing","text":"Vector indexing process isolating specific positions elements vector. Vector indexing uses [] notation.general syntax vector_name[positions], vector_name name vector, positions vector positions index.Logical vectors can indicate positions. case, elements TRUE positions returned","code":"\na <- c(23,34,45,56,67,78,89)\n\na[1] # returns the element in position 1\n#> [1] 23\n\na[1:3] # returns elements in positions 1 to 3\n#> [1] 23 34 45\n\na[c(4,5,6)]\n#> [1] 56 67 78\n\na[c(1,1,1)]\n#> [1] 23 23 23\na <- c(45,56,78)\n\na[c(TRUE, FALSE, FALSE)]\n#> [1] 45\n\na[c(FALSE, TRUE, FALSE)]\n#> [1] 56\n\na[c(FALSE, FALSE, TRUE)]\n#> [1] 78"},{"path":"coding-reference.html","id":"vector-indexing-and-assignment","chapter":"Coding Reference","heading":"0.15.1.4 Vector indexing and assignment","text":"Vector indexing can also used assign new elements indexed positions.","code":"\na <- c(45,56,78)\na\n#> [1] 45 56 78\n\na[3] <- 100\na\n#> [1]  45  56 100\n\na[1:3] <- \"Hello\"\na\n#> [1] \"Hello\" \"Hello\" \"Hello\""},{"path":"coding-reference.html","id":"logical-indexing","chapter":"Coding Reference","heading":"0.15.1.5 Logical indexing","text":"Vectors can indexing using logical comparisons (see section logic explanation examples logical comparisons).","code":"\na <- c(1,3,2,4,3,4)\n\na == 4 # create logical vector of positions containing 4\n#> [1] FALSE FALSE FALSE  TRUE FALSE  TRUE\n\n# inserting the above into a[] finds the elements equal to 4\na[a == 4] # elements equal to 4\n#> [1] 4 4\n\na[a < 4] # elements less than 4\n#> [1] 1 3 2 3\n\na[a <= 4] # elements less than or equal to 4\n#> [1] 1 3 2 4 3 4\n\na[a != 1] # elements not equal to 1\n#> [1] 3 2 4 3 4"},{"path":"coding-reference.html","id":"data.frame","chapter":"Coding Reference","heading":"0.15.2 Data.frame","text":"Data.frames 2-d storage objects, like table (excel sheet), columns rows.","code":"\na <- data.frame() # make an empty data.frame\na\n#> data frame with 0 columns and 0 rows\nclass(a)\n#> [1] \"data.frame\""},{"path":"coding-reference.html","id":"data.frame-creation","chapter":"Coding Reference","heading":"0.15.2.1 Data.frame creation","text":"common method create data.frame involves adding existing vectors together. Data.frames often also created loading data files 2-d tables. See also section using dplyr manipulate data dataframes. Data.frames also similar data.tables, tibbles, can usually interchanged.dim() returns number rows columns data.frame","code":"\nx <- c(1,2,3)\ny <- c(\"a\",\"b\",\"c\")\nz <- c(TRUE, TRUE,TRUE)\n\na <- data.frame(x,y,z)\na\n#>   x y    z\n#> 1 1 a TRUE\n#> 2 2 b TRUE\n#> 3 3 c TRUE\ndim(a)\n#> [1] 3 3"},{"path":"coding-reference.html","id":"indexing-by-column-name","chapter":"Coding Reference","heading":"0.15.2.2 Indexing by column name","text":"column data.frame name, can accessed using $ syntax:","code":"\nnames(a)\n#> [1] \"x\" \"y\" \"z\"\n\na$x\n#> [1] 1 2 3\n\na$y\n#> [1] \"a\" \"b\" \"c\"\n\na$z\n#> [1] TRUE TRUE TRUE\n\n#re-name by assigning a new vector \nnames(a) <- c(\"new_x\",\"Why\",\"Zee\")\na\n#>   new_x Why  Zee\n#> 1     1   a TRUE\n#> 2     2   b TRUE\n#> 3     3   c TRUE\n\na$new_x\n#> [1] 1 2 3\na$Why\n#> [1] \"a\" \"b\" \"c\"\na$Zee\n#> [1] TRUE TRUE TRUE"},{"path":"coding-reference.html","id":"indexing-with-rowscolumns","chapter":"Coding Reference","heading":"0.15.2.3 Indexing with [rows,columns]","text":"Data.frames rows columns, can indexed using [rows,columns] notation, rows vector row numbers, columns vector column numbers","code":"\na\n#>   new_x Why  Zee\n#> 1     1   a TRUE\n#> 2     2   b TRUE\n#> 3     3   c TRUE\n\na[1,] # row 1\n#>   new_x Why  Zee\n#> 1     1   a TRUE\n\na[,1] # column 1\n#> [1] 1 2 3\n\na[1:2,] # rows 1 to 2\n#>   new_x Why  Zee\n#> 1     1   a TRUE\n#> 2     2   b TRUE\n\na[,1:2] # columns 1 to 2\n#>   new_x Why\n#> 1     1   a\n#> 2     2   b\n#> 3     3   c\n\na[1:2,1:2] #rows 1 to 2 and columns 1 to 2\n#>   new_x Why\n#> 1     1   a\n#> 2     2   b\n\na[1:2,'new_x'] # Column names can be used\n#> [1] 1 2"},{"path":"coding-reference.html","id":"row-and-column-binding","chapter":"Coding Reference","heading":"0.15.2.4 row and column binding","text":"possible add rows using rbind(), add columns using cbind().","code":"\n# row bind a copy of a to itself\na\n#>   new_x Why  Zee\n#> 1     1   a TRUE\n#> 2     2   b TRUE\n#> 3     3   c TRUE\na <- rbind(a,a)\ndim(a)\n#> [1] 6 3\n\n# create a new vector, add it as a new column\nmy_new <- c(1,4,3,2,4,5)\na <- cbind(a,my_new)\na\n#>   new_x Why  Zee my_new\n#> 1     1   a TRUE      1\n#> 2     2   b TRUE      4\n#> 3     3   c TRUE      3\n#> 4     1   a TRUE      2\n#> 5     2   b TRUE      4\n#> 6     3   c TRUE      5"},{"path":"coding-reference.html","id":"indexing-and-assignment","chapter":"Coding Reference","heading":"0.15.2.5 Indexing and assignment","text":"elements data.frame can re-assigned your_dataframe[row:position] <- new stuff. generally necessary new elements class original elements","code":"\na\n#>   new_x Why  Zee my_new\n#> 1     1   a TRUE      1\n#> 2     2   b TRUE      4\n#> 3     3   c TRUE      3\n#> 4     1   a TRUE      2\n#> 5     2   b TRUE      4\n#> 6     3   c TRUE      5\n\na[,1] <- 5 #assign column 1 all 5s\n\na$Why <- c(\"new\",\"words\",\"are\",\"going\",\"in\",\"here\")\na\n#>   new_x   Why  Zee my_new\n#> 1     5   new TRUE      1\n#> 2     5 words TRUE      4\n#> 3     5   are TRUE      3\n#> 4     5 going TRUE      2\n#> 5     5    in TRUE      4\n#> 6     5  here TRUE      5\n\na[6,3] <- FALSE # row 6, column 3\na\n#>   new_x   Why   Zee my_new\n#> 1     5   new  TRUE      1\n#> 2     5 words  TRUE      4\n#> 3     5   are  TRUE      3\n#> 4     5 going  TRUE      2\n#> 5     5    in  TRUE      4\n#> 6     5  here FALSE      5"},{"path":"coding-reference.html","id":"logical-indexing-1","chapter":"Coding Reference","heading":"0.15.2.6 Logical indexing","text":"also possible index data.frame logical comparisons. example, following returns rows value column my_new equals 4","code":"\na[a$my_new == 4,]\n#>   new_x   Why  Zee my_new\n#> 2     5 words TRUE      4\n#> 5     5    in TRUE      4"},{"path":"coding-reference.html","id":"lists","chapter":"Coding Reference","heading":"0.15.3 Lists","text":"Lists objects can store arbitrary elements class, including vectors, dataframes, even lists. Lists commonly used store results model, especially model returns many different kinds results different formats.create list three elements, scalar, vector, dataframe.","code":"\nx <- 1\ny <- c(1,2,3,4,5)\nz <- data.frame(a= 1:5, b=1:5, c=1:5)\nmy_list <- list(x, y, z)\n\nmy_list\n#> [[1]]\n#> [1] 1\n#> \n#> [[2]]\n#> [1] 1 2 3 4 5\n#> \n#> [[3]]\n#>   a b c\n#> 1 1 1 1\n#> 2 2 2 2\n#> 3 3 3 3\n#> 4 4 4 4\n#> 5 5 5 5"},{"path":"coding-reference.html","id":"list-indexing","chapter":"Coding Reference","heading":"0.15.3.1 List indexing","text":"Access elements list using [[]]","code":"\nmy_list[[1]]\n#> [1] 1\n\nmy_list[[2]]\n#> [1] 1 2 3 4 5\n\nmy_list[[3]]\n#>   a b c\n#> 1 1 1 1\n#> 2 2 2 2\n#> 3 3 3 3\n#> 4 4 4 4\n#> 5 5 5 5"},{"path":"coding-reference.html","id":"named-elements","chapter":"Coding Reference","heading":"0.15.3.2 Named elements","text":"Elements list can given names, indexed name:","code":"\nx <- 1\ny <- c(1,2,3,4,5)\nz <- data.frame(a= 1:5, b=1:5, c=1:5)\nmy_list <- list(ex = x, why = y,zee=  z)\n\nmy_list\n#> $ex\n#> [1] 1\n#> \n#> $why\n#> [1] 1 2 3 4 5\n#> \n#> $zee\n#>   a b c\n#> 1 1 1 1\n#> 2 2 2 2\n#> 3 3 3 3\n#> 4 4 4 4\n#> 5 5 5 5\n\nmy_list$ex\n#> [1] 1\nmy_list$why\n#> [1] 1 2 3 4 5\nmy_list$zee\n#>   a b c\n#> 1 1 1 1\n#> 2 2 2 2\n#> 3 3 3 3\n#> 4 4 4 4\n#> 5 5 5 5\n\nmy_list[[\"ex\"]]\n#> [1] 1\nmy_list[[\"why\"]]\n#> [1] 1 2 3 4 5\nmy_list[[\"zee\"]]\n#>   a b c\n#> 1 1 1 1\n#> 2 2 2 2\n#> 3 3 3 3\n#> 4 4 4 4\n#> 5 5 5 5"},{"path":"coding-reference.html","id":"addremove-elements-in-lists","chapter":"Coding Reference","heading":"0.15.3.3 Add/Remove elements in lists","text":"possible assign new names elements list, e.g.:","code":"\nmy_list[[\"new_thing\"]] <- 12345\n\nmy_list\n#> $ex\n#> [1] 1\n#> \n#> $why\n#> [1] 1 2 3 4 5\n#> \n#> $zee\n#>   a b c\n#> 1 1 1 1\n#> 2 2 2 2\n#> 3 3 3 3\n#> 4 4 4 4\n#> 5 5 5 5\n#> \n#> $new_thing\n#> [1] 12345\n\n#set an element to NULL removes it\nmy_list[[\"zee\"]] <- NULL\n\nmy_list\n#> $ex\n#> [1] 1\n#> \n#> $why\n#> [1] 1 2 3 4 5\n#> \n#> $new_thing\n#> [1] 12345"},{"path":"coding-reference.html","id":"logic","chapter":"Coding Reference","heading":"0.16 Logic","text":"Logic statements used compare two things, two sets things. output comparison TRUE FALSE statment. many things compared , output many TRUE FALSE statements comparison","code":""},{"path":"coding-reference.html","id":"equal-to","chapter":"Coding Reference","heading":"0.16.1 equal to ==","text":"","code":"\n1==1 # is 1 equal to 1?\n#> [1] TRUE\n1==2 # is 1 equal to 2?\n#> [1] FALSE\n\nc(1,2,3) == c(2,1,3) # compares each element with each element\n#> [1] FALSE FALSE  TRUE\n1 == c(2,1,3)\n#> [1] FALSE  TRUE FALSE"},{"path":"coding-reference.html","id":"not-equal-to","chapter":"Coding Reference","heading":"0.16.2 not equal to !=","text":"","code":"\n1!=1 # is 1 equal to 1?\n#> [1] FALSE\n1!=2 # is 1 equal to 2?\n#> [1] TRUE\n\nc(1,2,3) != c(2,1,3) # compares each element with each element\n#> [1]  TRUE  TRUE FALSE\n1 != c(2,1,3)\n#> [1]  TRUE FALSE  TRUE"},{"path":"coding-reference.html","id":"greater-than-less-than","chapter":"Coding Reference","heading":"0.16.3 Greater than/ less than","text":"","code":"\n\n1 > 1 # is 1 greater than 1?\n#> [1] FALSE\n5 > 1 # is 5 greater than 1?\n#> [1] TRUE\n3 < 2 # is 3 less than 2?\n#> [1] FALSE\n3 < 1 # is 3 less than 1?\n#> [1] FALSE\n\nc(1,2,3) > c(2,1,3) # ask the question element by element\n#> [1] FALSE  TRUE FALSE\nc(1,2,3) < c(2,1,3)\n#> [1]  TRUE FALSE FALSE\n\n2 > c(1,2,3) # is greater than each of the numbers\n#> [1]  TRUE FALSE FALSE"},{"path":"coding-reference.html","id":"section","chapter":"Coding Reference","heading":"0.16.4 >= <=","text":"something greater equal something else","code":"\n1 >= 1 # is 1 greater than 1?\n#> [1] TRUE\n5 >= 1 # is 5 greater than 1?\n#> [1] TRUE\n3 <= 2 # is 3 less than 2?\n#> [1] FALSE\n3 <= 1 # is 3 less than 1?\n#> [1] FALSE\n\nc(1,2,3) >= c(2,1,3) # ask the question element by element\n#> [1] FALSE  TRUE  TRUE\nc(1,2,3) <= c(2,1,3)\n#> [1]  TRUE FALSE  TRUE\n\n2 >= c(1,2,3) # is greater than each of the numbers\n#> [1]  TRUE  TRUE FALSE"},{"path":"coding-reference.html","id":"and","chapter":"Coding Reference","heading":"0.16.5 AND","text":"ampersand & used , allows use evaluate whether two properties TRUE.","code":"\n# is 16 divisible by 4 AND 8\n16%%4 == 0 & 16%%8 ==0\n#> [1] TRUE\n\n# is 16 divisible by 4 AND 3\n16%%4 == 0 & 16%%3 ==0\n#> [1] FALSE\n\n# is 16 divisible by 8 and 4 and 2\n16%%4 == 0 & 16%%8 ==0 & 16%%2 ==0\n#> [1] TRUE"},{"path":"coding-reference.html","id":"or","chapter":"Coding Reference","heading":"0.16.6 OR","text":"| used , allows use evaluate least one properties TRUE.","code":"\n# is 16 divisible by 4 OR 8\n16%%4 == 0 | 16%%8 ==0\n#> [1] TRUE\n\n# is 16 divisible by 4 OR 3\n# it is divisible by 4, so the answer is TRUE\n# because at least one of the comparisons is TRUE\n16%%4 == 0 | 16%%3 ==0\n#> [1] TRUE"},{"path":"coding-reference.html","id":"true-false","chapter":"Coding Reference","heading":"0.16.7 TRUE FALSE","text":"R returns values TRUE FALSE, return logical variable. also treats TRUE 1, FALSE 0. example see possible sum logical variable multiple TRUE FALSE entries.","code":"\nc(1,2,3) == c(1,2,3)\n#> [1] TRUE TRUE TRUE\nsum(c(1,2,3) == c(1,2,3))\n#> [1] 3\n\nc(1,2,3) == c(2,1,3)\n#> [1] FALSE FALSE  TRUE\nsum(c(1,2,3) == c(2,1,3))\n#> [1] 1"},{"path":"coding-reference.html","id":"if-else","chapter":"Coding Reference","heading":"0.17 IF ELSE","text":"roller-coaster operator checks people taller line see can ride coaster. ELSE control structure. person taller line, can go ride; ELSE (otherwise) person can go ride.words, situation X, something; ELSE (situation X), something different.ELSE statements let us specify conditions specific actions taken. Generally, ELSE statements used inside loops (, , repeat loops), step iteration loop, want check something, something.Consider :Normally find ELSE loop like :can multiple conditions statements. See next section loops info using loops.","code":"\na <- 1 # define a to be a 1\nif(a==1){  \n  print(a) # this is what happens if a==1\n} else {\n  print(\"A is not 1\") # this is what happens if a is not 1\n}\n#> [1] 1\n\n\na <- 2 # define a to be a 1\nif(a==1){  \n  print(a) # this is what happens if a==1\n} else {\n  print(\"A is not 1\") # this is what happens if a is not 1\n}\n#> [1] \"A is not 1\"\na <- c(1,0,1,0,0,0,1) # make a variable contain 1s and 0s\n\n# write a loop to check each element in the variable\n# and do different things depending on the element\n\nfor(i in a){\n  if(i == 1){\n    print(\"I'm a 1\") # what to do when i is 1\n  } else {\n    print(\"I'm not a 1\") # what to do when i is not 1\n  }\n}\n#> [1] \"I'm a 1\"\n#> [1] \"I'm not a 1\"\n#> [1] \"I'm a 1\"\n#> [1] \"I'm not a 1\"\n#> [1] \"I'm not a 1\"\n#> [1] \"I'm not a 1\"\n#> [1] \"I'm a 1\"\na <- c(1,2,3,1,2,0,1) # make a variable contain 1s and 0s\n\n# write a loop to check each element in the variable\n# and do different things depending on the element\n\nfor(i in a){\n  if(i == 1){\n    print(\"I'm a 1\") # what to do when i is 1\n  } else if (i==2){\n    print(\"I'm a 2\") # what to do when i is 2\n  } else if (i==3){\n    print(\"I'm a 3\") # what to do when i is 3\n  } else {\n    print(\"I'm not any of the above\") #what to do when none are true\n  }\n}\n#> [1] \"I'm a 1\"\n#> [1] \"I'm a 2\"\n#> [1] \"I'm a 3\"\n#> [1] \"I'm a 1\"\n#> [1] \"I'm a 2\"\n#> [1] \"I'm not any of the above\"\n#> [1] \"I'm a 1\""},{"path":"coding-reference.html","id":"loops","chapter":"Coding Reference","heading":"0.18 Loops","text":"Check R help Control Flow ?Control.(){}\n(loop control){something iteration}Loop control defined parentheses. name iterator placed left (can assigned name want, need declared advance). execution loop, iterator takes values inside vector placed right side . Specifically, following happening.Loop steps:\n1. iterator <- vector[1]\n2. iterator <- vector[2]\n3. iterator <- vector[3]\n4. etc.loop automatically stop reaches last item vector. loop can stopped using break command.","code":"\nfor(iterator in vector){\n  #do something\n}\n# Make a loop do something 5 times\n# i is the iterator\n# 1:5 creates a vector with 5 numbers in it, 1, 2, 3, 4, 5\n# the loop will run 5 times, because there are five things to assign to i\nfor(i in 1:5){\n  print(\"hello\")\n}\n#> [1] \"hello\"\n#> [1] \"hello\"\n#> [1] \"hello\"\n#> [1] \"hello\"\n#> [1] \"hello\"\n# show the value of i each step of the loop\nfor(i in 1:5){\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# define the vector to loop over in advance\nx <- 1:5\nfor(i in x){\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# Reminder that i becomes the next value in the vector\n# your vector can have any order \nmy_sequence <- c(1,5,2,3,4)\nfor(i in my_sequence){\n  print(i)\n}\n#> [1] 1\n#> [1] 5\n#> [1] 2\n#> [1] 3\n#> [1] 4\n# index vector does not need to be numbers\nmy_things <- c(\"A\",\"B\",\"C\",\"D\")\nfor(i in my_things){\n  print(i)\n}\n#> [1] \"A\"\n#> [1] \"B\"\n#> [1] \"C\"\n#> [1] \"D\""},{"path":"coding-reference.html","id":"breaking-a-loop","chapter":"Coding Reference","heading":"0.18.1 Breaking a loop","text":"break stops loop. Used logical statements define conditions necessary cause break.","code":"\nfor(i in 1:10){\n  if(i <5){\n    print(i)\n  } else{\n    break\n  }\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4"},{"path":"coding-reference.html","id":"while-loops","chapter":"Coding Reference","heading":"0.18.2 While loops","text":"loops run logical condition met. iterator, just logic statement needs met.one prints less 6. soon becomes “less 6”, loop stops. Critically, inside loop, value increases iteration.","code":"\ni <- 1 # create an variable\nwhile (i < 6) {\n  print(i)\n  i = i+1 #add one eachs step of the loop\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5"},{"path":"coding-reference.html","id":"repeat-loops","chapter":"Coding Reference","heading":"0.18.3 Repeat loops","text":"Similar , let’s things condition met.","code":"\ni<-0\nrepeat{\n  i<-i+1\n  print(i)\n  if(i==5){\n    break\n  }\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5"},{"path":"coding-reference.html","id":"examples","chapter":"Coding Reference","heading":"0.18.4 Examples","text":"Braces needed one lineUsing value iterator assign values systematically another variable.Make counter, need oneNesting loopsbreak exits immediate loop","code":"\nfor(i in 1:5) print(i)\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# put 1 into the first five positions of x\nx <- c() # create empty vector\nfor(i in 1:5){\n  x[i] <- 1  # assign 1 to the ith slot in x\n}\nx\n#> [1] 1 1 1 1 1\n\n# put the numbers 1-5 in the first 5 positions of x\nx <-c()\nfor(i in 1:5){\n  x[i] <- i\n}\nx\n#> [1] 1 2 3 4 5\na <- c(1,4,3,5,7,6,8,2)\nodd <- c()\ncounter <- 0\nfor(i in a){  # i will the values of a in each position\n  counter <- counter+1\n  if(i%%2 != 0){\n    odd[counter] <- \"odd\"\n  } else {\n    odd[counter] <- \"even\"\n  }\n}\nodd\n#> [1] \"odd\"  \"even\" \"odd\"  \"odd\"  \"odd\"  \"even\" \"even\" \"even\"\n\n# An alternative strategy\n\na <- c(1,4,3,5,7,6,8,2)\nodd <- c()\n# 1:length(a) creates a sequence from 1 to length\nfor(i in 1:length(a)){  \n  if(a[i]%%2 != 0){\n    odd[i] <- \"odd\"\n  } else {\n    odd[i] <- \"even\"\n  }\n}\nodd\n#> [1] \"odd\"  \"even\" \"odd\"  \"odd\"  \"odd\"  \"even\" \"even\" \"even\"\n\nfor(i in 1:5){\n  for(j in 1:5){\n   print(c(i,j))\n  }\n}\n#> [1] 1 1\n#> [1] 1 2\n#> [1] 1 3\n#> [1] 1 4\n#> [1] 1 5\n#> [1] 2 1\n#> [1] 2 2\n#> [1] 2 3\n#> [1] 2 4\n#> [1] 2 5\n#> [1] 3 1\n#> [1] 3 2\n#> [1] 3 3\n#> [1] 3 4\n#> [1] 3 5\n#> [1] 4 1\n#> [1] 4 2\n#> [1] 4 3\n#> [1] 4 4\n#> [1] 4 5\n#> [1] 5 1\n#> [1] 5 2\n#> [1] 5 3\n#> [1] 5 4\n#> [1] 5 5\n\n# example of using nested loops to fill the contents\n# of a matrix\n\nmy_matrix <- matrix(0,ncol=5,nrow=5)\nfor(i in 1:5){\n  for(j in 1:5){\n   my_matrix[i,j] <- i*j\n  }\n}\nmy_matrix\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    1    2    3    4    5\n#> [2,]    2    4    6    8   10\n#> [3,]    3    6    9   12   15\n#> [4,]    4    8   12   16   20\n#> [5,]    5   10   15   20   25\n# the inside loop stops when i+j is greater than 5\n# the outside loop keeps going\n\nsum_of_i_j <- c()\ncounter <- 0\nfor(i in 1:5){\n  for(j in 1:5){\n    counter <- counter+1\n    sum_of_i_j[counter] <- i+j\n    if(i+j > 5){\n      break\n    }\n  }\n}\nsum_of_i_j\n#>  [1] 2 3 4 5 6 3 4 5 6 4 5 6 5 6 6"},{"path":"coding-reference.html","id":"functions","chapter":"Coding Reference","heading":"0.19 Functions","text":"section discusses syntax writing custom functions R.","code":""},{"path":"coding-reference.html","id":"function-syntax","chapter":"Coding Reference","heading":"0.19.1 function syntax","text":"","code":"\nfunction_name <- function(input1,input2){\n  #code here\n  return(something)\n}"},{"path":"coding-reference.html","id":"example-functions","chapter":"Coding Reference","heading":"0.19.2 example functions","text":"function input (). Whenever run function, simply return whatever placed inside return statement.function simply takes input, returns input without modifying .function takes input, creates internal variable called temp assigns input+1. contents temp returned. Note , checking input, return erro input character (can’t add one character R)function adds input checking. add one input numeric type. Otheriwse, use stop() return error message consoleA function three inputs","code":"\n# define the function\nprint_hello_world <- function(){\n  return(print(\"hello world\"))\n}\n\n# use the function\nprint_hello_world()\n#> [1] \"hello world\"\nreturn_input <- function(input){\n  return(input)\n}\n\n# the variable input is assigned a 1\n# then we return(input), which will result in a 1\n# because the function internally assigns 1 to the input\nreturn_input(1)\n#> [1] 1\n\na <- \"something\"\nreturn_input(a)\n#> [1] \"something\"\nadd_one <- function(input){\n  temp <- input+1\n  return(temp)\n}\n\nadd_one(1)\n#> [1] 2\nadd_one(\"a\")\n#> Error in input + 1: non-numeric argument to binary operator\nadd_one <- function(input){\n  if(class(input) == \"numeric\"){\n    temp <- input+1\n    return(temp)\n  } else {\n    return(stop(\"input must be numeric\"))\n  }\n}\n\nadd_one(1)\n#> [1] 2\nadd_one(\"a\")\n#> Error in add_one(\"a\"): input must be numeric\nadd_multiply <- function(input, x_plus,x_times){\n  temp <- (input+x_plus)*x_times\n  return(temp)\n}\n\n# input is 1\n# x_plus <- 2\n# x_times <- 3\n# will return (1+2)*3 = 9\nadd_multiply(1,2,3)\n#> [1] 9"},{"path":"coding-reference.html","id":"tidyverse","chapter":"Coding Reference","heading":"0.20 Tidyverse","text":"tidyverse set popular R packages convenient many aspects data-analysis. tidyverse packages can installed one go:","code":"\ninstall.packages(\"tidyverse\")"},{"path":"coding-reference.html","id":"dplyr","chapter":"Coding Reference","heading":"0.21 dplyr","text":"dplyr package several useful functions manipulating summarizing data.frames. illustrate dplyr functionality first create small fake data.frame. link dplyr cheatsheet","code":"\nsubjects <- rep(1:10)\ngrades <- rnorm(n = 10, mean = 65, sd = 5)\nage <- sample(18:20,10,replace=TRUE)\nlikes_chocolate <- sample(c(TRUE,FALSE), 10, replace=TRUE)\nfavorite_color <- sample(c(\"r\",\"o\",\"y\",\"g\",\"b\",\"i\",\"v\"), 10, replace=TRUE)\n\nfake_data <- data.frame(subjects,\n                        grades,\n                        age,\n                        likes_chocolate,\n                        favorite_color)\n\nknitr::kable(head(fake_data))"},{"path":"coding-reference.html","id":"group_by-and-summarize","chapter":"Coding Reference","heading":"0.21.1 group_by and summarize","text":"group_by() allows specify columns split groups analysis, groups levels column (e.g., unique entries column)summarize() conducts analysis group identified group_by step. analysis defined variable names, supplying function computes value given name measurement variable.","code":"\nlibrary(dplyr)\n\nfake_data %>%\n  group_by(likes_chocolate) %>%\n  summarize(mean_grade = mean(grades),\n            sd_grad = sd(grades))\n#> # A tibble: 2 × 3\n#>   likes_chocolate mean_grade sd_grad\n#>   <lgl>                <dbl>   <dbl>\n#> 1 FALSE                 65.4    4.88\n#> 2 TRUE                  71.1   NA\n\nfake_data %>%\n  group_by(likes_chocolate,age) %>%\n  summarize(mean_grade = mean(grades),\n            sd_grad = sd(grades))\n#> # A tibble: 3 × 4\n#> # Groups:   likes_chocolate [2]\n#>   likes_chocolate   age mean_grade sd_grad\n#>   <lgl>           <int>      <dbl>   <dbl>\n#> 1 FALSE              19       64.7    7.55\n#> 2 FALSE              20       66.0    1.98\n#> 3 TRUE               19       71.1   NA"},{"path":"coding-reference.html","id":"filter","chapter":"Coding Reference","heading":"0.21.2 filter","text":"Filter rows depending logical comparisons","code":"\nfake_data %>%\n  filter(grades < 65)\n#>   subjects   grades age likes_chocolate favorite_color\n#> 1        1 59.62167  19           FALSE              g\n#> 2        3 63.57263  19           FALSE              i\n#> 3        4 64.65839  20           FALSE              o\n#> 4        7 59.98810  19           FALSE              o\n#> 5        9 63.27830  20           FALSE              v\n\nfake_data %>%\n  filter(grades < 65,\n         likes_chocolate == TRUE)\n#> [1] subjects        grades          age             likes_chocolate\n#> [5] favorite_color \n#> <0 rows> (or 0-length row.names)"},{"path":"coding-reference.html","id":"select","chapter":"Coding Reference","heading":"0.21.3 select","text":"Select specific columns","code":"\nfake_data %>%\n  select(grades)\n#>      grades\n#> 1  59.62167\n#> 2  67.66261\n#> 3  63.57263\n#> 4  64.65839\n#> 5  75.72879\n#> 6  66.66997\n#> 7  59.98810\n#> 8  67.82681\n#> 9  63.27830\n#> 10 71.06737\n\nfake_data %>%\n  select(grades,likes_chocolate)\n#>      grades likes_chocolate\n#> 1  59.62167           FALSE\n#> 2  67.66261           FALSE\n#> 3  63.57263           FALSE\n#> 4  64.65839           FALSE\n#> 5  75.72879           FALSE\n#> 6  66.66997           FALSE\n#> 7  59.98810           FALSE\n#> 8  67.82681           FALSE\n#> 9  63.27830           FALSE\n#> 10 71.06737            TRUE"},{"path":"coding-reference.html","id":"mutate","chapter":"Coding Reference","heading":"0.21.4 mutate","text":"mutate() can add column","code":"\nfake_data <- fake_data %>%\n  mutate(new_thing = 0)\n\nfake_data\n#>    subjects   grades age likes_chocolate favorite_color new_thing\n#> 1         1 59.62167  19           FALSE              g         0\n#> 2         2 67.66261  20           FALSE              o         0\n#> 3         3 63.57263  19           FALSE              i         0\n#> 4         4 64.65839  20           FALSE              o         0\n#> 5         5 75.72879  19           FALSE              i         0\n#> 6         6 66.66997  20           FALSE              i         0\n#> 7         7 59.98810  19           FALSE              o         0\n#> 8         8 67.82681  20           FALSE              b         0\n#> 9         9 63.27830  20           FALSE              v         0\n#> 10       10 71.06737  19            TRUE              o         0"},{"path":"coding-reference.html","id":"ggplot2","chapter":"Coding Reference","heading":"0.22 ggplot2","text":"ggplot2 library created Hadley Wickham plotting graphing results, refers “grammar graphics”, standardized syntax organization graphing.","code":""},{"path":"coding-reference.html","id":"ggplot2-additional-resources","chapter":"Coding Reference","heading":"0.22.1 ggplot2 additional resources","text":"https://ggplot2.tidyverse.orghttps://r4ds..co.nz/data-visualisation.htmlhttps://ggplot2-book.orghttp://r-statistics.co/ggplot2-Tutorial--R.htmlhttps://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html","code":""},{"path":"coding-reference.html","id":"add-on-packages","chapter":"Coding Reference","heading":"0.22.1.1 Add-on packages","text":"https://www.ggplot2-exts.org repository 50+ add packages ggplot2https://gganimate.com\nallows create animated .gifs ggplots\nmade bunch animated gifs statistics textbook. along code https://crumplab.github.io/statistics/gifs.html\nallows create animated .gifs ggplotsI made bunch animated gifs statistics textbook. along code https://crumplab.github.io/statistics/gifs.htmlggrepel allows repel overlapping text labels away .esquisse GUI (graphic user interface) allows make ggplot graphs using drag-drop, clickable optionsggedit similar , clickable editing ggplot graphsplotly package similar ggplot, makes whole variety graphs, mainly use websites. Allows interactive graphs.\nexample, used plotly publications website, hover dots, info pops https://crumplab.github.io/Publications.html.\nexample, used plotly publications website, hover dots, info pops https://crumplab.github.io/Publications.html.ggpubr (install CRAN), many useful things, including ggarrange function allows knit multiple plots togetherggthemes extra themes, scales, geoms","code":""},{"path":"coding-reference.html","id":"example-code","chapter":"Coding Reference","heading":"0.22.2 Example code","text":"Remember load ggplot2 library use ggplot2.","code":"\nlibrary(ggplot2)"},{"path":"coding-reference.html","id":"scatterplot","chapter":"Coding Reference","heading":"0.22.3 Scatterplot","text":"","code":"\n# Create dataframe\na <- c(1,2,3,2,3,4,5,4)\nb <- c(4,3,4,3,2,1,2,3)\nplot_df <- data.frame(a,b)\n\n# basic scatterplot\nggplot(plot_df, aes(x=a,y=b))+\n  geom_point()\n\n# customize, add regression line\nggplot(plot_df, aes(x=a,y=b))+\n  geom_point(size=2)+\n  geom_smooth(method=lm)+\n  coord_cartesian(xlim=c(0,7),ylim=c(0,10))+\n  xlab(\"x-axis label\")+\n  ylab(\"y-axis label\")+\n  ggtitle(\"I made a scatterplot\")+\n  theme_classic(base_size=12)+\n  theme(plot.title = element_text(hjust = 0.5))"},{"path":"coding-reference.html","id":"bar-graph","chapter":"Coding Reference","heading":"0.22.4 bar graph","text":"1 factor2 factor3 factor","code":"\n#Create a dataframe\nfactor_one <- as.factor(c(\"A\",\"B\",\"C\"))\ndv_means <- c(20,30,40)\ndv_SEs   <- c(4,3.4,4)\nplot_df <- data.frame(factor_one,\n                      dv_means,\n                      dv_SEs)\n\n# basic bar graph\n\nggplot(plot_df, aes(x=factor_one,y=dv_means))+\n  geom_bar(stat=\"identity\")\n\n# adding error bars, customizing\n\nggplot(plot_df, aes(x=factor_one,y=dv_means))+\n  geom_bar(stat=\"identity\")+\n  geom_errorbar(aes(ymin=dv_means-dv_SEs,\n                    ymax=dv_means+dv_SEs),\n                width=.2)+\n  coord_cartesian(ylim=c(0,100))+\n  xlab(\"x-axis label\")+\n  ylab(\"y-axis label\")+\n  ggtitle(\"I made a bar graph\")+\n  theme_classic(base_size=12)+\n  theme(plot.title = element_text(hjust = 0.5))\n#Create a dataframe\nfactor_one <- rep(as.factor(c(\"A\",\"B\",\"C\")),2)\nfactor_two <- rep(as.factor(c(\"IIA\",\"IIB\")),3)\ndv_means <- c(20,30,40,20,40,40)\ndv_SEs   <- c(4,3.4,4,3,2,4)\nplot_df <- data.frame(factor_one,\n                      factor_two,\n                      dv_means,\n                      dv_SEs)\n\n# basic bar graph\n\nggplot(plot_df, aes(x=factor_one,y=dv_means,\n                    group=factor_two,\n                    color=factor_two))+\n  geom_bar(stat=\"identity\", position=\"dodge\")\n\n# adding error bars, customizing\n\nggplot(plot_df, aes(x=factor_one,y=dv_means,\n                    group=factor_two,\n                    color=factor_two,\n                    fill=factor_two))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin=dv_means-dv_SEs,\n                    ymax=dv_means+dv_SEs),\n                position=position_dodge(width=0.9),\n                width=.2,\n                color=\"black\")+\n  coord_cartesian(ylim=c(0,100))+\n  xlab(\"x-axis label\")+\n  ylab(\"y-axis label\")+\n  ggtitle(\"Bar graph 2 factors\")+\n  theme_classic(base_size=12)+\n  theme(plot.title = element_text(hjust = 0.5))\n#Create a dataframe\nfactor_one <- rep(rep(as.factor(c(\"A\",\"B\",\"C\")),2),2)\nfactor_two <- rep(rep(as.factor(c(\"IIA\",\"IIB\")),3),2)\nfactor_three <- rep(as.factor(c(\"IIIA\",\"IIIB\")),each=6)\ndv_means <- c(20,30,40,20,40,40,\n              10,20,50,50,10,10)\ndv_SEs   <- c(4,3.4,4,3,2,4,\n              1,2,1,2,3,2)\nplot_df <- data.frame(factor_one,\n                      factor_two,\n                      factor_three,\n                      dv_means,\n                      dv_SEs)\n\n# basic bar graph\n\nggplot(plot_df, aes(x=factor_one,y=dv_means,\n                    group=factor_two,\n                    color=factor_two))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  facet_wrap(~factor_three)"},{"path":"coding-reference.html","id":"line-graph","chapter":"Coding Reference","heading":"0.22.5 Line Graph","text":"1 factor2 factor","code":"\n#Create a dataframe\nfactor_one <- as.factor(c(\"A\",\"B\",\"C\"))\ndv_means <- c(20,30,40)\ndv_SEs   <- c(4,3.4,4)\nplot_df <- data.frame(factor_one,\n                      dv_means,\n                      dv_SEs)\n\n# basic line graph\n\nggplot(plot_df, aes(x=factor_one,y=dv_means, group=1))+\n  geom_point()+\n  geom_line()\n\n# adding error bars, customizing\n\nggplot(plot_df, aes(x=factor_one,y=dv_means, group=1))+\n  geom_point()+\n  geom_line()+\n  geom_errorbar(aes(ymin=dv_means-dv_SEs,\n                    ymax=dv_means+dv_SEs),\n                width=.2)+\n  coord_cartesian(ylim=c(0,100))+\n  xlab(\"x-axis label\")+\n  ylab(\"y-axis label\")+\n  ggtitle(\"I made a line graph\")+\n  theme_classic(base_size=12)+\n  theme(plot.title = element_text(hjust = 0.5))\n#Create a dataframe\nfactor_one <- rep(as.factor(c(\"A\",\"B\",\"C\")),2)\nfactor_two <- rep(as.factor(c(\"IIA\",\"IIB\")),3)\ndv_means <- c(20,30,40,20,40,40)\ndv_SEs   <- c(4,3.4,4,3,2,4)\nplot_df <- data.frame(factor_one,\n                      factor_two,\n                      dv_means,\n                      dv_SEs)\n\n# basic line graph\n\nggplot(plot_df, aes(x=factor_one,y=dv_means,\n                    group=factor_two,\n                    color=factor_two,\n                    linetype=factor_two))+\n  geom_point()+\n  geom_line()"},{"path":"coding-reference.html","id":"histogram","chapter":"Coding Reference","heading":"0.22.6 Histogram","text":"base R","code":"\na<-rnorm(100,0,1)\nhist(a)\nscore <- rnorm(100,0,1)\nn <- 1:100\nplot_df <- data.frame(score,n)\n\nggplot(plot_df, aes(x=score))+\n  geom_histogram(bins=10,\n                 color=\"white\")"},{"path":"coding-reference.html","id":"knitr","chapter":"Coding Reference","heading":"0.23 knitr","text":"knitr package used compile R markdown documents formats html (webpages) pdf.","code":""},{"path":"coding-reference.html","id":"knitting-to-pdf","chapter":"Coding Reference","heading":"0.23.1 knitting to pdf","text":"latex installation required order knit pdf. Latex also free cross-platform, however complete installation can quite large.think advice Frederick Aust (author papaja package) installing latex worth following:https://crsh.github.io/papaja_man/introduction.html#getting-startedBasically, advice install complete tex distribution (follow links), use tinytex package R. tinytex package sufficient knitting pdf duties.Install tinytex library:Run command installing library","code":"install.packages(\"tinytex\")tinytex::install_tex()"},{"path":"coding-reference.html","id":"knitr-options","chapter":"Coding Reference","heading":"0.23.2 knitr options","text":"create new R Markdown document see following code chunk underneath yaml, beginning document. usually looks like :chunk named setup, printed output, controls global setup options whole document. option set applies remaining code chunks create. ’s way setting defaults.helpful defaults can add. turn option TRUE, turn FALSE.echo=TRUE sets default print remaining code blocks output, FALSE sets default print code blockswarning = FALSE turns printing warningsmessage = FALSE turns printing messages, commonly occur load package, receive message package loadedeval = FALSE sets default evaluate code chunk R Code. run code block, code block still print echo=TRUEerror=TRUE normally knit fails error code. set error=TRUE knit complete, return error message code blocks errors.","code":"```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n``````{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE,\n                      warning = FALSE,\n                      message = FALSE, \n                      eval = FALSE,\n                      error = TRUE)\n```"},{"path":"coding-reference.html","id":"knitr-figure-output-defaults","chapter":"Coding Reference","heading":"0.23.3 knitr figure output defaults","text":"following setup options useful figure output.fig.width = 3 sets default width inches figuresfig.height = 3 sets default height inches figuresfig.path = \"myfigs/\" defines folder figure files saved. relative current working directoydev = c(\"pdf\", \"png\") tells knitr output .png, .pdf versions figure. .pdf contains vector graphics, meaning figure can resized without pixelization.","code":"```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE,\n                      fig.width = 3,\n                      fig.height = 3,\n                      fig.path = \"myfigs/\",\n                      dev = c(\"pdf\", \"png\"))\n```"},{"path":"coding-reference.html","id":"figure-output-per-code-block","chapter":"Coding Reference","heading":"0.23.4 figure output per code block","text":"can set options remaining code blocks individually. overrule default options specify setup chunk.","code":"```{r figurename, fig.width =5, fig.height =7}\n\n# some code that generates a figure\n\n```"},{"path":"coding-reference.html","id":"tables","chapter":"Coding Reference","heading":"0.24 tables","text":"several useful (incredible even) packages making tables R. :knitr package ’s kable function (Create tables Latex, HTML, Markdown)xtable package, lots functions tables. xtable examples xtablekableExtra lots additional table functionalitytangram grammar tables approachpapaja apa-style tables","code":""},{"path":"coding-reference.html","id":"important-table-info","chapter":"Coding Reference","heading":"0.24.1 Important table info","text":"Two things note, tables can difficult, different output formats.Tables R can difficult. example, comfortable making tables Excel, R much difficult comparison. Excel, easy enter information cell, merge cells, add kind formatting want anywhere, just clicking around making changes. R, every detail table specified script. table packages make simple tables easy (hurray!), make complicated tables possible (also good), necessarilly easy.Tables R can difficult. example, comfortable making tables Excel, R much difficult comparison. Excel, easy enter information cell, merge cells, add kind formatting want anywhere, just clicking around making changes. R, every detail table specified script. table packages make simple tables easy (hurray!), make complicated tables possible (also good), necessarilly easy.R can output tables many formats including HTML web, Latex .pdf, formats (e.g., word, markdown). Sometimes (depending functions using) run issues outputting tables different formats. , take steps ensure outputting table format want.R can output tables many formats including HTML web, Latex .pdf, formats (e.g., word, markdown). Sometimes (depending functions using) run issues outputting tables different formats. , take steps ensure outputting table format want.","code":""},{"path":"coding-reference.html","id":"knitrkable","chapter":"Coding Reference","heading":"0.24.2 knitr::kable","text":"kable() function inside knitr package. use need load knitr library, can use knitr::kable(), tells R find kable inside knitr package haven’t loaded knitr using library(knitr).kable() great quickly rendering data frames nice tables without much hassle.","code":"\ndf <- data.frame(A=1,\n                 B=2,\n                 C=3,\n                 D=4)\nknitr::kable(df)"},{"path":"coding-reference.html","id":"xtable","chapter":"Coding Reference","heading":"0.24.3 xtable","text":"Look xtable examples document info. https://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf","code":"\nlibrary(xtable) # load xtable\ndata(tli) # loads a sample data frame\n\n# conduct an ANOVA\n fm1 <- aov(tlimth ~ sex + ethnicty + grade + disadvg, data = tli)\n \n# print the table for HTML using xtable and kable together\nknitr::kable(xtable(fm1))\n\n# Note this will print a table in latex for .pdf\n# xtable(fm1)"},{"path":"coding-reference.html","id":"kableextra","chapter":"Coding Reference","heading":"0.24.4 kableExtra","text":"many great things kableExtra. One great thing kableExtra unique table options html formatting .pdf latex. example, interactive tables possible html, .pdf. Another great thing ability add rows headers top . example, data.frames R one row headers columns, kableExtra can add top.HTML info kableExtraLatex info (pdf) kableExtra","code":"\nlibrary(kableExtra)\ndf <- data.frame(A=1,\n                 B=2,\n                 C=3,\n                 D=4)\nkable(df) %>%\n  kable_styling(\"striped\") %>%\n  add_header_above(c(\"Group 1\" = 2, \"Group 2\" = 2))"},{"path":"coding-reference.html","id":"tangram","chapter":"Coding Reference","heading":"0.24.5 tangram","text":"package implements grammar tables. Similar concept behind ggplot2, implements grammar graphics figures.tangram githubtangram html examples","code":""},{"path":"coding-reference.html","id":"papaja","chapter":"Coding Reference","heading":"0.25 papaja","text":"papaja package rendering APA-style manuscripts pdf using R Markdown. learn papaja class. One feature papaja supports APA-style tables.papaja documentationpapaja APA tables","code":""},{"path":"coding-reference.html","id":"installing-papaja","chapter":"Coding Reference","heading":"0.25.1 Installing papaja","text":"papaja requires latex installation order compile .Rmd documents pdf. papaja documentation provides guidance installing latex papaja, see getting started section.can also watch video, goes steps :","code":"\n## install tinytex\nif(!\"tinytex\" %in% rownames(installed.packages())) install.packages(\"tinytex\")\n\n## initialize tinytex\ntinytex::install_tinytex()\n\n# Install devtools package if necessary\nif(!\"devtools\" %in% rownames(installed.packages())) install.packages(\"devtools\")\n\n# Install the stable development verions from GitHub\ndevtools::install_github(\"crsh/papaja\")"},{"path":"coding-reference.html","id":"vectorized-approaches","chapter":"Coding Reference","heading":"0.26 Vectorized approaches","text":"Loops common tool something many times. R can accomplish goal “something many times” without loops, using vectorized approach.","code":""},{"path":"coding-reference.html","id":"basic-examples","chapter":"Coding Reference","heading":"0.26.1 Basic examples","text":"Let’s take close look basic differences using loop, using R’s vectorized approachConsider problem adding single number numbers vector.adding two vectors together, add first two numbers together, second two numbers etc.comparing identity elements two vectors see ?","code":"\nnums <- c(1,2,3,4)\n\n# vectorized approach\n# R automatically adds 1 to all of the numbers\nnums+1\n#> [1] 2 3 4 5\n\n# loop approach\n# much longer to write out\nfor(i in 1:length(nums)){\n  nums[i] <- nums[i]+1\n}\nnums\n#> [1] 2 3 4 5\nA <- c(1,2,3,4)\nB <- c(1,2,3,4)\n\n# vectorized approach\nA+B\n#> [1] 2 4 6 8\n\n# loop approach\nthe_sum <-c()\nfor(i in 1:length(A)){\n  the_sum[i] <- A[i]+B[i]\n}\nthe_sum\n#> [1] 2 4 6 8\nA <- c(\"c\",\"e\",\"f\",\"g\")\nB <- c(\"d\",\"e\",\"f\",\"g\")\n\n#vectorized approach\nA==B\n#> [1] FALSE  TRUE  TRUE  TRUE\n\n# loop approach\ncompared <-c()\nfor(i in 1:length(A)){\n  if(A[i]==B[i]){\n    compared[i] <- TRUE\n  } else {\n    compared[i] <- FALSE\n  }\n}\ncompared\n#> [1] FALSE  TRUE  TRUE  TRUE"},{"path":"coding-reference.html","id":"replicate","chapter":"Coding Reference","heading":"0.26.2 Replicate","text":"replicate(n, expr) allows repeat function many times, return answer vectorThe next example shows write function something, use function inside replicate repeat function many times.example, write function run one-sample t-test random sample drawn normal distribution","code":"\n# returns 1 randomly sampled number from 1 to 10\nsample(1:10,1)\n#> [1] 2\n\n# let's repeat the above 10 times using replicate\nreplicate(10,sample(1:10,1))\n#>  [1]  1  2  5 10  5  2  9  3  4  8\nttest_result <- function(){\n  sample <- rnorm(10,0,1)\n  t_out <- t.test(sample, mu=0)\n  return(t_out$statistic)\n}\n\n# get 10 t-values from repeating the above 10 times\nreplicate(10, ttest_result() )\n#>          t          t          t          t          t          t          t \n#>  1.3199503  0.5352204  1.0307364  0.7180495  1.8898355 -2.5935801  0.2701770 \n#>          t          t          t \n#> -0.3495035 -0.1420472  2.1285776"},{"path":"coding-reference.html","id":"apply-family","chapter":"Coding Reference","heading":"0.26.3 apply family","text":"apply family functions can used “apply” function across elements object. general overview can found hereSome apply functions include: apply(), lapply, sapply.","code":""},{"path":"coding-reference.html","id":"lapply-and-sapply","chapter":"Coding Reference","heading":"0.26.4 lapply and sapply","text":"part definition lapply help file:lapply returns list length X, element result applying FUN corresponding element X.Let’s see examples:Let’s apply function elements vector. keep things simple, function add 1 numberAn alternative syntax lapply sapply let’s define function want apply inside lapply sapply function.case, element vector some_numbers become x value function.","code":"\nsome_numbers <- c(1,2,3,4)\n\nadd_one <- function(x){\n  return(x+1)\n}\n\n# returns a list, containing the answers\nlapply(some_numbers, add_one)\n#> [[1]]\n#> [1] 2\n#> \n#> [[2]]\n#> [1] 3\n#> \n#> [[3]]\n#> [1] 4\n#> \n#> [[4]]\n#> [1] 5\n\n# unlists the list\nunlist(lapply(some_numbers,add_one))\n#> [1] 2 3 4 5\n\n# sapply does the unlisting for you\nsapply(some_numbers, add_one)\n#> [1] 2 3 4 5\nsome_numbers <- c(1,2,3,4)\n\nlapply(some_numbers, FUN = function(x){x+1})\n#> [[1]]\n#> [1] 2\n#> \n#> [[2]]\n#> [1] 3\n#> \n#> [[3]]\n#> [1] 4\n#> \n#> [[4]]\n#> [1] 5\nsapply(some_numbers, FUN = function(x){x+1})\n#> [1] 2 3 4 5"},{"path":"coding-reference.html","id":"apply","chapter":"Coding Reference","heading":"0.26.5 apply","text":"apply function can used 2-dimensional data, allows apply function across rows columns data.Let’s say 5x5 matrix random numbers. Let’s find sum rowThe sum columnLet’s say matrix storing 3 samples. sample 10 numbers. sample stored column, row represents observation.Let’s use apply conduct 10 one-sample t-tests, one column. example, can pass mu=0 parameter t.test function. However, return entire ouput t-test list.wanted return t-values, rather whole output?might try , doesn’t workSo, write custom function","code":"\nrandom_matrix <- matrix(sample(1:10,25, replace=TRUE),ncol=5)\n\n# applies the sum function to each row\n# 1 tells apply to go across rows\napply(random_matrix,1,sum)\n#> [1] 31 25 26 37 37\n# applies the sum function to each column\n# 2 tells apply to go across columns\napply(random_matrix, 2, sum)\n#> [1] 14 36 23 40 43\nsample_matrix <- matrix(rnorm(30,0,1),ncol=3)\napply(sample_matrix,2,t.test, mu=0)\n#> [[1]]\n#> \n#>  One Sample t-test\n#> \n#> data:  newX[, i]\n#> t = 0.034762, df = 9, p-value = 0.973\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.3706933  0.3822637\n#> sample estimates:\n#>   mean of x \n#> 0.005785176 \n#> \n#> \n#> [[2]]\n#> \n#>  One Sample t-test\n#> \n#> data:  newX[, i]\n#> t = -0.9425, df = 9, p-value = 0.3705\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.7877233  0.3243796\n#> sample estimates:\n#>  mean of x \n#> -0.2316719 \n#> \n#> \n#> [[3]]\n#> \n#>  One Sample t-test\n#> \n#> data:  newX[, i]\n#> t = 0.58176, df = 9, p-value = 0.575\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.5081706  0.8600290\n#> sample estimates:\n#> mean of x \n#> 0.1759292\napply(sample_matrix,2,t.test$statistic, mu=0)\n#> Error in t.test$statistic: object of type 'closure' is not subsettable\napply(sample_matrix, 2, \n      FUN = function(x){\n        t_out <- t.test(x,mu=0)\n        return(t_out$statistic)\n      })\n#> [1]  0.03476155 -0.94249931  0.58175659"},{"path":"textbooks-and-other-resources.html","id":"textbooks-and-other-resources","chapter":"Textbooks and Other Resources","heading":"Textbooks and Other Resources","text":"(yet…maybe one day knows) complete statistics textbook statistics R. series weekly exercises used labs statistics courses psychology students. aimed initiating novice students learning programming environment statistics like R, also using R teaching tool aid conceptual understanding statistics.","code":""},{"path":"textbooks-and-other-resources.html","id":"statistics-textbooks-we-are-using","chapter":"Textbooks and Other Resources","heading":"0.27 Statistics textbooks we are using","text":"Students taking course Brooklyn College also taking separate series bi-weekly lectures, arrive lab discussions digested readings. beginning lab refer readings assigned students, come three different textbooks:Vokey & Allen8, pdf available onlineAbdi, Edelman, Dowling, & Valentin9, portions may downloadable google scholar, otherwise try find printed copy somewhere.Crump, Navarro, & Suzuki10, https://crumplab.github.io/statistics/, lab manual R https://crumplab.github.io/statisticsLab/","code":""},{"path":"textbooks-and-other-resources.html","id":"other-online-textbooks","chapter":"Textbooks and Other Resources","heading":"0.28 Other online textbooks","text":"increasing numbers excellent, free, online resources learning statistics R, :Danielle Navarro’s Learning Statistics R website learning R R Psychological ScienceRussell Poldracks’s Statistical Thinking 21st CenturyMartin Speekenbrink’s Statistics: data analysis modelling, companion R book R companion Statistics: data analysis modellingInto python instead? Check Todd Gureckis’ Lab Cognition PerceptionLooking stats videos, check Erin Buchanan’s STATISTICS DOOM! youtube: https://www.youtube.com/channel/UCMdihazndR0f9XBoSXWqnYg","code":""},{"path":"textbooks-and-other-resources.html","id":"a-longer-list","chapter":"Textbooks and Other Resources","heading":"0.29 A longeR list","text":"Hadley Wickham written several fantastic free booksv keep coming back time, R Data Science, ggplot 2: elegant graphics data analyis, Advanced R, R packages.R markdown knitr core libraries using R create sorts reproducible documents pdfs websites. excellent resources:\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\nhttps://bookdown.org/yihui/rmarkdown/https://bookdown.org/yihui/rmarkdown-cookbook/Github got ? Jenny Bryan pick https://happygitwithr.comGoogling R questions can often turn example someone solving issue closely related one. example, can copy error messages google , ask “X R”.Stackoverflow great, Google often take someone already asked question, someone else answered, usually many people answered question many ways.Danielle Navarro recently made website introducing R, ’s great, check (also made using R markdown process): http://compcogscisydney.org/psyr/Check slightly older programming book also introduces R https://crumplab.github.io/programmingforpsych/, actually don’t , ’s old now worth .Another solid accessible resource psyc stats using R https://ademos.people.uic.edu/index.html.https://cran.r-project.org/doc/contrib/Short-refcard.pdf link takes reference card, shows big long list intrinsic r functions.really great really long list resources R! https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/’s bunch R markdown tricks right https://holtzy.github.io/Pimp--rmd/.","code":""},{"path":"r-basics.html","id":"r-basics","chapter":"1 R Basics","heading":"1 R Basics","text":"“8/27/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"r-basics.html","id":"reading-and-walkthrough-video","chapter":"1 R Basics","heading":"1.1 Reading and walkthrough video","text":"Vokey & Allen,11 Chapter 1, available online : http://people.uleth.ca/~vokey/pdf/thinking.pdf.","code":""},{"path":"r-basics.html","id":"overview","chapter":"1 R Basics","heading":"1.2 Overview","text":"labs course designed give students exposure free open-source statistical programming language R. assumption students may zero prior experience scripting, coding, computer programming. semester use R tool data-analysis, tool sharpen understanding statistical concepts.starting lab follow getting started instructions install R, R-studio, create Github.com account, download github Desktop; , make sure test github pipeline. Throughout semester posting assignments Github.com, submitting links repositories blackboard.","code":""},{"path":"r-basics.html","id":"problem-1-summing-1-to-100","chapter":"1 R Basics","heading":"1.3 Problem 1: Summing 1 to 100","text":"Vokey & Allen12 tell story teacher giving challenging students add numbers 1 100. supposed stump one students, young Gauss. Apparently, , Gauss quickly wrote sum 5050.Gauss didn’t R, , can solve problem quickly :Use R find sum sequence numbers 1 100:Using sum(), simple fast solve Gauss’s problem R. easily find sums changing 1 100There many details going behind scenes R allow sum() function work. One detail get R create sequence numbers, another take action like adding numbers sequence. Two major concepts variables storing information (like sequences numbers), functions take actions transform input (number sequence) desired output (sum number sequence).","code":"\nsum(1:100)\n#> [1] 5050\nsum(5:10)\n#> [1] 45\nsum(100:200)\n#> [1] 15150"},{"path":"r-basics.html","id":"r-basics-background","chapter":"1 R Basics","heading":"1.4 R Basics Background","text":"","code":""},{"path":"r-basics.html","id":"creating-sequences-of-numbers-in-r","chapter":"1 R Basics","heading":"1.4.1 Creating sequences of numbers in R","text":"multiple ways create number sequences R. sequence integers can generated x:y, x starting value, y ending value.","code":"\n1:5\n#> [1] 1 2 3 4 5\n1:10\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n5:-5\n#>  [1]  5  4  3  2  1  0 -1 -2 -3 -4 -5"},{"path":"r-basics.html","id":"seq","chapter":"1 R Basics","heading":"1.4.2 seq()","text":"Sequences incremented constant value can created using seq() function. Look “help” R function typing ?name_of_function consoleR comes pre-packaged many functions like seq(), can write functions, download libraries functions people written extend base functionality R. look closely functions throughout semester.R function usually three components. 1) receive kind input, 2) “something”, 3) return kind output. R, use functions writing name function parentheses name(). function takes inputs, define inputs inside parentheses name(x=1). Functions can multiple inputs, separated commas.Let’s take look using seq() function generate sequences numbers.","code":"\n?seq\n#lines beginning with # are comments and not run\n\n#seq(from, to)\nseq(from = 1, to = 5)\n#> [1] 1 2 3 4 5\nseq(1, 5)\n#> [1] 1 2 3 4 5\n\n#seq(from, to, by= )\nseq(from = 1, to = 5, by = 2)\n#> [1] 1 3 5\nseq(1, 5, .5)\n#> [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nseq(1, 10, 2)\n#> [1] 1 3 5 7 9\n\n#seq(from, to, length.out= )\nseq(from = 1, to = 2, length.out =5)\n#> [1] 1.00 1.25 1.50 1.75 2.00\n\nseq(5)\n#> [1] 1 2 3 4 5"},{"path":"r-basics.html","id":"problem-2-summing-any-constant-series","chapter":"1 R Basics","heading":"1.5 Problem 2: Summing any constant series","text":"Now seen sum() seq() functions, able use find sum constant series.example, find sum series 100 200, going five.also possible write analytic formula R, compare results, remember :\\(X_1 + X_2 + \\ldots + X_n = (\\frac{X_n-X_1}{c}+1)(\\frac{X_1+X_n}{2})\\)\\(X_1\\) starting value, \\(X_n\\) ending value \\(c\\) constant step value.example writing formula R. create variables names X1, Xn, step, assign (<-) value want. compute formula. give value previous example, sequence.","code":"\nsum( seq(100,200,5) )\n#> [1] 3150\nX1 <- 100\nXn <- 200\nstep <- 5\n\n(((Xn - X1)/step) + 1) * ((X1 + Xn)/2)\n#> [1] 3150\n\n( ( (Xn-X1)/step ) + 1 ) * ( (X1+Xn)/2 )\n#> [1] 3150"},{"path":"r-basics.html","id":"vectors-1","chapter":"1 R Basics","heading":"1.6 Vectors","text":"","code":""},{"path":"r-basics.html","id":"the-gaussian-trick","chapter":"1 R Basics","heading":"1.6.1 The Gaussian trick","text":"Remember Gauss added numbers 1 100 imagining two number lines:noted sum columns always added 101 (e.g., 1+100 = 101, 2+99 = 101, etc.). possible demonstrate R can directly add number lines:time created sequence numbers (e.g., 1 100) creating object called numeric vector R. Vectors can store multiple numbers.Vectors kind variable R. Vectors can names, can saved, can manipulated. Let’s take quick look vectors:name new vector. <- called assignment operator. 1:5 creates vector length 5, containing sequence numbers 1 5. plain language, “assign” object right (1:5), “name” written left side <-. words, put numbers 1 5 something called .create variable like , name appear global environment (top right environment tab). variable created first time (executing code console), becomes registered saved computer’s memory current R session.want double-check variable exists memory, enter name console, press enter:can clear (remove) variable using rm(). , can clear entire global environment using Session > Clear Workspace... RStudio menu.can also check class variable R using class() function. Let’s create two vectors check classes. vector contains integer class, b vector contains numeric class decimal values.","code":"\n1:100\n#>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n#>  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n#>  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n#>  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n#>  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n#>  [91]  91  92  93  94  95  96  97  98  99 100\n100:1\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n#>  [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n#>  [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n#>  [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n#>  [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n#>  [91]  10   9   8   7   6   5   4   3   2   1\n1:100 + 100:1\n#>   [1] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n#>  [19] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n#>  [37] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n#>  [55] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n#>  [73] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n#>  [91] 101 101 101 101 101 101 101 101 101 101\n#creates a variable a\na <- 1:5\na\n#> [1] 1 2 3 4 5\nrm(a)\na <- 1:10\nclass(a)\n#> [1] \"integer\"\n\nb <- seq(1,2,.25)\nclass(b)\n#> [1] \"numeric\""},{"path":"r-basics.html","id":"c","chapter":"1 R Basics","heading":"1.6.2 c()","text":"also possible create vectors contain characters, rather numbers. illustrate example character vector, introduce one basic R function c(), short “combine”.Vectors like trains, slots (train cars) can contain things, like passengers, oil coal. Trains can number train cars, just like vector can length (number slots).Importantly, train cars need connected together form whole train. Similarly, R, make vector need concatenate connect individual slots vector together. c() function . combines individual units together form vector.use c(), insert individual items separated commas parentheses. insert characters, wrap character (string characters) quotations.elements commas like individual train cars, c() function connects elements together single entity comprised multiple units, like train, called vector.","code":"\n?c\nletters <- c(\"a\",\"b\",\"c\")\nnumbers <- c(1,2,3)\nnumbers_as_chars <- c(\"1\",\"2\",\"3\")\nwords <- c(\"this\",\"is\",\"a\",\"vector\",\"of\",\"strings\")"},{"path":"r-basics.html","id":"length","chapter":"1 R Basics","heading":"1.6.3 length()","text":"Just like train specific number cars, vector specific number slots. called length vector. R length() function reports length vector.","code":"\nlength(letters)\n#> [1] 3\nlength(words)\n#> [1] 6"},{"path":"r-basics.html","id":"more-on-combining","chapter":"1 R Basics","heading":"1.6.4 More on combining","text":"c() function flexible can combine sorts elements, including vectors variables.Remember vectors different classes depending kind elements inside vector. important, R requires elements vector class.possible combine vectors start different classes, R may give error, convert one class another. go back train car analogy, R doesn’t like trains different kinds cars…wants whole train passenger cars, whole train oil tankers.","code":"\nsome_numbers <- c(1, 2, 3, 1:5)\n\nsome_characters <- c(letters, words)\nclass(c(1,2,3))\n#> [1] \"numeric\"\nclass(c(\"A\",\"B\",\"C\"))\n#> [1] \"character\"\nclass(c(TRUE,FALSE,TRUE))\n#> [1] \"logical\"\n# the numbers are converted to characters\nc(1,2,3,\"a\",\"b\",\"c\")\n#> [1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\""},{"path":"r-basics.html","id":"indexing-a-vector","chapter":"1 R Basics","heading":"1.6.5 Indexing a vector","text":"Vector indexing iallows elements inspected changed. like train, inspect contents cars 3 5, unload car 7 put something else .Square bracket [] notation indexes vector, variable_name[x]; , x another vector specifying indexed slots.following examples use [] index specific elements vector . outcome elements specified inside square brackets printed console.also possible assign new values specific elements vector:","code":"\na <- c(1,6,3,2,8,9) # make a vector\na[1] # first element\n#> [1] 1\na[2] # second element\n#> [1] 6\na[1:3] # 1st to 3rd elements\n#> [1] 1 6 3\na[c(1,5)] # elements 1 and 5\n#> [1] 1 8\n# assign 100 to the first slot of a\na[1] <- 100\na\n#> [1] 100   6   3   2   8   9\n\n# assign the value 1 to slots 5 to 6 of a\na[5:6] <- 1\na\n#> [1] 100   6   3   2   1   1"},{"path":"r-basics.html","id":"growing-a-vector","chapter":"1 R Basics","heading":"1.6.6 Growing a vector","text":"upcoming labs use vectors store information. Sometimes know advance many slots need vector, times might know, instead decide build vector one slot time.begin empty (NULL) vector. use c() command, don’t combine anything together. like starting train cars .can add slot vector combining new element existing variable. combine 1, assign result back , replacing ’s original NULL value.keep , keep adding 1s, end .Consider alternative method growing vector:","code":"\na <- c()\na\n#> NULL\na <- c(a,1)\na\n#> [1] 1\na <- c(a,1)\na\n#> [1] 1 1\na <- c(a,1) # c(1,1,1)\na\n#> [1] 1 1 1\na <- c(a,1)\na\n#> [1] 1 1 1 1\na <- c(a,1)\na\n#> [1] 1 1 1 1 1\na <- c()\na\n#> NULL\n\na[1] <- 1\na\n#> [1] 1\n\na[2] <- 1\na\n#> [1] 1 1\n\na[3] <- 1\na\n#> [1] 1 1 1\n\na[10] <- 1\na\n#>  [1]  1  1  1 NA NA NA NA NA NA  1"},{"path":"r-basics.html","id":"problem-3-writing-a-sum-function-in-r","chapter":"1 R Basics","heading":"1.7 Problem 3: Writing a sum function in R","text":"already used R solve problems Chapter 1 Vokey & Allen (2018). can create sequences numbers, can create custom vectors numbers, can use sum() function find sum. However, haven’t discussed sum() function actually works. R know find sum?example writing sum function R. example involves understanding loops writing custom functions, explained next sections.","code":"\nmy_sum <- function(x) {\n  sum <- 0\n  for(i in x) sum <- sum + i\n  return(sum)\n}\n\nmy_sum(1:100)\n#> [1] 5050"},{"path":"r-basics.html","id":"algorithms","chapter":"1 R Basics","heading":"1.8 Algorithms","text":"understand functions like sum() work R, need understand general concept algorithm. ’ll define algorithm recipe, series steps/actions result particular outcome. scripting language like R, possible define algorithms infallible. , given input, always apply steps arrive answer demanded algorithm.sum series numbers head, say numbers 1 5, probably applying simple algorithm describe like :Take first number add second (1+2 = 3)Take sum (3) add next number (3+3 = 6)Repeat step 2 numbers series6+4 = 1010+5 = 15report final sum (15)Consider look R. one example producing algorithm R. Everytime run script, always end sum 15, answer demanded series steps wrote .","code":"\nsum(1:5)\n#> [1] 15\na <- 1:5\na\n#> [1] 1 2 3 4 5\n\nthe_sum <- a[1]+a[2]\nthe_sum\n#> [1] 3\nthe_sum <- the_sum + a[3]\nthe_sum\n#> [1] 6\nthe_sum <- the_sum + a[4]\nthe_sum\n#> [1] 10\nthe_sum <- the_sum + a[5]\nthe_sum\n#> [1] 15"},{"path":"r-basics.html","id":"loops-1","chapter":"1 R Basics","heading":"1.9 Loops","text":"example shows algorithm written hand R. tiresome write sum sequence many numbers. Fortunately, ways automate repetitive processes R. common method repeating commands R use loop.Check R help Control Flow ?Control.(){}\n(loop control){something iteration}basic syntax loops follows:Loop control defined parentheses. name iterator placed left (can assigned name want, need declared advance). execution loop, iterator takes values inside vector placed right side . Specifically, following happening.Loop steps:\n1. iterator <- vector[1]\n2. iterator <- vector[2]\n3. iterator <- vector[3]\n4. etc.loop automatically stop reaches last item vector. loop can stopped using break command.","code":"\nfor(iterator in vector){\n  #do something\n}\n# Make a loop do something 5 times\n# i is the iterator\n# 1:5 creates a vector with 5 numbers in it, 1, 2, 3, 4, 5\n# the loop will run 5 times, because there are five things to assign to i\nfor(i in 1:5){\n  print(\"hello\")\n}\n#> [1] \"hello\"\n#> [1] \"hello\"\n#> [1] \"hello\"\n#> [1] \"hello\"\n#> [1] \"hello\"\n# show the value of i each step of the loop\nfor(i in 1:5){\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# define the vector to loop over in advance\nmy_sequence <- 1:5\nfor(i in my_sequence){\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# Reminder that i becomes the next value in the vector\n# your vector can have any order \nmy_sequence <- c(1,5,2,3,4)\nfor(i in my_sequence){\n  print(i)\n}\n#> [1] 1\n#> [1] 5\n#> [1] 2\n#> [1] 3\n#> [1] 4\n# index vector does not need to be numbers\nmy_things <- c(\"A\",\"B\",\"C\",\"D\")\nfor(i in my_things){\n  print(i)\n}\n#> [1] \"A\"\n#> [1] \"B\"\n#> [1] \"C\"\n#> [1] \"D\""},{"path":"r-basics.html","id":"breaking-a-loop-1","chapter":"1 R Basics","heading":"1.9.1 Breaking a loop","text":"break stops loop. Used logical statements define conditions necessary cause break.","code":"\nfor(i in 1:10){\n  if(i <= 5){\n    print(i)\n  } else {\n    break\n  }\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5"},{"path":"r-basics.html","id":"while-loops-1","chapter":"1 R Basics","heading":"1.9.2 While loops","text":"loops run logical condition met. iterator, just logic statement needs met.one prints less 6. soon becomes “less 6”, loop stops. Critically, inside loop, value increases iteration.","code":"\ni <- 1 # create an variable\nwhile (i < 6) {\n  print(i)\n  i = i+1 #add one each step of the loop\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5"},{"path":"r-basics.html","id":"repeat-loops-1","chapter":"1 R Basics","heading":"1.9.3 Repeat loops","text":"Similar , let’s things condition met.","code":"\ni<-0\nrepeat{\n  i<-i+1\n  print(i)\n  if(i==5){\n    break\n  }\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5"},{"path":"r-basics.html","id":"examples-1","chapter":"1 R Basics","heading":"1.9.4 Examples","text":"Braces needed one lineUsing value iterator assign values systematically another variable.Using loop add numbers vector.example shows use loop compute sum numbers vector . example using loop algorithm. end checked custom script sum() function, see arrived answer.final task lab take look R functions, learn write basic functions like sum().","code":"\nfor(i in 1:5) print(i)\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# put 1 into the first five positions of x\nx <- c() # create empty vector\nfor(i in 1:5){\n  x[i] <- 1  # assign 1 to the ith slot in x\n}\nx\n#> [1] 1 1 1 1 1\n\n\n# put the numbers 1-5 in the first 5 positions of x\nx <-c()\nfor(i in 1:5){\n  x[i] <- i\n}\nx\n#> [1] 1 2 3 4 5\na <- c(10,20,30,40,50) #some numbers\n\nthe_sum <- 0 # initialize a variable that will keep track of the sum\n\nfor (i in a) {\n the_sum <- the_sum + i  \n}\n\nthe_sum\n#> [1] 150\nsum(a) #check against the sum function\n#> [1] 150"},{"path":"r-basics.html","id":"functions-1","chapter":"1 R Basics","heading":"1.10 Functions","text":"Functions re-useable algorithms. example, rather re-writing code necessary compute sum everytime want find sum, instead store necessary code inside named variable called sum(), “call” function writing name providing inputs.fairly straightforward write custom functions R, learning write functions excellent method improve understanding R fundamentals.","code":""},{"path":"r-basics.html","id":"function-syntax-1","chapter":"1 R Basics","heading":"1.10.1 function syntax","text":"general syntax writing functions:","code":"\nfunction_name <- function(input1,input2){\n  #code here\n  return(something)\n}"},{"path":"r-basics.html","id":"example-functions-1","chapter":"1 R Basics","heading":"1.10.2 Example functions","text":"function input (). Whenever run function, simply return whatever placed inside return statement.function simply takes input, returns input without modifying .function takes input, creates internal variable called temp assigns input+1. contents temp returned. Note , checking input, return erro input character (can’t add one character R)function adds input checking. add one input numeric type. Otherwise, use stop() return error message consoleA function three inputs","code":"\n# define the function\nprint_hello_world <- function(){\n  return(print(\"hello world\"))\n}\n\n# use the function\nprint_hello_world()\n#> [1] \"hello world\"\nreturn_input <- function(input){\n  return(input)\n}\n\n# the variable input is assigned a 1\n# then we return(input), which will result in a 1\n# because the function internally assigns 1 to the input\nreturn_input(1)\n#> [1] 1\na <- \"something\"\nreturn_input(a)\n#> [1] \"something\"\nadd_one <- function(input){\n  temp <- input+1\n  return(temp)\n}\nadd_one(1)\n#> [1] 2\nadd_one(\"a\") #this will cause an error\n#> Error in input + 1: non-numeric argument to binary operator\nadd_one <- function(input){\n  if(class(input) == \"numeric\"){\n    temp <- input+1\n    return(temp)\n  } else {\n    return(stop(\"input must be numeric\"))\n  }\n}\nadd_one(1)\n#> [1] 2\nadd_one(\"a\")\n#> Error in add_one(\"a\"): input must be numeric\nadd_multiply <- function(input, x_plus, x_times){\n  temp <- (input+x_plus)*x_times\n  return(temp)\n}\n\n# input is 1\n# x_plus <- 2\n# x_times <- 3\n# will return (1+2)*3 = 9\nadd_multiply(1,2,3)\n#> [1] 9"},{"path":"r-basics.html","id":"lab-1-generalization-assignment","chapter":"1 R Basics","heading":"1.11 Lab 1 Generalization Assignment","text":"Follow instructions complete assignment lab 1 hand due date blackboard. first lab taken extra step pretending student course, completed first lab . next video shows complete lab student. important try solve problems , please use video resource help get stuck.","code":""},{"path":"r-basics.html","id":"instructions","chapter":"1 R Basics","heading":"1.11.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Make new R project (initialized git repository) called “StatsLab1’.Create new R Markdown document called “Lab1.Rmd”Upload StatsLab1 R project Github.com using Github DesktopUse Lab1.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.Submit github repository link Lab 1 blackboard.six problems solve, worth 1 point.Refer getting started videos examples creating new R project uploading Github. problems steps resolved first class, please email , create issue course github page https://github.com/CrumpLab/psyc7709Lab/issues","code":""},{"path":"r-basics.html","id":"problems","chapter":"1 R Basics","heading":"1.11.2 Problems","text":"Compute sum sequence 100 1000, going constant value 100 (100,200,300,400,500,600,700,800,900,1000).Compute sum numbers (1,3,2,4,3,5,4,3,4,5,6,5,6,7,6,5,6,5,4,3,4,5)Write custom sequence generator function using loop generates sequence starting integer value ending integer value steps 1. Demonstrate can produce sequence 1 10.Write custom function implement following general equation find sum constant series:\\(X_1 + X_2 + \\ldots + X_n = (\\frac{X_n-X_1}{c}+1)(\\frac{X_1+X_n}{2})\\)Demonstrate function correctly produces sum series :Write custom function generates constant series start end values, constant, finds sum. function output sequence sum. problem, feel free use existing seq() sum() functions custom function. Demonstrate function correctly prints sequence (10 100 steps 10), sum.Use sum() length() functions calculate mean (average) numbers x = c(1,2,3,4,5).","code":"\nseq(10,100,10)\n#>  [1]  10  20  30  40  50  60  70  80  90 100"},{"path":"descriptives.html","id":"descriptives","chapter":"2 Descriptives","heading":"2 Descriptives","text":"“9/2/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"descriptives.html","id":"reading","chapter":"2 Descriptives","heading":"2.1 Reading","text":"13, Chapters 2 & 3 descriptive statistics, including measures central tendency (e.g., means) dispersion (variances); Crump, Navarro, & Suzuki14 chapter 2 describing data; /Abdi, Edelman, Dowling, & Valentin15, appendix .","code":""},{"path":"descriptives.html","id":"overview-1","chapter":"2 Descriptives","heading":"2.2 Overview","text":"General note, trying find consistent way structure lab content. lab adopt structure splits lab conceptual practical parts. conceptual sections use R demonstrate reinforce concepts statistics. practical sections use R data analysis.Conceptual Section : Using R demonstrate properties meanPractical Sections II:\nimporting data\ncalculating means descriptive statistics\ngraphing means ggplot2\nimporting datacalculating means descriptive statisticsgraphing means ggplot2","code":""},{"path":"descriptives.html","id":"means-demo","chapter":"2 Descriptives","heading":"2.3 Means Demo","text":"quick piece example code showing steps calculate graph means data set R using tidyverse. need install tidyverse libraries system can run code :","code":"\ninstall.packages(\"tidyverse\")\n# load libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# chickwts is a built in data set\n# with chick weights by feed\n\n# calculate means for each feed type\nmeans_df <- chickwts %>%\n  group_by(feed) %>%\n  summarize(means = mean(weight))\n\n# print table of means\nknitr::kable(means_df)\n\n# plot the means\nggplot(means_df, aes(x=feed, y=means)) +\n  geom_bar(stat=\"identity\")"},{"path":"descriptives.html","id":"base-r-descriptive-statistics-functions","chapter":"2 Descriptives","heading":"2.3.1 Base R descriptive statistics functions","text":"Base R comes functions many common descriptive statistics. general, functions take vector number input, return statistic output. examples computing statistics sequence integers 1 10.time, base R existing functions every descriptive statistic, custom descriptive statistics might want make . example, mode function. find another package mode function, write .","code":"\na <- 1:10\n\nmean(a) # arithmetic mean\n#> [1] 5.5\nmedian(a) # median\n#> [1] 5.5\nsd(a) # sample standard deviation (n-1)\n#> [1] 3.02765\nvar(a) # sample variance (n-1)\n#> [1] 9.166667"},{"path":"descriptives.html","id":"concepts-i-demonstrating-properties-of-the-arithmetic-mean-in-r","chapter":"2 Descriptives","heading":"2.4 Concepts I: Demonstrating properties of the arithmetic mean in R","text":"section :use built mean() functionwrite mean functionconduct simulation R demonstrate mean number causes sum deviations mean equal zero.can use mean() function calculate arithmetic means R. examples show calculating means different vectors inputted mean function.order calculate means need set numbers start . research context, sets numbers data points research project. , learn throughout course input real data, manipulate , calculate descriptive inferential statistics. However, section, use R create sets numbers, rather input data.","code":"\nmean(1:10)\n#> [1] 5.5\nmean(c(1,2,3))\n#> [1] 2\nmean(chickwts$weight)\n#> [1] 261.3099"},{"path":"descriptives.html","id":"arithmetic-mean","chapter":"2 Descriptives","heading":"2.4.1 Arithmetic Mean","text":"mean sum numbers, divided number numbers:\\(\\bar{X} = \\frac{\\sum_i^n{Xi}}{n}\\)wanted break steps R compute mean sequence 1 10, look like :","code":"\na <- 1:10 # create a vector\nsum_a <- sum(a) # store the sum\nlength_a <- length(a) # store the length (n)\nmean_a <- sum(a)/length(a) # store the sum/n\nmean_a # report the mean\n#> [1] 5.5"},{"path":"descriptives.html","id":"writing-a-custom-mean-function","chapter":"2 Descriptives","heading":"2.4.2 Writing a custom mean function","text":"write custom version mean function? examples:example “long” writes step computing mean line. Sometimes desirable make steps function clear easy follow. also possible sometimes desirable rewrite function accomplishes steps short number lines code. show example systematically rewriting function make take fewer lines code. see return() statement necessary, {} necessary function can written one line.","code":"\n# A long-form mean function that shows each step\nmy_mean <- function(x) {\n  sum_x    <- sum(x) # store the sum\n  length_x <- length(x) # store the length (n)\n  mean_x   <- sum_x/length_x # divide and store mean\n  return(mean_x) # output the mean\n}\n\nmy_mean(1:10)\n#> [1] 5.5\n# return() is not necessary if the function ends with a\n# variable name being printed\nmy_mean <- function(x) {\n  sum_x    <- sum(x) # store the sum\n  length_x <- length(x) # store the length (n)\n  mean_x   <- sum_x/length_x # compute and store mean\n  mean_x # output the mean\n}\n\nmy_mean(1:10)\n#> [1] 5.5\n\n# end with the mean computation\nmy_mean <- function(x) {\n  sum_x <- sum(x) # store the sum\n  length_x <- length(x) # store the length (n)\n  sum_x/length_x # compute and output the mean\n}\n\nmy_mean(1:10)\n#> [1] 5.5\n\n# no intermediate saving of sum or length\nmy_mean <- function(x) {\n  sum(x)/length(x) # compute and output the mean\n}\n\nmy_mean(1:10)\n#> [1] 5.5\n\n# one-liner\nmy_mean <- function(x) sum(x)/length(x)\n\nmy_mean(1:10)\n#> [1] 5.5"},{"path":"descriptives.html","id":"demonstrate-that-the-mean-is-the-point-from-which-the-sum-of-the-deviations-is-0.","chapter":"2 Descriptives","heading":"2.4.3 Demonstrate that the mean is the point from which the sum of the deviations is 0.","text":"learned mean set numbers point sum deviations equal 0. problem use R demonstrate property mean. can ?Let’s break steps. need numbers:need able compute sum deviations. wish compute differences one number (going mean), numbers set. deviations. want add arrive sum deviations.can find deviations numbers scores, number choice? Let’s pick number 5. turns can simply subtract 5 scores. produce vector differences deviationsThen, can find sum:Notice sum deviations (numbers scores 5) equal 0. , Clearly, mean numbers must 5. mean value cause deviations sum 0.","code":"\nscores <- c(1,64,5,4,3,4,5,6,7,8,3)\nscores-5\n#>  [1] -4 59  0 -1 -2 -1  0  1  2  3 -2\nsum(scores-5)\n#> [1] 55"},{"path":"descriptives.html","id":"simulations-to-approximate-the-mean","chapter":"2 Descriptives","heading":"2.4.4 Simulations to approximate the mean","text":"quickly show mean scores causes deviations sum 0. example, mean 10, sum deviations scores mean 0.Instead, let’s use R approximate value mean trying different values, rather computing mean first. example, previously tried 5, sum deviations 0. tried bunch numbers, say lowest number (1) highest number (64)? time compute deviations, sum deviations. record outcome test. look results see numbers comes closest creating sum deviations equals 0. require 64 individual tests, much hand. Fortunately, can use R accomplish goals quickly.Basically, process want …don’t want write 64 lines code either…want repeatedly apply similar computation R, remember can use loop. test integers smallest value largest value. , first create sequence numbers called numbers_to_test. , use loop compute sum deviations scores test numbers.just computed first simulation produced 64 sums deviations, displayed . question approximate mean, finding value produces sum deviations closest 0. final step evaluate results simulation answer question.Let’s discuss three ways get answer. First, look sums see 10th value 0, means number 10 produced sum deviations closest 0 (equaling 0 10 mean).","code":"\nscores # the scores\n#>  [1]  1 64  5  4  3  4  5  6  7  8  3\n\nmean(scores) # the mean\n#> [1] 10\n\nscores-mean(scores) #the vector of deviations from the mean\n#>  [1] -9 54 -5 -6 -7 -6 -5 -4 -3 -2 -7\n\nsum(scores-mean(scores)) # the sum\n#> [1] 0\nsum(scores-1)\n#> [1] 99\nsum(scores-2)\n#> [1] 88\nsum(scores-3)\n#> [1] 77\nsum(scores-4)\n#> [1] 66\nsum(scores-5)\n#> [1] 55\nsum(scores-6)\n#> [1] 44\n## and so on to 64\nmin(scores) \n#> [1] 1\nmax(scores)\n#> [1] 64\nnumbers_to_test <- min(scores):max(scores)\n\nsum_deviations <-c() # create an empty vector to store sums\nfor(i in numbers_to_test) {\n  sum_deviations[i] <- sum(scores-i)\n}\n\nsum_deviations\n#>  [1]   99   88   77   66   55   44   33   22   11    0  -11  -22  -33  -44  -55\n#> [16]  -66  -77  -88  -99 -110 -121 -132 -143 -154 -165 -176 -187 -198 -209 -220\n#> [31] -231 -242 -253 -264 -275 -286 -297 -308 -319 -330 -341 -352 -363 -374 -385\n#> [46] -396 -407 -418 -429 -440 -451 -462 -473 -484 -495 -506 -517 -528 -539 -550\n#> [61] -561 -572 -583 -594"},{"path":"descriptives.html","id":"plotting-the-results","chapter":"2 Descriptives","heading":"2.4.5 Plotting the results","text":"Second, visualize sum deviations. quickly plot() function.looking value x-axis causes sum deviations y-axis closest 0. One way help locate value look absolute values. Absolute values remove negative (-) sign numbers, leaving everything positive. can convert absolute values R using abs()Now, easy see 10 produces sum deviations closest 0 (case produces value 0, 10 also mean).","code":"\nplot(sum_deviations)\nplot(abs(sum_deviations))"},{"path":"descriptives.html","id":"locating-the-result-using-which","chapter":"2 Descriptives","heading":"2.4.6 Locating the result using which()","text":"Finally, use R functions help us compute answer. example, use () function. function can used determine indices position “logically” obtained values vector. long way saying want R tell us position vector contains 0","code":"\n#find the positions in vector that equal 0\nwhich(sum_deviations == 0)\n#> [1] 10\n\n#show the value in position 10\nsum_deviations[10]\n#> [1] 0\n\n#all in one\nsum_deviations[ which(sum_deviations == 0) ]\n#> [1] 0"},{"path":"descriptives.html","id":"minor-details","chapter":"2 Descriptives","heading":"2.4.7 Minor details","text":"approach worked pretty well. simulation process, found 10 produced sum deviations equaled 0. None numbers produced sum deviations equaled 0. However, example code won’t work well sets numbers.example, used set number 1 20, mean 10.5, find none sums deviations ever get 0 (’s testing integers 1 20). , () function never find number exactly equal 0., need modify comparison () function return “approximate” numbers give sums deviations closest zero.","code":"\n\nscores <- 1:20\nnumbers_to_test <- min(scores):max(scores)\n\nsum_deviations <-c() # create an empty vector to store sums\nfor(i in numbers_to_test) {\n  sum_deviations[i] <- sum(scores-i)\n}\n\nsum_deviations\n#>  [1]  190  170  150  130  110   90   70   50   30   10  -10  -30  -50  -70  -90\n#> [16] -110 -130 -150 -170 -190\nwhich(sum_deviations == 0)\n#> integer(0)\nwhich(abs(sum_deviations) == min(abs(sum_deviations)))\n#> [1] 10 11"},{"path":"descriptives.html","id":"advanced-example-writing-a-function-for-our-demonstration","chapter":"2 Descriptives","heading":"2.4.8 Advanced Example: Writing a function for our demonstration","text":"example show simulation written function. function used approximate mean set scores, test values.","code":"\napproximate_mean <- function(scores,test_sequence){\n  sum_deviations <- c()\n  for(i in 1:length(test_sequence)){\n    sum_deviations[i] <- sum(scores-test_sequence[i])\n  }\n  locate_index <- which(abs(sum_deviations) == min(abs(sum_deviations)))\n  test_sequence[locate_index]\n}\n\na <- c(1,4,3,2,4,3,5,4,6,5,7,6,8,7,9,8,7,6,7,6,5)\nb <- seq(0,10,.1)\napproximate_mean(a,b)\n#> [1] 5.4\n\n#actual mean\nmean(a)\n#> [1] 5.380952"},{"path":"descriptives.html","id":"practical-i-inputting-real-data-and-calculating-descriptive-statistics-with-tidyverse","chapter":"2 Descriptives","heading":"2.5 Practical I: Inputting real data and calculating descriptive statistics with tidyverse","text":"practical examples gloss many important details cover throughout semester, example code enough get started. following tidyverse approach, minimally involves\n1. Importing data data.frame similar (tibble, data.table)\n2. “Wrangling” data desired format analysis\n3. Applying calculations data","code":""},{"path":"descriptives.html","id":"importing-data","chapter":"2 Descriptives","heading":"2.5.1 Importing Data","text":"First, need “real data” import. Download zip file. contains folder open_data, several data files taken published psychology papers (whose authors made data publicly available).order follow along example, unzip file, copy move open_data folder R project folder. open_data folder R project folder .Rmd document using lab.use gapminder data set, includes measures life expectancy, population, gdp per capita, function year, country, continent. First, load gapminder.csv file (csv stands comma separated value) using read.table() function.many ways accomplish goal importing data R. discuss options throughout course.","code":"\ngapminder_data <- read.table(\"open_data/gapminder.csv\", \n                             sep = \",\",\n                             header = TRUE )"},{"path":"descriptives.html","id":"calculating-means","chapter":"2 Descriptives","heading":"2.5.2 Calculating means","text":"mean measures?","code":"\nmean(gapminder_data$lifeExp)\n#> [1] 59.47444\nmean(gapminder_data$pop)\n#> [1] 29601212\nmean(gapminder_data$gdpPercap)\n#> [1] 7215.327"},{"path":"descriptives.html","id":"dplyr-group_by-then-summarize","chapter":"2 Descriptives","heading":"2.5.3 dplyr (group_by then summarize)","text":"can calculate means separately function factors year, country, continent?use dplyr library (part tidyverse), three things:group data levels factor (case, separate data different continents, using continent variable)summarize data using mean functionoutput table resultsChange name variable inside group_by() change factor want evaluate (e.g., enter year, country). can include multiple factors:Note, group_means now contains large data frame, used head() function printing table, prints first five rows table, useful previewing large tables.","code":"\nlibrary(dplyr)\n\ngroup_means <- gapminder_data %>%\n                group_by(continent) %>%\n                summarize(mean_lifeExp = mean(lifeExp))\n\nknitr::kable(group_means)\ngroup_means <- gapminder_data %>%\n                group_by(continent,country) %>%\n                summarize(mean_lifeExp = mean(lifeExp))\n\nknitr::kable(head(group_means))"},{"path":"descriptives.html","id":"more-dplyr-examples","chapter":"2 Descriptives","heading":"2.5.4 More dplyr examples","text":"possible add multiple functions inside summarize. calculate mean, median, standard deviation, variance Life Expectancy function continent.Note, can even write custom functions use inside summarize.","code":"\ngroup_means <- gapminder_data %>%\n                group_by(continent) %>%\n                summarize(mean_lifeExp = mean(lifeExp),\n                          median_lifeExp = median(lifeExp),\n                          sd_lifeExp = sd(lifeExp),\n                          var_lifeExp = var(lifeExp))"},{"path":"descriptives.html","id":"practical-ii-plotting-the-means-with-ggplot2","chapter":"2 Descriptives","heading":"2.6 Practical II: Plotting the means with ggplot2","text":"ggplot2 powerful library plotting graphing R. many details using ggplot2 learn throughout semester. Today, create bar plots line graphs displaying means error bars.","code":""},{"path":"descriptives.html","id":"bar-graph-1","chapter":"2 Descriptives","heading":"2.6.1 Bar graph","text":"options modifying plot:","code":"\ngroup_means <- gapminder_data %>%\n                group_by(continent) %>%\n                summarize(mean_lifeExp = mean(lifeExp))\n\nggplot(group_means, aes(x = continent, y = mean_lifeExp))+\n  geom_bar(stat=\"identity\")\nggplot(group_means, aes(x = continent, y = mean_lifeExp))+\n  geom_bar(stat=\"identity\")+\n  ylab(\"Mean Life Expectancy\")+\n  xlab(\"Continent\")+\n  theme_classic()+\n  ggtitle(\"Mean Life Expectancy by Continent\")"},{"path":"descriptives.html","id":"line-graph-1","chapter":"2 Descriptives","heading":"2.6.2 Line graph","text":"\nNote get message adjusting group aesthestic, even though asked ggplot draw lines connecting dots, get line graph. good example unexpected behavior often requires quick google. copied message google found setting group=1 produced lines expecting:","code":"\nggplot(group_means, aes(x = continent, y = mean_lifeExp))+\n  geom_point()+\n  geom_line()\n#> geom_path: Each group consists of only one observation. Do you need to adjust\n#> the group aesthetic?\nggplot(group_means, aes(x = continent, \n                        y = mean_lifeExp,\n                        group = 1))+\n  geom_point()+\n  geom_line()"},{"path":"descriptives.html","id":"error-bars","chapter":"2 Descriptives","heading":"2.6.3 Error bars","text":"common present means bar graphs line graphs along error bars indicate variability around mean. ggplot need calculate measures variability condition, use values error bars. example add single standard deviation means error bars.","code":"\ngroup_means <- gapminder_data %>%\n                group_by(continent) %>%\n                summarize(mean_lifeExp = mean(lifeExp),\n                          sd_lifeExp = sd(lifeExp))\n\nggplot(group_means, aes(x = continent, y = mean_lifeExp))+\n  geom_bar(stat=\"identity\") +\n  geom_errorbar(aes(ymin = mean_lifeExp - sd_lifeExp,\n                    ymax = mean_lifeExp + sd_lifeExp),\n                width = .25)\n\nggplot(group_means, aes(x = continent, \n                        y = mean_lifeExp,\n                        group = 1))+\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_lifeExp - sd_lifeExp,\n                    ymax = mean_lifeExp + sd_lifeExp),\n                width = .25)"},{"path":"descriptives.html","id":"lab-2-generalization-assignment","chapter":"2 Descriptives","heading":"2.7 Lab 2 Generalization Assignment","text":"","code":""},{"path":"descriptives.html","id":"instructions-1","chapter":"2 Descriptives","heading":"2.7.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” made lab 1.Create new R Markdown document called “Lab2.Rmd”Use Lab2.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 2 blackboard.six problems solve, worth 1 point.","code":""},{"path":"descriptives.html","id":"problems-1","chapter":"2 Descriptives","heading":"2.7.2 Problems","text":"Use R demonstrate mean minimizes sum squared deviations mean. Accomplish following steps:Produce sample least 10 different numbersProduce simulation following example concepts sectionUse simulation test range numbers smaller larger mean show mean minimizes sum squared deviations mean.Plot results.Write custom R function one following descriptive statistics: median, mode, standard deviation, variance. Demonstrate produces value base R function given set numbers.Write custom R function one following descriptive statistics: median, mode, standard deviation, variance. Demonstrate produces value base R function given set numbers.Imagine instructor taught morning, afternoon, evening section course. , average scores section midterm 85% morning, 90% afternoon, 93% evening sections. Create data.frame representing means section. , use ggplot2 plot means bar graph. (hint need one vector class sections, one vector means. can combine data.frame plotting )Imagine instructor taught morning, afternoon, evening section course. , average scores section midterm 85% morning, 90% afternoon, 93% evening sections. Create data.frame representing means section. , use ggplot2 plot means bar graph. (hint need one vector class sections, one vector means. can combine data.frame plotting )Imagine two instructors, taught different sections morning, afternoon evening. midterm averages instructor 1 75%, 78%, 80% morning, afternoon, evening. midterm averages instructor 2 88%, 76%, 63% morning, afternoon, evening. Create data.frame representing means, time day, instructors (three columns). plot data.frame using ggplot2 bar graph.Imagine two instructors, taught different sections morning, afternoon evening. midterm averages instructor 1 75%, 78%, 80% morning, afternoon, evening. midterm averages instructor 2 88%, 76%, 63% morning, afternoon, evening. Create data.frame representing means, time day, instructors (three columns). plot data.frame using ggplot2 bar graph.Import WHR2018.csv data file, containing measure World Happiness report 2018. years 2010 2015, mean “healthy life expectancy birth” year (find mean year across countries). Show results table graph using ggplot.Import WHR2018.csv data file, containing measure World Happiness report 2018. years 2010 2015, mean “healthy life expectancy birth” year (find mean year across countries). Show results table graph using ggplot.Repeat , except addition calculating mean year, also calculate standard deviation “healthy life expectancy birth” year. , add error bars graph using +1 -1 standard deviations means year.Repeat , except addition calculating mean year, also calculate standard deviation “healthy life expectancy birth” year. , add error bars graph using +1 -1 standard deviations means year.","code":""},{"path":"descriptives.html","id":"advanced","chapter":"2 Descriptives","heading":"2.7.3 Advanced","text":"problem officially assigned points. ’m placing consideration.mean minimizes sum squared deviations. median minimizes sum absolute deviations. Demonstrate properties simulation R.Create set numbers mean median different. show plot values around mean median showing mean minimizes sum squared deviations. Finally, create plot values around mean median showing median minimizes sum absolute deviations.","code":""},{"path":"distributions-i.html","id":"distributions-i","chapter":"3 Distributions I","heading":"3 Distributions I","text":"“9/2/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"distributions-i.html","id":"reading-1","chapter":"3 Distributions I","heading":"3.1 Reading","text":"Vokey & Allen16, Chapters 5 & 6 additional descriptive statistics, recovering distribution; Abdi, Edelman, Dowling, & Valentin17, Appendices C & D.","code":""},{"path":"distributions-i.html","id":"overview-2","chapter":"3 Distributions I","heading":"3.2 Overview","text":"spend three entire labs devoted understanding working distributions. cover topics closely relate main statistics lecture, also take advantage R examine distributions direct hands manner possible without programming environment.lab one practical section two conceptual sections.Practical \n- Sampling distributions RConceptual \n- Monte-Carlo simulationIn research context data collected form measurements various conditions. data sample, representing outcomes happen. Generally, researchers recognize variability measuring process, sample data different. analyzing data interested happen, happened terms pattern numbers. really understand issues play, need take deep dive distributions understand happens take samples .","code":""},{"path":"distributions-i.html","id":"practical-i-sampling-from-distributions-in-r","chapter":"3 Distributions I","heading":"3.3 Practical I: Sampling from distributions in R","text":"","code":""},{"path":"distributions-i.html","id":"what-is-a-distribution","chapter":"3 Distributions I","heading":"3.3.1 What is a distribution?","text":"take informal approach defining distributions. can think distribution place machine controlling numbers come . words, distributions number creation machines. get define , choices determine kinds numbers can produced distribution.formally, probability distribution defines probabilities particular numbers can drawn sampled distribution.","code":""},{"path":"distributions-i.html","id":"creating-your-own-distributions-in-r-with-sample","chapter":"3 Distributions I","heading":"3.3.2 Creating your own distributions in R with sample()","text":"R several built-functions sampling common distributions (discussed later). look , let’s make using sample().input syntax sample(x, size, replace = FALSE, prob = NULL). x vector one elements, size many samples take elements vector, replace can set TRUE FALSE (controlling whether sampling done without replacement), prob vector probabilities controlling probability sampling element vector x.use sample, can create discrete distributions sample . , default every element equal probability sampled.Create distribution two equally possible numbers, sample twice:Create distribution two equally possible numbers, sample 10 times (note must set replace=TRUE, sampling items exist vector):Create distribution first number probability 90% sampled, second number probability 10% sampled, sample 10 timesCreate distribution model coin flip unbiased coin, flip coin 10 times, distribution return “heads” “tails”.Create distribution numbers 1 1000, allows sampled equal probability. Sample 10 numbers distribution without replacement (sampled one number, allowed sample taken ):","code":"\nsample(x= 1:2, size = 2)\n#> [1] 1 2\nsample(x= 1:2, size = 10, replace = TRUE)\n#>  [1] 1 2 2 2 1 2 2 1 2 2\nsample(x= 1:2, size = 10, replace = TRUE, prob=c(.9,.1))\n#>  [1] 1 2 1 2 1 1 1 1 1 1\nsample(x = c(\"heads\",\"tails\"), size=10, replace= TRUE) \n#>  [1] \"tails\" \"heads\" \"tails\" \"tails\" \"tails\" \"heads\" \"tails\" \"tails\" \"heads\"\n#> [10] \"tails\"\nsample(x= 1:1000, size = 10, replace = FALSE)\n#>  [1] 295 305 647 958 650  34 601 750  84 871"},{"path":"distributions-i.html","id":"normal-distribution","chapter":"3 Distributions I","heading":"3.3.3 Normal distribution","text":"sample random deviates normal distribution, use rnorm(n, mean = 0, sd = 1) function. n number observations sample, mean mean normal distribution, sd standard deviation normal distribution.Two ways sample 10 numbers normal distribution mean = 0, standard deviation = 1.Visualize sample quickly hist()Visualize sample ggplot2 using geom_histogram(). requirement sample data formatted data.frame first. create data frame 100 observations sample_data column, add sample column contains 1s, refer fact numbers sample_data belong sample #1.Visualizing multiple samples individual histograms ggplot2. Let’s say want sample 25 values normal distribution, want repeat process four times. samples 1 4, containing 25 observations. also want generate four histograms quickly look four samples. can setting dataframe represent situation, using facet_wrap().Note: use rep() function new, creates vector repeats numbers 1 4, 25 times . way, first 25 rows dataframe represent 25 observations sample 1, next 25 rows represent observations sample 2, .","code":"\nrnorm(n= 10,mean = 0, sd = 1)\n#>  [1]  1.53852567  2.14642930  0.80763544  1.79687191 -1.98008976 -0.07184228\n#>  [7]  0.48002104 -2.87379573  0.07574962  0.25929634\n\nrnorm(10,0,1)\n#>  [1]  1.52940450 -0.54155168 -0.38429585 -0.38380544  0.32573162 -0.08141934\n#>  [7]  0.20515938  0.29457096 -0.35019916 -0.48560447\nmy_sample <- rnorm(100,0,1)\nhist(my_sample)\nmy_data <- data.frame(sample_data = rnorm(100,0,1),\n                      sample = 1)\n\nlibrary(ggplot2)\n\nggplot(my_data, aes(x=sample_data))+\n  geom_histogram()\nmy_data <- data.frame(sample_data = rnorm(100,0,1),\n                      sample = rep(1:4, each=25))\n\nggplot(my_data, aes(x=sample_data))+\n  geom_histogram()+\n  facet_wrap(~sample)"},{"path":"distributions-i.html","id":"uniform-distribution-rectangle-distribution","chapter":"3 Distributions I","heading":"3.3.4 Uniform Distribution (rectangle distribution)","text":"uniform distribution equal probability distribution, numbers smallest largest equal probability sampled.Use runif(n, min = 0, max = 1) sample numbers uniform distribution. n number observations, min starting minimum value, max largest value.Sample plot 1000 values uniform distribution 0 1.Sample plot 10000 values uniform distribution 100 1000.Take one sample 100 numbers uniform distribution 0 1. , one sample return count many numbers less value .05.","code":"\nhist(runif(1000,0,1))\nhist(runif(10000,100,1000))\nmy_sample <- runif(100,0,1)\nlength(my_sample[my_sample < .05])\n#> [1] 2"},{"path":"distributions-i.html","id":"other-distributions","chapter":"3 Distributions I","heading":"3.3.5 Other distributions","text":"R contains many distributions sample numbers . list can found ?distributions. examples:Exponential distributionBinomial DistributionWeibull distribution","code":"\nhist(rexp(1000,rate =2))\nhist(rbinom(100,1,prob=c(.5,.5)))\nhist(rweibull(n=1000, shape=2, scale = 1))"},{"path":"distributions-i.html","id":"other-descriptive-statistics","chapter":"3 Distributions I","heading":"3.3.6 Other descriptive statistics","text":"Chapter 5, Vokey Allen discuss skewness kurtosis additional descriptive statistics describe shapes sets numbers. Functions skewness kurtosis can obtained R installing additional packages moments packages.Compute mean, sd, skewness, kurtosis sample 1000 observations normal distribution:","code":"\nlibrary(moments)\nmy_sample <- rnorm(1000,0,1)\nmean(my_sample)\n#> [1] -0.03866426\nsd(my_sample)\n#> [1] 1.002691\nskewness(my_sample)\n#> [1] -0.08843055\nkurtosis(my_sample)\n#> [1] 3.036247\nhist(my_sample)\nmy_sample <- rexp(1000,2)\nmean(my_sample)\n#> [1] 0.4997936\nsd(my_sample)\n#> [1] 0.5206859\nskewness(my_sample)\n#> [1] 2.206418\nkurtosis(my_sample)\n#> [1] 9.676076\nhist(my_sample)"},{"path":"distributions-i.html","id":"conceptual-i-monte-carlo-simulations","chapter":"3 Distributions I","heading":"3.4 Conceptual I: Monte carlo simulations","text":"Many next conceptual sections labs involve process called Monte carlo simulation. short, Monte Carlo simulation one sampling process carried hundreds thousands times order estimate sampling process behaves long run. Monte Carlo simulations can conducted easily R, can write scripts make R repeatedly sample things, can measure assess samples created.Monte-carlo simulations can used tool demonstrate statistical facts concepts, take opportunity use tool many different ways throughout course. purpose conceptual section introduce running Monte-Carlo simulations, show can done different ways.general :Simulate repeated sampling processSave sampled iterationSample many times want (usually thousand)Evaluate simulationAnd, important, identify important statistical concepts use monte-carlo simulations demonstrate understanding concepts.","code":""},{"path":"distributions-i.html","id":"fair-coin","chapter":"3 Distributions I","heading":"3.4.1 Fair coin","text":"coin fair comes heads equally often tails long run. Let’s consider use simulation demonstrate idea. need toHave way sample outcomes binary variableTake several samplesLook get equal number “heads” “tails” long run.one way use R accomplish goals. , use sample function, sample 1s heads, 0s tails. also create loop, repeat sampling process 100 times. iteration flip coin, save result, calculate proportion heads tails far. save everything data.frame, plot proportion heads go 1 100 flips. see proportion get closer .5 increase number flips.","code":"\n#initialize variables\nflip <- c()\noutcome <- c()\nproportion_heads <- c()\nproportion_tails <- c()\n\n# run the simulation\nfor(i in 1:1000){\n  flip[i] <- i\n  outcome[i] <- sample(x = c(1,0), size = 1)\n  proportion_heads[i] <- sum(outcome)/length(outcome)\n  proportion_tails[i] <- 1-proportion_heads[i]\n}\n\n# create a dataframe with saved data\nsim_data <- data.frame(flip,\n                       outcome,\n                       proportion_heads,\n                       proportion_tails)\n\n# plot the simulation results\nggplot(sim_data, aes(x=flip,y=proportion_heads))+\n  geom_point()+\n  geom_line()+\n  geom_hline(yintercept=.5, color=\"red\")"},{"path":"distributions-i.html","id":"samples-become-the-population-as-n-increases","chapter":"3 Distributions I","heading":"3.4.2 Samples become the population as n increases","text":"fundamental concept sampling samples numbers become increasingly like parent population (distribution) size sample (n number observations sample) increases. Let’s demonstrate example phenomena.parent population normal distribution mean =100, sd = 50. want conduct simulation takes sample across different ranges n. sample, calculate sample statistic mean standard deviation. sample statistics become closer closer “true” parent distribution parameters n increases.","code":"\n#initialize variables\nn <- seq(1000,100000,1000)\nsample_mean <- c()\nsample_sd <- c()\n\n#run simulation\nfor(i in 1:length(n)){\n  sim_sample <- rnorm(n[i], mean = 100, sd = 50)\n  sample_mean[i] <- mean(sim_sample)\n  sample_sd[i] <- sd(sim_sample)\n}\n\n# organize results in dataframe\nsim_data <- data.frame(n,\n                       sample_mean,\n                       sample_sd)\n\n# graph results\nggplot(sim_data,aes(x=n,y=sample_mean))+\n  geom_point()+\n  geom_line()+\n  geom_hline(yintercept=100, color=\"red\")\n\nggplot(sim_data,aes(x=n,y=sample_sd))+\n  geom_point()+\n  geom_line()+\n  geom_hline(yintercept=50, color=\"red\")"},{"path":"distributions-i.html","id":"lab-3-generalization-assignment","chapter":"3 Distributions I","heading":"3.5 Lab 3 Generalization Assignment","text":"","code":""},{"path":"distributions-i.html","id":"instructions-2","chapter":"3 Distributions I","heading":"3.5.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab3.Rmd”Use Lab3.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 3 blackboard.four problems solve","code":""},{"path":"distributions-i.html","id":"problems-2","chapter":"3 Distributions I","heading":"3.5.2 Problems","text":"Create five samples 25 observations normal distribution mean 200, standard deviation 100. Compute mean sample, plot means graph using ggplot2. (1 point)Create five samples 25 observations normal distribution mean 200, standard deviation 100. Compute mean sample, plot means graph using ggplot2. (1 point)Additionally calculate standard deviation sample . Use standard deviations error bars, produce another graph means along error bars using ggplot2. (1 point)Additionally calculate standard deviation sample . Use standard deviations error bars, produce another graph means along error bars using ggplot2. (1 point)last two problems concern concept using sample estimate property population distribution sample came . example, know mean sample, can confident population mean? trying guess population mean, statistics sample use?sample statistics “biased”, may systematically overestimate population parameter. Others “unbiased”, case sample statistic tends correctly estimate population parameter long run.Demonstrate sample mean across range n, unbiased estimator population mean using monte-carlo simulation. (2 points).population normal distribution mean = 10, standard deviation = 5.Test variety n (sample size), including n = 2, 5, 10, 50, 100For sample size n, task draw 10,000 samples size, sample, calculate sample mean. mean unbiased, expect “average” sample means population mean. determine true, compute mean sample means produce see close population mean.Show mean sample means sample size.Use monte carlo simulation compare standard deviation formulas (divide N vs N-1), show N-1 formula better unbiased estimate population standard deviation, especially small n. (2 points)Use normal distribution samples sizes aboveRather computing mean sample, compute forms standard deviation formula, including sample standard deviation divides N-1, regular standard deviation divides NYou 10,000 samples sample size, 10,000 standard deviations sample regular standard deviation. task find average , sample-size.standard deviations systematically biased? , one systematically worse estimating population standard deviation?","code":""},{"path":"distributions-ii.html","id":"distributions-ii","chapter":"4 Distributions II","heading":"4 Distributions II","text":"“9/17/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"distributions-ii.html","id":"reading-2","chapter":"4 Distributions II","heading":"4.1 Reading","text":"Vokey & Allen18, Chapter 16; Crump, Navarro, & Suzuki19, 4.1 - 4.7; Abdi, Edelman, Dowling, & Valentin20, Appendix C.","code":""},{"path":"distributions-ii.html","id":"overview-3","chapter":"4 Distributions II","heading":"4.2 Overview","text":"lecture learning basic probability. lab continue learn working distributions R, examine issues relating basic probability.look three conceptual issues probability, gain practical skills R organizing managing simulation data useful rest course. examples generate events particular distributions examine .Concept : Probabilistic event generationConcept II: Experiencing probabilityConcept III: Subjective Probability","code":""},{"path":"distributions-ii.html","id":"concept-i-probabilisitic-event-generation","chapter":"4 Distributions II","heading":"4.3 Concept I: Probabilisitic event generation","text":"concept section use R generate events specified probabilities. mainly use sample() function, already familiar . concept section explores probabilistic event generation different ways examining probability problems:","code":""},{"path":"distributions-ii.html","id":"dice-problems","chapter":"4 Distributions II","heading":"4.3.1 Dice Problems","text":"Use R roll fair six-sided die, show number comes equally frequently long run. Roll die 10,000 times, report many times number (1-6) rolled.number come 10000/6 = 1666.667 times.pair six-sided dice possible roll numbers 2 12. Use simulation 10000 rolls (pair dice) R calculate probability rolling possible numbers.Let’s compare result simulation known probabilities. First, need determine number ways number can obtained rolling pair dice. can use R well:","code":"\nrolls <- sample(1:6,1000, replace=TRUE)\ntable(rolls)\n#> rolls\n#>   1   2   3   4   5   6 \n#> 191 159 152 166 165 167\none <- sample(1:6,1000, replace=TRUE)\ntwo <- sample(1:6,1000, replace=TRUE)\ncombined <- one+two\ntable(combined)/1000\n#> combined\n#>     2     3     4     5     6     7     8     9    10    11    12 \n#> 0.027 0.052 0.080 0.114 0.146 0.170 0.150 0.102 0.079 0.054 0.026\nfirst <- rep(x= 1:6, each = 6)\nsecond <- rep(x= 1:6, times = 6)\nsum_rolls <- first+second\ntable(sum_rolls)/length(sum_rolls)\n#> sum_rolls\n#>          2          3          4          5          6          7          8 \n#> 0.02777778 0.05555556 0.08333333 0.11111111 0.13888889 0.16666667 0.13888889 \n#>          9         10         11         12 \n#> 0.11111111 0.08333333 0.05555556 0.02777778\n\n## compare\nsim_result <- table(combined)/1000\ntrue_probs <- table(sum_rolls)/length(sum_rolls)\n\n## Difference\ntrue_probs-sim_result\n#> sum_rolls\n#>             2             3             4             5             6 \n#>  0.0007777778  0.0035555556  0.0033333333 -0.0028888889 -0.0071111111 \n#>             7             8             9            10            11 \n#> -0.0033333333 -0.0111111111  0.0091111111  0.0043333333  0.0015555556 \n#>            12 \n#>  0.0017777778"},{"path":"distributions-ii.html","id":"event-generators","chapter":"4 Distributions II","heading":"4.3.2 Event generators","text":"Remember can use sample() generate events specific probabilities:Generate P(“”) = .8, P(“B”) =.2, run generator 20 times.Generate letters alphabet letter occur equal probability. Generate 50 letters:Note, conveniently, R contains variable called letters, vector lowercase letters (uppercase one called LETTERS)Create random string generator creates strings random letters. example string 5 random letters look like “fjwud”. Generate 50 random letter strings, 5 random letters .","code":"\nsample(c(\"A\",\"B\"), 20, replace = TRUE, prob = c(.8, .2))\n#>  [1] \"A\" \"B\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"A\" \"A\"\n#> [20] \"A\"\nletters\n#>  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n#> [20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nLETTERS\n#>  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n#> [20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\nsample(letters,50,replace=TRUE)\n#>  [1] \"b\" \"r\" \"s\" \"n\" \"o\" \"i\" \"u\" \"f\" \"t\" \"g\" \"v\" \"u\" \"w\" \"x\" \"a\" \"i\" \"n\" \"k\" \"a\"\n#> [20] \"t\" \"b\" \"v\" \"g\" \"p\" \"k\" \"o\" \"d\" \"s\" \"w\" \"j\" \"p\" \"s\" \"t\" \"a\" \"w\" \"t\" \"l\" \"p\"\n#> [39] \"d\" \"a\" \"i\" \"c\" \"x\" \"t\" \"m\" \"k\" \"w\" \"m\" \"v\" \"v\"\nmy_letters <- sample(letters,50*5,replace=TRUE)\n\n# turn the vector into a matrix with 5 columns\nmy_strings <- matrix(my_letters, ncol=5)\n\n# each row is a word, need to collapse the column to create a string\npaste(my_strings[1,], collapse=\"\")\n#> [1] \"cvioy\"\n\n# loop to collapse all of the rows into words\nrandom_strings <-c()\nfor(i in 1:dim(my_strings)[1]){\n  random_strings[i] <- paste(my_strings[i,], collapse=\"\")\n}\n\nrandom_strings\n#>  [1] \"cvioy\" \"rmnsx\" \"msrcq\" \"kqhqo\" \"mqzdz\" \"mbpvk\" \"lsvtv\" \"cefbp\" \"kaxri\"\n#> [10] \"wnrux\" \"yjgic\" \"wnurs\" \"bpier\" \"ualrd\" \"erdgq\" \"arikl\" \"kqmoz\" \"ppqyr\"\n#> [19] \"rgwcl\" \"jbinm\" \"fvaaj\" \"jxhjs\" \"wubgx\" \"tkfpa\" \"eotge\" \"ijfxd\" \"noamg\"\n#> [28] \"cwagt\" \"izjxq\" \"bshry\" \"dxtdp\" \"gjjtt\" \"rrhdr\" \"uhmsx\" \"qqxva\" \"umnfp\"\n#> [37] \"lnatb\" \"jlviq\" \"izbnu\" \"jkjif\" \"zylpa\" \"futnc\" \"piqum\" \"zpifj\" \"siuxl\"\n#> [46] \"jhbck\" \"pbvhy\" \"vtwxg\" \"fpdyv\" \"hkrxt\""},{"path":"distributions-ii.html","id":"concept-ii-experiencing-probability","chapter":"4 Distributions II","heading":"4.4 Concept II: Experiencing probability","text":"People talk probabilities time. example, tomorrow might 10% chance rain, fair coin 50% chance landing heads tails, common examples. already begun look probabilities behave lab 3, used R flip coin demonstrate coin fair long run. expand demonstration .One takeaway point coin flipping example P(heads) = .5 (probability getting heads equals 50%), true “long run”. short run, get bunch tails.","code":""},{"path":"distributions-ii.html","id":"short-run-coin-flipping","chapter":"4 Distributions II","heading":"4.4.1 Short-run coin flipping","text":"Consider flipping fair coin ten times. possible outcomes? probability outcomes? Let’s answer question simulation R.possible outcomes 10 Tails 10 Heads, combination heads tails , can described 0 heads 10 heads.flipped coin 10000 times, find many different kinds short-run sequences. example, HH, HT, TH, TT. probability kinds sequences?","code":"\nsim_results <- replicate(10000,\n                         sample( c(1,0), 10, replace=TRUE)\n                         )\nnumber_of_heads <- colSums(sim_results)\ntable(number_of_heads)/10000\n#> number_of_heads\n#>      0      1      2      3      4      5      6      7      8      9     10 \n#> 0.0005 0.0113 0.0415 0.1204 0.1988 0.2449 0.2071 0.1189 0.0457 0.0107 0.0002\n\n# alternative solution using rbinom\n\nnumber_of_heads <- rbinom(10000,10,prob=.5)\ntable(number_of_heads)/10000\n#> number_of_heads\n#>      0      1      2      3      4      5      6      7      8      9     10 \n#> 0.0014 0.0106 0.0420 0.1187 0.2042 0.2528 0.1983 0.1186 0.0433 0.0093 0.0008\nflips <- sample(c(\"H\",\"T\"), 10000, replace=TRUE)\n\nsequence <- c()\nfor(i in 2:length(flips)){\n  first_element <- flips[i-1]\n  second_element <- flips[i]\n  sequence[i-1] <- paste0(first_element,second_element)\n}\n\ntable(sequence)/sum(table(sequence))\n#> sequence\n#>        HH        HT        TH        TT \n#> 0.2462246 0.2539254 0.2539254 0.2459246\n\n## 3 element sequences\n\nflips <- sample(c(\"H\",\"T\"), 10000, replace=TRUE)\n\nsequence <- c()\nfor(i in 3:length(flips)){\n  first_element <- flips[i-2]\n  second_element <- flips[i-1]\n  third_element <- flips[i]\n  sequence[i-1] <- paste0(first_element,\n                          second_element,\n                          third_element)\n}\n\ntable(sequence)/sum(table(sequence))\n#> sequence\n#>       HHH       HHT       HTH       HTT       THH       THT       TTH       TTT \n#> 0.1231246 0.1277255 0.1259252 0.1273255 0.1277255 0.1254251 0.1272254 0.1155231"},{"path":"distributions-ii.html","id":"concept-iii-subjective-probability","chapter":"4 Distributions II","heading":"4.5 Concept III: Subjective Probability","text":"Vokey & Allen discuss Bayesian concept subjective probability, practice assigning probabilities beliefs, updating probabilities belief data-gathering process.concept section use R demonstrate basic example belief updating.First, create sequence events. stick coin flips. Let’s create situation event probability changes point, task collect data determine can update beliefs world., flip fair coin 100 times, flip biased coin 100 times. biased coin likely come heads (60%).Next, imagine idea kind coins flipped, sequence flips. start first coin flip, go , time use data update belief coin.beliefs probability getting heads look like allow remember last 20 coin flips?confident correct belief probability getting heads?","code":"\nsimulated_sequence <- c(rbinom(100,1,.5),\n                        rbinom(100,1,.6))\nmy_knowledge <- c()\nmy_belief <- c()\nfor(i in 1:length(simulated_sequence)){\n  \n    my_knowledge[i] <- simulated_sequence[i]\n    my_belief[i] <- sum(my_knowledge)/length(my_knowledge)\n    \n}\n\nplot(my_belief)\n\nsimulated_sequence <- c(rbinom(100,1,.5),\n                        rbinom(100,1,.6))\n\nmy_knowledge <- c()\nmy_belief <- c()\nfor(i in 1:length(simulated_sequence)){\n  \n    my_knowledge[i] <- simulated_sequence[i]\n    if(i <= 20){\n      my_belief[i] <- sum(my_knowledge)/length(my_knowledge)\n    }else{\n      my_belief[i] <- sum(my_knowledge[i:(i-20)])/length(my_knowledge[i:(i-20)])\n    }\n    \n}\n\nplot(my_belief)"},{"path":"distributions-ii.html","id":"lab-4-generalization-assignment","chapter":"4 Distributions II","heading":"4.6 Lab 4 Generalization Assignment","text":"","code":""},{"path":"distributions-ii.html","id":"instructions-3","chapter":"4 Distributions II","heading":"4.6.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab4.Rmd”Use Lab4.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 4 blackboard.five problems solve","code":""},{"path":"distributions-ii.html","id":"problems-3","chapter":"4 Distributions II","heading":"4.6.2 Problems","text":"Estimate letter occurrence probabilities 26 letters English measuring paragraph English text wikipedia. (hint use strsplit() split paragraph individual letters) (1 point).Generate “random” strings letters sampled distribution letter occurrence probability natural English. Use probabilities letter wikipedia article, use estimates previous question (2 points).Generate “random” strings letters sampled distribution letter occurrence probability natural English. Use probabilities letter wikipedia article, use estimates previous question (2 points).Generate random walk 10,000 steps. random walk, simulating process randomly taking step , infinite staircase. step flip coin. get heads go one step, get tails go one step. Start step 0, simulate random walk 10,000 steps. vector preserve step number step. example, first three steps heads, vector begin 0,1,2,3, indicates single step time. Plot first 1,000 steps. (1 point)Generate random walk 10,000 steps. random walk, simulating process randomly taking step , infinite staircase. step flip coin. get heads go one step, get tails go one step. Start step 0, simulate random walk 10,000 steps. vector preserve step number step. example, first three steps heads, vector begin 0,1,2,3, indicates single step time. Plot first 1,000 steps. (1 point)positive negative step reached 10,000? (1 point)positive negative step reached 10,000? (1 point)longest run steps steps positive numbers. example, sequence: 1,2,3,2,1,0,-1,-2,-1,-2,-1,0,1,2,3; answer 5 first five values positive, longest sequence positive values. (1 point).longest run steps steps positive numbers. example, sequence: 1,2,3,2,1,0,-1,-2,-1,-2,-1,0,1,2,3; answer 5 first five values positive, longest sequence positive values. (1 point).","code":"\nmy_paragraph <- \"This is a paragraph, with some stuff in it. This is another sentence in the paragraph\"\nthe_letters <- unlist(strsplit(my_paragraph, split=\"\"))\nthe_letters\n#>  [1] \"T\" \"h\" \"i\" \"s\" \" \" \"i\" \"s\" \" \" \"a\" \" \" \"p\" \"a\" \"r\" \"a\" \"g\" \"r\" \"a\" \"p\" \"h\"\n#> [20] \",\" \" \" \"w\" \"i\" \"t\" \"h\" \" \" \"s\" \"o\" \"m\" \"e\" \" \" \"s\" \"t\" \"u\" \"f\" \"f\" \" \" \"i\"\n#> [39] \"n\" \" \" \"i\" \"t\" \".\" \" \" \"T\" \"h\" \"i\" \"s\" \" \" \"i\" \"s\" \" \" \"a\" \"n\" \"o\" \"t\" \"h\"\n#> [58] \"e\" \"r\" \" \" \"s\" \"e\" \"n\" \"t\" \"e\" \"n\" \"c\" \"e\" \" \" \"i\" \"n\" \" \" \"t\" \"h\" \"e\" \" \"\n#> [77] \"p\" \"a\" \"r\" \"a\" \"g\" \"r\" \"a\" \"p\" \"h\""},{"path":"sampling-distributions.html","id":"sampling-distributions","chapter":"5 Sampling Distributions","heading":"5 Sampling Distributions","text":"“10/2/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"sampling-distributions.html","id":"readings","chapter":"5 Sampling Distributions","heading":"5.1 Readings","text":"Crump, Navarro, & Suzuki21, 4.8 - 4.10","code":""},{"path":"sampling-distributions.html","id":"review","chapter":"5 Sampling Distributions","heading":"5.2 Review","text":"last two labs begun explore distributions, used R create sample distributions. continue exploration many following labs.continue want point major conceptual goal . nuanced understanding one statement one question:Chance can thingsWhat can chance ?put concrete terms previous labs. know ‘chance’ can produce different outcomes flip coin. sense chance things. 50% chance process can sometimes make heads, sometimes tails. Also, started ask “can chance ?” prior labs. example, asked often chance produces 10 heads row, flip coin. found chance doesn’t often, compared say 5 heads 5 tails.using next labs find different ways use R experience 1) chance can things, 2) likely things happen chance. working toward third question (next lab), 3) chance ?…run experiment, possible chance alone produced data collected?","code":""},{"path":"sampling-distributions.html","id":"overview-4","chapter":"5 Sampling Distributions","heading":"5.3 Overview","text":"lab following modules:Conceptual Review : Probability Distributionswe review sampling probability distributions using R examine additional aspects base R distribution functionsConceptual II: Sampling Distributionswe use R create new kind distribution, called sampling distribution. prepare future statistics lectures concepts fundamentally depend sampling distributions.Understanding sampling distributions may well fundamental thing understand statistics (’s just opinion).","code":""},{"path":"sampling-distributions.html","id":"probability-distributions","chapter":"5 Sampling Distributions","heading":"5.4 Probability Distributions","text":"previous labs learned possible sample numbers particular distributions R.","code":"#see all the distribution functions\n?distributions"},{"path":"sampling-distributions.html","id":"normal-distribution-1","chapter":"5 Sampling Distributions","heading":"5.4.1 Normal Distribution","text":"use rnorm() sample numbers normal distribution:can ‘see’ distribution sampling large number observations, plotting histogram:can see example using random chance sample distribution caused numbers observed. , can see “chance something”. can also see chance things others. Values close 0 sampled much often values larger 2.5.often sample value larger 2.5? probability chance produce value larger 2.5? value distribution? answer questions, need get specific exactly chance situation, distribution, capable .can answer question like observation. can look sample generated, see many numbers total larger particular value:also compute probability directly using analytical formulas. , formulas also exist R. Specifically, distribution formulas begin d, p, q, r, dnorm, pnorm, qnorm, rnorm functions normal distribution (distributions).","code":"\nrnorm(n=10, mean = 0, sd = 1)\n#>  [1] -0.4262326 -1.9504370 -0.2630845 -1.4325022  0.5692190  0.3995944\n#>  [7] -0.1543729 -0.3256509  0.3089172 -0.9080528\nlibrary(ggplot2)\nsome_data <- data.frame(observations = rnorm(n=10000, mean = 0, sd = 1),\n                        type = \"A\")\n\nggplot(some_data, aes(x=observations)) +\n  geom_histogram(bins=100, color=\"black\", \n                 fill= 'orange')\nsome_data$observations[some_data$observations > 2.5]\n#>  [1] 2.569002 2.854438 2.542498 2.911574 2.741360 2.875751 2.642938 2.607074\n#>  [9] 2.699894 2.751360 2.520366 3.512031 2.704728 3.038817 2.664322 2.752404\n#> [17] 2.586985 2.780469 2.929764 3.362060 2.746399 2.537388 3.434642 2.533305\n#> [25] 2.787945 2.923204 2.581067 2.754448 2.783801 3.000941 2.626614 2.781184\n#> [33] 2.607430 3.364519 2.544195 3.340397 3.130555 2.788952 4.106499 2.547435\n#> [41] 3.282487 2.608786 2.702069 3.177433 2.500632 3.176734 2.828287 3.000135\n#> [49] 2.865655 2.765614 2.500413 2.930355 2.847117 2.981198 2.910288 2.661948\n#> [57] 2.635070 2.555418 2.625948 3.025801 2.506846\nlength(some_data$observations[some_data$observations > 2.5])\n#> [1] 61\nlength(some_data$observations[some_data$observations > 2.5])/10000\n#> [1] 0.0061"},{"path":"sampling-distributions.html","id":"rnorm","chapter":"5 Sampling Distributions","heading":"5.4.1.1 rnorm()","text":"rnorm(n, mean = 0, sd = 1) samples observations (random deviates) normal distribution specified mean standard deviation.","code":"\nrnorm(n=10, mean = 0, sd = 1)\n#>  [1]  1.1903046  1.8631129  0.9828581  0.8447314 -0.7088629 -1.1957783\n#>  [7] -0.4183044 -0.1706795  1.0815985 -0.2198671"},{"path":"sampling-distributions.html","id":"dnorm","chapter":"5 Sampling Distributions","heading":"5.4.1.2 dnorm()","text":"dnorm(x, mean = 0, sd = 1, log = FALSE) probability density function. returns probability density distribution value can obtained distribution.example, histogram, can see distribution produces values roughly -3 3, perhaps -4 4. also see values approach 0, happen often. , probability density changes across distribution. can plot directly using dorm(), supplying sequence value, say -4 4.pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) takes given value distribution produce input called q quantile. function returns proportional area curve value.example, area curve starting left (e.g., negative infinity) way 2.5?“complement” question. 99.37903% values drawn distribution expected less 2.5. just determined probability getting number smaller 2.5, otherwise known lower tail.\ndefault, pnorm calculates lower tail, area curve q value point left side plot.calculate probability getting number larger particular value can take complement, set lower.tail=FALSE","code":"\nlibrary(ggplot2)\nsome_data <- data.frame(density = dnorm(-4:4, mean = 0, sd = 1),\n                        x = -4:4)\n\nknitr::kable(some_data)\n\nggplot(some_data, aes(x=x, y=density)) +\n  geom_point()\nsome_data <- data.frame(density = dnorm(seq(-4,4,.001), mean = 0, sd = 1),\n                        x = seq(-4,4,.001))\n\nggplot(some_data, aes(x=x, y=density)) +\n  geom_line()\nlibrary(dplyr)\nsome_data <- data.frame(density = dnorm(seq(-4,4,.001), mean = 0, sd = 1),\n                        x = seq(-4,4,.001))\n\nregion_data <- some_data %>%\n  filter(x > 2.5)\n\nggplot(some_data, aes(x=x, y=density)) +\n  geom_line()+\n  geom_ribbon(data = region_data, \n              fill = \"red\",\n              aes(ymin=0,ymax=density))\npnorm(2.5, mean=0, sd = 1)\n#> [1] 0.9937903\nsome_data <- data.frame(density = dnorm(seq(-4,4,.001), mean = 0, sd = 1),\n                        x = seq(-4,4,.001))\nregion_data <- some_data %>%\n  filter(x < 2.5)\n\nggplot(some_data, aes(x=x, y=density)) +\n  geom_line()+\n  geom_ribbon(data = region_data, \n              fill = \"red\",\n              aes(ymin=0,ymax=density))\n1 - pnorm(2.5, mean=0, sd = 1)\n#> [1] 0.006209665\n\npnorm(2.5, mean=0, sd = 1, lower.tail=FALSE)\n#> [1] 0.006209665"},{"path":"sampling-distributions.html","id":"qnorm","chapter":"5 Sampling Distributions","heading":"5.4.1.3 qnorm","text":"qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) similar pnorm, takes probability input, returns specific point value (quantile) x-axis. formally, specify proportional area curve starting left, function tells number area corresponds .remaining assume mean=0 sd =1.number x-axis location 25% values smaller value?number x-axis location 50% values smaller value?number 95% values larger number?","code":"\nqnorm(.25, mean= 0, sd =1)\n#> [1] -0.6744898\nqnorm(.5, mean= 0, sd =1)\n#> [1] 0\nqnorm(.95, mean= 0, sd =1, lower.tail = FALSE)\n#> [1] -1.644854\n\nqnorm(.05, mean = 0 , sd =1, lower.tail = FALSE)\n#> [1] 1.644854"},{"path":"sampling-distributions.html","id":"summary","chapter":"5 Sampling Distributions","heading":"5.4.2 Summary","text":"Focusing normal distribution functions, learned chance can produce different kinds numbers normal distribution. , can use dnorm, qnorm, pnorm functions exactly compute specific probabilities certain ranges values occur.","code":""},{"path":"sampling-distributions.html","id":"conceptual-sampling-distributions","chapter":"5 Sampling Distributions","heading":"5.5 Conceptual: Sampling Distributions","text":"collect data assume come “distribution”, “causes” numbers occur others. data collect “sample” portion “distribution”.know sample distributions chance can play role. Specifically, chance alone one sample observations look different another sample, even came distribution. words, recognize process sampling distribution involves variability uncertainty.can use sampling distributions tool help us understand predict sampling process behave. way can information variable uncertain samples .","code":""},{"path":"sampling-distributions.html","id":"confusing-jargon","chapter":"5 Sampling Distributions","heading":"5.5.1 Confusing jargon","text":"Throughout course come across terms like, “sampling distributions”, “sampling distribution sample mean”, “sampling distribution sample statistic”, “standard deviation sampling distribution sample mean standard error mean”. Although sentence hard parse opinion jargony, represent important ideas need distinguished well understood. going work things lab.","code":""},{"path":"sampling-distributions.html","id":"the-sample-mean","chapter":"5 Sampling Distributions","heading":"5.5.2 The sample mean","text":"remaining examples use normal distribution mean = 0 sd =1.already know sample mean calculate R. example calculating sample mean, number observations (n) sample 10.","code":"\nmean(rnorm(10, mean=0, sd =1))\n#> [1] -0.4239849"},{"path":"sampling-distributions.html","id":"multiple-sample-means","chapter":"5 Sampling Distributions","heading":"5.5.3 Multiple sample means","text":"can repeat process many times like, time creating sample 10 observations computing mean.example creating 5 sample means 5 sets 10 observations.Notice sample means different, variability introduced randomly choosing values normal distribution.mean distribution samples come 0, expect mean samples ? general, expect 0, can see sample means exactly 0.much variability can expect sample mean? words, going obtain sample 10 numbers distribution, kinds sample means get?.","code":"\nmean(rnorm(10, mean=0, sd =1))\n#> [1] -0.04471581\nmean(rnorm(10, mean=0, sd =1))\n#> [1] -0.2384895\nmean(rnorm(10, mean=0, sd =1))\n#> [1] 0.376665\nmean(rnorm(10, mean=0, sd =1))\n#> [1] 0.0569266\nmean(rnorm(10, mean=0, sd =1))\n#> [1] 0.1866335"},{"path":"sampling-distributions.html","id":"the-sampling-distribution-of-the-sample-means","chapter":"5 Sampling Distributions","heading":"5.5.4 The sampling distribution of the sample means","text":"answer question kinds sample means get? “sampling distribution sample means”. words, work actually find create bunch samples, find means, bunch sample means, numbers form distribution. distribution effectively showing different ways random chance can produce particular sample means.Let’s make distribution sample means. create 10,000 samples, 10 observations, compute mean . save look means histogram.wanted know expect single sample mean (knew taking values normal distribution), look sampling distribution.Sample means close 0 happen . , time, take sample distribution, mean sample 0. rare sample mean larger .5. rare sample mean larger 1.","code":"\nsample_means <- replicate(10000, mean(rnorm(10,0,1)))\nhist(sample_means)"},{"path":"sampling-distributions.html","id":"the-standard-error-the-mean","chapter":"5 Sampling Distributions","heading":"5.5.5 The standard error the mean","text":"discussed concept definition standard error mean lecture portion class. However, seen mean sample taken distribution expected variability, specifically fact distribution different sample means shows variability.descriptive statistic already discussed provides measures variability? One option standard deviation. example, following measure variability associated distribution sample means.Generate distribution sample meansCalculate standard deviation sample meansThe standard deviation sample means give us idea much variability expect sample mean. quickly R like :value calculated standardized unit, describes amount error expect general sample mean. Specifically, true population mean 0, obtain samples, expect sample means error, average 0, plus minus standard deviation calculated.necessarily generate distribution sample means calculate standard error. know population standard deviation (\\(\\sigma\\)), can use formula standard error mean (SEM) :\\(\\text{SEM} = \\frac{\\sigma}{\\sqrt{N}}\\)\\(\\sigma\\) population standard deviation, \\(N\\) sample size.can also compare SEM formula one obtained simulation, find similar.","code":"\nsample_means <- replicate(10000, mean(rnorm(10,0,1)))\nsd(sample_means)\n#> [1] 0.3225278\n# simulation SEM\nsample_means <- replicate(10000, mean(rnorm(10,0,1)))\nsd(sample_means)\n#> [1] 0.3181567\n\n# analytic SEM\n\n1/sqrt(10)\n#> [1] 0.3162278"},{"path":"sampling-distributions.html","id":"lab-5-generalization-assignment","chapter":"5 Sampling Distributions","heading":"5.6 Lab 5 Generalization Assignment","text":"","code":""},{"path":"sampling-distributions.html","id":"instructions-4","chapter":"5 Sampling Distributions","heading":"5.6.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab5.Rmd”Use Lab5.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 5 blackboard.five problems solve","code":""},{"path":"sampling-distributions.html","id":"problems-4","chapter":"5 Sampling Distributions","heading":"5.6.2 Problems","text":"Trust verify. trust rnorm() generate random deviates accordance definition normal distribution. example, learned lab, normal distribution mean = 0, sd =1 , produce values larger 2.5 specific small probability, P(x>2.5) = 0.006209665. Verify approximately case randomly sampling 1 million numbers distribution, calculate proportion numbers larger 2.5. (1 point)Trust verify. trust rnorm() generate random deviates accordance definition normal distribution. example, learned lab, normal distribution mean = 0, sd =1 , produce values larger 2.5 specific small probability, P(x>2.5) = 0.006209665. Verify approximately case randomly sampling 1 million numbers distribution, calculate proportion numbers larger 2.5. (1 point)performance standardized test known follow normal distribution mean 100 standard deviation 10, 10,000 people took test, many people expected achieve score higher 3 standard deviations mean? (1 point)performance standardized test known follow normal distribution mean 100 standard deviation 10, 10,000 people took test, many people expected achieve score higher 3 standard deviations mean? (1 point)randomly sample 25 numbers normal distribution mean = 10 standard deviation = 20. obtain sample mean 12. want know probability received sample mean 12 larger.randomly sample 25 numbers normal distribution mean = 10 standard deviation = 20. obtain sample mean 12. want know probability received sample mean 12 larger.Create sampling distribution mean scenario least 10,000 sample means (1 point). , calculate proportion sample means 12 larger (1 point).randomly sample 100 numbers normal distribution mean = 10 standard deviation = 20. obtain sample mean 12. want know probability received sample mean 12 larger.Create sampling distribution mean scenario least 10,000 sample means. , calculate proportion sample means 12 larger. proportion different question 3, ? (1 point).randomly sample 25 numbers normal distribution mean = 10 standard deviation = 20. obtain sample standard deviation 15. want know probability received sample standard deviation 15 less.Create sampling distribution standard deviations scenario least 10,000 sample standard deviations. , calculate proportion sample standard deviations 15 less. (1 point)","code":""},{"path":"statistical-inference.html","id":"statistical-inference","chapter":"6 Statistical Inference","heading":"6 Statistical Inference","text":"“10/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"statistical-inference.html","id":"readings-and-review","chapter":"6 Statistical Inference","heading":"6.1 Readings and Review","text":"Vokey & Allen22, Chapter 10; Crump, Navarro, & Suzuki23, Chapter 5This lab marks departure land statistical inference. working toward concepts using R help us understand basic aspects distributions sampling distributions. concepts applied throughout remainder course discuss many different tools tests statistical inference.instructor (Matt Crump), know many “feelings” statistical inference. , many perspectives, sometimes competing even inconsistent ones, value, goals, practice “statistical inference”. partly many well-founded opinions approaches statistical inference try , also partly many known issues flawed practices researchers sometimes engage , also many limitations kinds inferences can made depending .take one example, researcher might want “X test” data, even though don’t really know “X test” , know can put data program press button, program X test, can report results X test paper. view kind behavior irresponsible. view , general, researcher extremely well thought reasons justifications motivations advance collecting data analysis. able describe X test works, ’s assumptions , appropriate use case, well ’s limitations , exactly can concluded test.Another example line view. researcher understands general principles statistical inference works, collect data particular experiment designed, make use custom test completely justifiable issue addressing.","code":""},{"path":"statistical-inference.html","id":"be-able-to-justify-your-statistical-inference","chapter":"6 Statistical Inference","heading":"6.1.1 Be able to justify your statistical inference","text":"examples kind extreme cases. However, whether use well-known test, common canned approach, roll--statistics, strongly believe able justify approach. means able present argument process statistical inference minimally suitable task hand., think able justify statistical inferences important, designed following labs help learn fundamental principles statistical inference, can use understand justify even create process statistical inference appropriate research .","code":""},{"path":"statistical-inference.html","id":"foundations-for-statistical-inference-and-the-crump-test","chapter":"6 Statistical Inference","heading":"6.1.2 Foundations for Statistical Inference and the Crump test","text":"new concept tests statistical inference, /want slightly unconventional take statistical inference, ’ll forward chapter undergraduate statistics textbook:24 https://crumplab.github.io/statistics/foundations--inference.html. chapter discusses basic ideas behind statistical inference, also provides test “made ” called Crump test, doesn’t use formulas, tries highlight process ideas behind statistical inference.","code":""},{"path":"statistical-inference.html","id":"a-parting-metaphor","chapter":"6 Statistical Inference","heading":"6.1.3 A parting metaphor","text":"Imagine detective scene crime. want know committed crime. Another detective says, hypothesis, crime scene consistent believe Mickey Mouse committed crime. puzzled, Mickey Mouse fictional cartoon character, impossible Mickey Mouse committed crime. ask clarification. Detective says, “mean , Mickey Mouse real, think committed crime”. Also, several detectives say think Donald Duck , mean, crime scene inconsistent Donald Duck done crime Donald Duck real.might wondering metaphor? imagine detective finds strange even consider hypotheses fictional cartoon characters may may committed crime. view, strangeness different researcher considers statistical hypotheses data collect. see throughout rest course, statistical hypotheses sometimes different fictional characters, although can immensely useful tasks data analysis interpretation, without clear limitations explanatory power.","code":""},{"path":"statistical-inference.html","id":"overview-5","chapter":"6 Statistical Inference","heading":"6.2 Overview","text":"begin discussion statistical tests Permutation Randomization tests.Concepts : Permutation testConcepts II: Randomization testPractical : Randomization test real data","code":""},{"path":"statistical-inference.html","id":"why-are-these-tests-not-very-common","chapter":"6 Statistical Inference","heading":"6.2.1 Why are these tests not very common?","text":"read many research papers psychology, might see many permutation randomization tests reported. Specific disciplines sub-disciplines tend adopt specific tests various reasons, inertia can set . tests used good enough, use different ones?Permutation randomization tests good examples tests “easy” computer , “hard” person . conducting process permuting /random sampling hand can take long time. manual labor involved probably one reason tests become popular computers invented. , use specific tests discipline already firmly established computers widely available, seems permutation randomization tests fell wayside. computers available earlier , guess test used commonly today.Fortunately, computers R, can now use create evaluate permutation randomization tests.","code":""},{"path":"statistical-inference.html","id":"c1-permutation-test","chapter":"6 Statistical Inference","heading":"6.3 C1: Permutation Test","text":"","code":""},{"path":"statistical-inference.html","id":"what-is-a-permutation","chapter":"6 Statistical Inference","heading":"6.3.1 What is a permutation?","text":"permutation reordering sequence. example, can easily re-order sequence using sample() function.number permutations (unique re-orderings) n distinct items \\(n!\\)Therefore, take 5 numbers distinct values 1, 2, 3, 4, 5, able form 120 distinct orders permutations numbers.put numbers 1, 2, 3, 4, 5 basket, randomly take , chances take order 1,2,3,4,5?Well, 1 order 120, 1/120 =Let’s quickly test :quick summary. Permutations different ways order numbers. given set numbers, always total number possible orders. assume orders obtained chance, can calculate chances getting particular order.","code":"\na <- c(1,2,3,4,5)\nsample(a)\n#> [1] 4 1 2 3 5\n5*4*3*2*1\n#> [1] 120\n1/120\n#> [1] 0.008333333\n# generate 10000 random samples\nmy_samples <- replicate(10000,sample(c(1,2,3,4,5)))\n\ncount_examples <- 0 \nfor(i in 1:10000){\n  if( sum( my_samples[,i] == c(1,2,3,4,5) ) == 5) {\n    count_examples <- count_examples+1\n  } \n}\n\ncount_examples/10000\n#> [1] 0.0084"},{"path":"statistical-inference.html","id":"a-permutation-test-example","chapter":"6 Statistical Inference","heading":"6.3.2 A permutation test example","text":"8 people, assign 4 group , 4 group B. basket contains 8 balls, number , just like lottery. balls numbered 1 8, completely identical. Everybody puts blindfold goes one time take one ball.possible outcomes situation? First, \\(8! = 8*7*6*5*4*3*2*1 = 40,320\\) possible permutations, number different ways person group randomly choose ball.also means chances getting highly specific outcome, like \\(1/40320\\), 2.4801587^{-5}.chances sum balls Group different sum balls chosen Group B? figure kinds questions access permutations. Let’s make matrix possible permutations.First, install package ‘combinat’, contains permn() function. can use function create matrix permutations sequence 1 8. Note, sequence much larger, number permutations becomes large, wouldn’t recommend using function generate permutations large sequences.matrix can assume first four columns choices persons 1 4 group made, last four columns choices persons 5 8 made.Let’s now determine various sums obtained permutation. , sum ball numbers Group group B, can learn different possible outcomes actually .Now, let’s determine possible differences sums Group B.possible differences look like? Let’s make histogram.Let’s sanity check. biggest possible difference obtained? occur one group chose 4 smallest numbers (1,2,3,4), sums 10; group chose 4 biggest numbers (5,6,7,8), sums 26. Therefore, largest possible difference 26-10 = 16.largest difference observed possible_differences vector? Checks , ’s good.Let’s now answer specific questions. probability difference group sums larger 16? just determined impossible, can confidently say 0.probability absolute value difference group sums larger 10?First, convert differences positive values:figure many differences larger 10, divide total possible outcomes:","code":"\nexample_outcome <- data.frame(group = rep(c(\"A\",\"B\"),each=4),\n                              person = c(1,2,3,4,1,2,3,4),\n                              ball = sample(1:8)\n)\nknitr::kable(example_outcome)\nlibrary(combinat)\npermutation_matrix <- matrix(unlist(permn(1:8)), ncol=8, byrow=TRUE)\ngroup_A_sums <- rowSums(permutation_matrix[,1:4])\ngroup_B_sums <- rowSums(permutation_matrix[,5:8])\npossible_differences <- group_A_sums - group_B_sums\nhist(possible_differences)\nmax(possible_differences)\n#> [1] 16\npossible_differences[possible_differences > 16]\n#> numeric(0)\nabsolute_differences <- abs(possible_differences)\nlength(absolute_differences[absolute_differences > 10])/length(absolute_differences)\n#> [1] 0.1142857"},{"path":"statistical-inference.html","id":"interim-summary-general-principles","chapter":"6 Statistical Inference","heading":"6.3.3 Interim Summary: General Principles","text":"permutation test :obtain data different conditionspermute data across conditions produce possible outcomes, ways data obtained across conditionsCalculate odds specific data patterns (subset permutations) occurred relative possible permutations.generally, obtain data specific case example. “happen”. interested “happened”, generate possible outcomes. compare summary happen, happened. learn exercise? current example, learn chance can produce differences various sizes.","code":""},{"path":"statistical-inference.html","id":"permutation-test-on-experimental-data.","chapter":"6 Statistical Inference","heading":"6.3.4 Permutation test on experimental data.","text":"Consider experiment conducted two groups B. 8 total participants, randomly assigned groups. Group received 1 million dollars motivation well midterm. Group B received 0 dollars. groups took midterm. mean performance subject group 85, 75, 76, 89. mean performance subject Group B 90, 65, 68, 69.Overall mean group :, mean group B :, group , got million dollars, average better midterm group B. difference 81.25 - 73 = 8.25.","code":"\ngroup_A <- c(85,75,76,89)\ngroup_B <- c(90,65,68,69)\nmean(group_A)\n#> [1] 81.25\nmean(group_B)\n#> [1] 73\nmean(group_A) - mean(group_B)\n#> [1] 8.25"},{"path":"statistical-inference.html","id":"what-caused-the-difference","chapter":"6 Statistical Inference","heading":"6.3.4.1 What caused the difference?","text":"experimental manipulation cause difference? giving group one million dollars cause somehow better test? consider one possibility.possibilities? kind process caused pattern numbers observed? Another possible explanation random sampling, assigned participants groups. possible group randomly participants prepared test.","code":""},{"path":"statistical-inference.html","id":"assessing-chance-with-a-permutation-test","chapter":"6 Statistical Inference","heading":"6.3.4.2 Assessing chance with a permutation test","text":"can use permutation test assess possible ways participants scores assigned groups. can calculate possible group differences observed. can compare group difference observe (8.25), group differences observed. let us know random sampling process likely unlikely produce difference.possible_differences vector contains possible mean differences produced random sampling. Let’s compare observed. two ways.histogram shows possible differences obtained randomly sampling participants different groups. red line shows difference obtained. facts example.like think “distribution possible differences” window opportunity chance. , looking differences data produced chance. also see values occur often others. example, red line, observed difference 8.25 1) inside distribution, means chance produced difference, 2) kind far left, seems chance doesn’t produce difference big often.precise calculate odds getting difference 8.25 larger, find 11.4%.…return cartoon character metaphor, dunnit question. Mickey Mouse million dollars caused difference 8.25? Donald Duck random chance accidentally produced difference 8.25? know Mr. Duck produces difference 8.25 larger 10% time…enough say Mr. Duck ? don’t think . clearly done . Can absolutely certain million dollar manipulation? . learned anything useful ? think , seems got result irregular perspective chance. might give confidence experimental manipulation actually caused difference, act randomly assigning participants groups.just chance, expect experiment wouldn’t replicate, average wouldn’t big differences groups. hand, replications showed big difference , confident manipulation caused difference.","code":"\n# put all means in a variable\nall_scores <- c(group_A,group_B) \n\n# generate permutation matrix\npermutation_matrix <- matrix(unlist(permn(all_scores)), ncol=8, byrow=TRUE)\n\n# calculate overall group means for each permutation\ngroup_A_means <- rowSums(permutation_matrix[,1:4])/4\ngroup_B_means <- rowSums(permutation_matrix[,5:8])/4\n\n# generate all possible differences\npossible_differences <- group_A_means - group_B_means\n# visualize the analysis\n\nlibrary(ggplot2)\n\nqplot(possible_differences)+\n  geom_histogram(color=\"orange\")+\n  geom_vline(xintercept=mean(group_A) - mean(group_B), color =\"red\")+\n  theme_classic()\nlength(possible_differences[possible_differences >= 8.25]) / length(possible_differences)\n#> [1] 0.1142857"},{"path":"statistical-inference.html","id":"c2-randomization-test","chapter":"6 Statistical Inference","heading":"6.4 C2: Randomization test","text":"randomization test version permutation test can used number permutations large impractical generate. example, previous toy example 8 means, permuting across two groups required generating 40320 sequences. hundreds thousands subjects, computer quickly loose memory capacity generate possible permutations. Instead, randomly sample possible permutations, create sampling distribution, use become informed happened chance.Imagine 50 subjects group B. Let’s generate fake scores participant. Let’s also imagine experiment manipulation actually WORK. , pretend scores participant coming distribution, let’s say normal distribution mean = 65 sd = 10.can calculate mean group, look difference.general sense manipulation works, causes difference, expect see non-zero difference. manipulation ineffective nothing, expect average difference, however recognize obtain differences just chance alone, randomly sampling participants different groups.now want calculate sampling distribution possible mean differences. , take values Group B, randomly re-assign across groups, recalculate means mean difference. , multiple times, monte-carlo simulation.Now can plot mean differences get sense kinds differences chance produced, red line shows particular mean difference observed toy example.histogram “sampling distribution mean differences”, shows kinds differences group group B obtained purely randomly assigning people different groups.terms detective novel, chances alibi, shows chance capable . situation, chance alone easily produce differences group B large 4 -4. Notice, chance never produces difference large 20. information example, researcher attempt rule possibility chance involved producing difference observed.can also use distribution calculate specific probabilities concerning chance. example, probability getting 4 larger chance alone?","code":"\ngroup_A <- rnorm(50,65,10)\ngroup_B <- rnorm(50,65,10)\nmean(group_A)\n#> [1] 65.43351\nmean(group_B)\n#> [1] 64.66912\nmean(group_A)-mean(group_B)\n#> [1] 0.7643827\n# creating one random permutation\nall_scores <- c(group_A,group_B)\nresample <- sample(all_scores)\nnew_A_mean <- mean(resample[1:50])\nnew_B_mean <- mean(resample[51:100])\nnew_difference <- new_A_mean-new_B_mean\nnew_difference\n#> [1] 1.635763\n\n# Simulate the above process 10000 times\nmean_differences <- c()\nfor(i in 1:10000){\n  resample <- sample(all_scores)\n  new_A_mean <- mean(resample[1:50])\n  new_B_mean <- mean(resample[51:100])\n  mean_differences[i] <- new_A_mean-new_B_mean\n}\nqplot(mean_differences)+\n  geom_histogram(color=\"orange\")+\n  geom_vline(xintercept=mean(group_A) - mean(group_B), color =\"red\")+\n  theme_classic()\nlength(mean_differences[mean_differences >=4 ])/length(mean_differences)\n#> [1] 0.0186"},{"path":"statistical-inference.html","id":"p1-randomization-test-with-real-data","chapter":"6 Statistical Inference","heading":"6.5 P1: Randomization test with real data","text":"Randomization tests flexible can constructed experiments. simply involve:randomizing data across conditionscalculating statistic interestDo thousands times create sampling distribution statistic interestCompare observed statistic interest (e.g., difference group means) sampling distribution determine observed statistic ) likely produced random sampling, B) unlikely produced random samplingHere construct randomization test experiment asking question…come across smarter (interview) evaluators get hear say, get read say.can learn experiment : https://crumplab.github.io/statisticsLab/lab-7-t-test-independent-sample.html. link lab manual gives example conducting t-test, something discuss later labs. use data experiment, contained ‘open_data’ folder, titled, SchroederEpley2015data.csv, conduct randomization test.Note condition code 1 refers audio group, 0 refers reading group.can see audio group (1) received higher intellect ratings (5.63) read group (0), 3.64. difference, effect 2.difference result occurred randomly assigning people different groups? Let’s randomization test create sampling distribution possible mean differences.Now look sampling distribution possible differences, compare observed difference 2, happened:can concluded exercise? conclude experimental manipulation actually caused difference Intellect ratings? conclude chance produce difference?’m bit conservative can concluded. say exercise producing sampling distribution differences useful. gives pretty good sense kinds differences chance alone produce. shows difference 2 rarely produced chance alone. gives confidence rule chance candidate. words, really think chance great alibi, wouldn’t produced pattern data often. time, , just often…, can confident, completely sure chance didn’t .experimental manipulation…wasn’t chance caused difference, necessarily case manipulation caused difference? like saying Donald Duck cause difference, therefore manipulation cause difference. makes sense. Instead, sure, manipulation caused difference, also maybe confounds addressed caused difference. Ruling chance doesn’t tell caused difference, just suggests difference wasn’t caused chance.","code":"\n#load the data\nthe_data <- read.csv(\"open_data/SchroederEpley2015data.csv\", header = TRUE)\n\n# compute the group means\nlibrary(dplyr)\n\nthe_data %>% \n  group_by(CONDITION) %>%\n  summarize(group_means = mean(Intellect_Rating))\n#> # A tibble: 2 × 2\n#>   CONDITION group_means\n#>       <int>       <dbl>\n#> 1         0        3.65\n#> 2         1        5.63\n# restrict the dataset to the columns of interest\nsimulation_data <- the_data %>%\n  select(CONDITION,Intellect_Rating)\n\n# example of randomizing the scores across conditions\nsimulation_data %>%\n  mutate(Intellect_Rating = sample(Intellect_Rating))\n#>    CONDITION Intellect_Rating\n#> 1          1        3.6666667\n#> 2          1        4.6666667\n#> 3          1        1.6666667\n#> 4          0        5.6666667\n#> 5          0        9.0000000\n#> 6          0        3.6666667\n#> 7          1        4.6666667\n#> 8          0        1.0000000\n#> 9          1        2.3333333\n#> 10         0        6.3333333\n#> 11         0        4.6666667\n#> 12         1        3.3333333\n#> 13         1        3.6666667\n#> 14         0        3.3333333\n#> 15         0        9.0000000\n#> 16         1        5.0000000\n#> 17         1        1.6666667\n#> 18         1        2.0000000\n#> 19         0        6.0000000\n#> 20         0        4.6666667\n#> 21         0        6.6666667\n#> 22         0        5.6666667\n#> 23         0        3.3333333\n#> 24         1        3.6666667\n#> 25         1        5.6666667\n#> 26         1        5.6666667\n#> 27         0        2.3333333\n#> 28         1        3.6666667\n#> 29         1        6.0000000\n#> 30         1        6.6666667\n#> 31         0        6.0000000\n#> 32         0        5.0000000\n#> 33         1        6.0000000\n#> 34         1        5.3333333\n#> 35         0        6.0000000\n#> 36         1        0.6666667\n#> 37         0        7.0000000\n#> 38         1        5.0000000\n#> 39         1        7.6666667\n\n# example of calculating a new mean difference\n\nnew_data <- simulation_data %>%\n  mutate(Intellect_Rating = sample(Intellect_Rating)) %>%\n  group_by(CONDITION) %>%\n  summarize(new_means = mean(Intellect_Rating), .groups=\"drop\")\n\nnew_data\n#> # A tibble: 2 × 2\n#>   CONDITION new_means\n#>       <int>     <dbl>\n#> 1         0      4.81\n#> 2         1      4.63\nnew_data[new_data$CONDITION == 0,]$new_means\n#> [1] 4.814815\nnew_data[new_data$CONDITION == 1,]$new_means\n#> [1] 4.634921\nnew_difference <- new_data[new_data$CONDITION == 1,]$new_means-new_data[new_data$CONDITION == 0,]$new_means\n\n# Run a randomization test\n\npossible_differences <-c()\nfor(i in 1:1000){\n  # permute the data and calculate new means\n  new_data <- simulation_data %>%\n    mutate(Intellect_Rating = sample(Intellect_Rating)) %>%\n    group_by(CONDITION) %>%\n    summarize(new_means = mean(Intellect_Rating), .groups='drop')\n  \n  # calculate and save mean difference\n  possible_differences[i] <- new_data[new_data$CONDITION == 1,]$new_means-new_data[new_data$CONDITION == 0,]$new_means\n}\nqplot(possible_differences)+\n  geom_histogram(color=\"orange\")+\n  geom_vline(xintercept=2, color =\"red\")+\n  theme_classic()\nlength(possible_differences[possible_differences >= 2]) / length(possible_differences)\n#> [1] 0.001"},{"path":"statistical-inference.html","id":"some-coding-alternatives","chapter":"6 Statistical Inference","heading":"6.5.1 Some coding alternatives","text":"script randomization test written many different ways. additional example:","code":"\n#load the data\nthe_data <- read.csv(\"open_data/SchroederEpley2015data.csv\", header = TRUE)\n\n# compute the group means\nthe_data %>% \n  group_by(CONDITION) %>%\n  summarize(group_means = mean(Intellect_Rating))\n#> # A tibble: 2 × 2\n#>   CONDITION group_means\n#>       <int>       <dbl>\n#> 1         0        3.65\n#> 2         1        5.63\n\n# how many participants per group?\ntable(the_data$CONDITION)\n#> \n#>  0  1 \n#> 18 21\n\n# create permutations\nmean_differences <- c()\nfor(i in 1:10000){\n  resample <- sample(the_data$Intellect_Rating)\n  new_1_mean <- mean(resample[1:18])\n  new_0_mean <- mean(resample[19:39])\n  mean_differences[i] <- new_1_mean-new_0_mean\n}\n\n#plot\nqplot(mean_differences)+\n  geom_histogram(color=\"orange\")+\n  geom_vline(xintercept=2, color =\"red\")+\n  theme_classic()"},{"path":"statistical-inference.html","id":"lab-6-generalization-assignment","chapter":"6 Statistical Inference","heading":"6.6 Lab 6 Generalization Assignment","text":"","code":""},{"path":"statistical-inference.html","id":"instructions-5","chapter":"6 Statistical Inference","heading":"6.6.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab6.Rmd”Use Lab6.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 6 blackboard.one problem solve","code":""},{"path":"statistical-inference.html","id":"problems-5","chapter":"6 Statistical Inference","heading":"6.6.2 Problems","text":"Write function conducts randomization test mean difference two groups, show works. Specifically, using function, conduct randomization test data used example lab. Report results briefly discuss results randomization tell . (6 points). Extra: observed mean difference experiment found .5, concluded randomization test?Inputs:inputs include vector group 1, vector group 2, number permutations/re-samplings data create.Outputs:output group mean, difference group meansoutput histogram sampling distribution possible mean differences produced randomization processoutput probability odds obtaining observed mean difference larger.Optional:include ability calculate probability obtaining mean difference largerdeal negative difference scores appropriatelyadd one two-tailed test options","code":""},{"path":"binomial-test.html","id":"binomial-test","chapter":"7 Binomial Test","heading":"7 Binomial Test","text":"“10/8/2020 | Last Compiled: 2022-04-26”Extraordinary claims require extraordinary evidence (ECREE) - Carl Sagan","code":""},{"path":"binomial-test.html","id":"readings-1","chapter":"7 Binomial Test","heading":"7.1 Readings","text":"Vokey & Allen25, Chapter 11; Abdi, Edelman, Dowling, & Valentin26, appendix D E binomial test.Imagine wise pigeon can ask Yes question. ask question. pigeon responds pecking Yes . many answers pigeon need get correct row willing believe pigeon knew correct answer every question?someone claimed magical wise pigeon knew answer every Yes question, put test? many questions ask, satisfied extraordinary claim TRUE FALSE?can consider kinds questions perspective binomial test. , happens, binomial tests often used pigeons real research. example, pigeons excellent classifying visual patterns different categories, binomial tests often used part process establishing pigeon actually possesses visual skill/ability make discrimination, simply getting lucky.","code":""},{"path":"binomial-test.html","id":"overview-6","chapter":"7 Binomial Test","heading":"7.2 Overview","text":"Practical : Conducting binomial test RConcepts : Binomial model foundationsConcepts II: Task design binomial models","code":""},{"path":"binomial-test.html","id":"practical-i-conducting-a-binomial-test-in-r","chapter":"7 Binomial Test","heading":"7.3 Practical I: Conducting a binomial test in R","text":"already conducted binomial tests R previous labs discussed coin flipping examples. However, use language binomial test. practical example, go couple different ways conduct binomial test R.","code":""},{"path":"binomial-test.html","id":"classification-performance-example","chapter":"7 Binomial Test","heading":"7.3.1 Classification performance example","text":"Binomial tests commonly used situations researcher wants know whether subject guessing, truly able perform task. example, comparative cognition, pigeon researcher might ask whether pigeon can discriminate different classes visual stimuli, red vs. green, circles vs. squares, buildings vs. trees.Imagine pigeon given 2AFC task (two-alternative forced-choice task) discriminate pictures circular shapes angular shapes. trial pigeon shown two pictures, one circular one angular. rewarded correctly peck circular shape.Pigeon received 100 trials, pecked correct circular shape 65% trials. can conduct report binomial test follows, using binom.test() function.researcher might report results binomial test way:Pigeon 65% correct (p < .05, binomial test).Pigeon 65% (p <= .0018, binomial test).also possible embed results binomial test text .Rmd document. First, save results variable:, now possible write:Pigeon 65% correct (p <= 0.0017588)","code":"\n?binom.test\nbinom.test(x = 65,\n           n = 100,\n           p = .5,\n           alternative='greater')\n#> \n#>  Exact binomial test\n#> \n#> data:  65 and 100\n#> number of successes = 65, number of trials = 100, p-value = 0.001759\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.5639164 1.0000000\n#> sample estimates:\n#> probability of success \n#>                   0.65\ntest_results <- binom.test(x = 65,\n           n = 100,\n           p =.5,\n           alternative='greater')\n\ntest_results$p.value\n#> [1] 0.001758821\n\n# values can embedded using `r test_results$p.value`"},{"path":"binomial-test.html","id":"examining-the-report","chapter":"7 Binomial Test","heading":"7.3.2 Examining the report:","text":"binom.test() function returns printout inputs (number successes, number trials), importantly, returns p-value. p-value refers probability. However, precise meaning p-value depends alternative = c(\"two.sided\", \"less\", \"greater\") input.focus “less”, “greater” options, “two.sided” contentious (multiple interpretations /whether two-sided tests done).now, also ignore alternative hypothesis confidence interval sections. view neither outputs part binomial test.","code":"Exact binomial test\n\ndata:  65 and 100\nnumber of successes = 65, number of trials = 100,\np-value = 0.001759\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.5639164 1.0000000\nsample estimates:\nprobability of success \n                  0.65 "},{"path":"binomial-test.html","id":"what-can-we-conclude","chapter":"7 Binomial Test","heading":"7.3.3 What can we conclude?","text":"example, chose alternative= \"greater\", found p-value 0.001759. mean?Officially, probability 50/50 binomial process produce 65 successes 100 chance p = .001759. specifically, 50/50 binomial process one 50% chance success, 50% chance failure. just like coin flipping examples, except using success failure two possible outcomes instead heads tails.","code":""},{"path":"binomial-test.html","id":"what-does-this-have-to-do-with-the-pigeon","chapter":"7 Binomial Test","heading":"7.3.4 What does this have to do with the pigeon","text":"Remember research example. pigeon chose correct shape 65/100 times. can say pigeon? ability classify circular shapes angular shapes? “answer” question binomial test, using “compared ” strategy. Clearly pigeon perfectly good discrimination, otherwise pigeon got 100%. compared chance?chance ? Imagine removed pigeon entirely, replaced pigeon coin. trial flip coin choose one two shapes. Clearly, coin “knows nothing shapes”, coins “choices” “random guesses”. coin flip capable ? coin get 65/100 correct?binomial test tells us something coin done. answer pigeon. learned binomial process (coin), capable producing 65/100 greater small probability (.001759).","code":""},{"path":"binomial-test.html","id":"facts-and-inferences","chapter":"7 Binomial Test","heading":"7.3.5 Facts and Inferences","text":"example facts. pigeon got 65/100 correct. coin-flipping process get 65 heads 100 small probability .001759.facts, engage inferences attempting relate behavior pigeon coin. example, say unlikely 65% correct explained chance. gives confidence pigeon wasn’t just guessing. Just guessing wouldn’t well pigeon often. information go , probably also bet pigeon similar level performance, even better given another 100 trials.","code":""},{"path":"binomial-test.html","id":"concepts-i-binomial-model-foundations","chapter":"7 Binomial Test","heading":"7.4 Concepts I: Binomial model foundations","text":"Although can use binom.test() function conduct binomial test, can also use family binom functions already come across occasionally.example, 100 trials, limited number possible outcomes. 0 successes way 100 successes. ’s . model terms 50% success/failure, histogram shows binomial distribution situation:return example, probability getting 65 successes?probability binom.test() returned.","code":"\nlibrary(ggplot2)\nqplot(y=dbinom(0:100,100,.5))+\n  geom_bar(stat='identity',position=\"dodge\")+\n  ylab('density')+\n  xlab('# of successes')\nsum(dbinom(0:100,100,.5))\n#> [1] 1\nsum(dbinom(65:100,100,.5))\n#> [1] 0.001758821"},{"path":"binomial-test.html","id":"the-test-and-the-model","chapter":"7 Binomial Test","heading":"7.4.1 The test and the model","text":"binomial test often conducted make inferences data point. , example, flipped coin 10 times got 7 heads 10. binomial test :, p-value :worth recognizing example scratches surface full binomial model. example, 11 possible outcomes, 0 heads 10 heads. outcomes ’s probability. table shows probabilities possible outcomes:consider value thinking full model next section.","code":"\nsum(dbinom(7:10,10,.5))\n#> [1] 0.171875\nbinom.test(x=7,n=10,alternative=\"greater\")\n#> \n#>  Exact binomial test\n#> \n#> data:  7 and 10\n#> number of successes = 7, number of trials = 10, p-value = 0.1719\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.3933758 1.0000000\n#> sample estimates:\n#> probability of success \n#>                    0.7\ncoin_flips <- data.frame( number_of_heads = 0:10,\n                          probability = dbinom(x=0:10,size=10,prob= .5),\n                          cumulative = pbinom(0:10, size = 10, prob = .5),\n                          rev_cumulative = pbinom(-1:9, 10, .5, lower.tail=FALSE))\nknitr::kable(coin_flips)"},{"path":"binomial-test.html","id":"concepts-ii-task-design-and-binomial-models","chapter":"7 Binomial Test","heading":"7.5 Concepts II: Task design and binomial models","text":"Consider . appropriate conduct binomial test, situation consideration can described binomial process. words, possible describe possible outcomes experiment conducting .example, pigeon experiment, 100 trials. trial involved decision correct incorrect. , 101 total possible outcomes (0/100 100/100 correct). possibilities defined design task. 10 trials experiment, 11 possible outcomes.matter? relationship task design statistical model can greatly influence kinds inferences researcher comfortable making data. , recognizing connection allows researcher design experiment allow strong inferences first place.example, showed pigeon truck car told peck truck. correctly pecked truck. 1 trial experiment, 50% chance correct. claim pigeon can tell difference cars trucks based evidence? , 100% correct.point 100% correct good enough evidence 1 trial experiment. obvious chance get 100% correct 50% time, pigeon easily “guessing”.pigeon correctly chose truck 100% time 10 different choices, start think maybe pigeon “knows” something. know unlikely get “10---row” chance.","code":""},{"path":"binomial-test.html","id":"setting-task-parameters","chapter":"7 Binomial Test","heading":"7.5.1 Setting task parameters","text":"costs benefits changing task design. costs include longer experiment takes time resources run. benefits include confident measuring “non-chance” phenomena.designing task evaluate subject making correct incorrect decisions, many trials want experiment? possibilities:keep listing tables increasing number trials 1 time. , time generate another binomial model. tables represent possible outcomes different designs, well probabilities outcomes occur chance. general rule thumb number trials increase, range possible outcomes chance narrows toward 50% successes 50% failures.Consider value close 50%, like 51%. found participant performed 51% might want say level performance easily obtained chance. However, inference strongly depends number trials.example, 100 trials, probability getting 51% correct greater pretty good.However, 1000 trials…10,000 trials…","code":"\n# 1 trial\ntask_designs <- data.frame( number_of_heads = 0:1,\n                          probability = dbinom(x=0:1,size=1,prob= .5),\n                          cumulative = pbinom(0:1, size = 1, prob = .5),\n                          rev_cumulative = pbinom(-1:0, 1, .5, lower.tail=FALSE))\nknitr::kable(task_designs)\n\n# 2 trials\ntask_designs <- data.frame( number_of_heads = 0:2,\n                          probability = dbinom(x=0:2,size=2,prob= .5),\n                          cumulative = pbinom(0:2, size = 2, prob = .5),\n                          rev_cumulative = pbinom(-1:1, 2, .5, lower.tail=FALSE))\nknitr::kable(task_designs)\n\n# 3 trials\ntask_designs <- data.frame( number_of_heads = 0:3,\n                          probability = dbinom(x=0:3,size=3,prob= .5),\n                          cumulative = pbinom(0:3, size = 3, prob = .5),\n                          rev_cumulative = pbinom(-1:2, 2, .5, lower.tail=FALSE))\nknitr::kable(task_designs)\n\n# etc...\nbinom.test(51,100,.5,alternative=\"greater\")\n#> \n#>  Exact binomial test\n#> \n#> data:  51 and 100\n#> number of successes = 51, number of trials = 100, p-value = 0.4602\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.4234107 1.0000000\n#> sample estimates:\n#> probability of success \n#>                   0.51\nbinom.test(510,1000,.5,alternative=\"greater\")\n#> \n#>  Exact binomial test\n#> \n#> data:  510 and 1000\n#> number of successes = 510, number of trials = 1000, p-value = 0.274\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.4835011 1.0000000\n#> sample estimates:\n#> probability of success \n#>                   0.51\nbinom.test(5100,10000,.5,alternative=\"greater\")\n#> \n#>  Exact binomial test\n#> \n#> data:  5100 and 10000\n#> number of successes = 5100, number of trials = 10000, p-value = 0.02329\n#> alternative hypothesis: true probability of success is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.501726 1.000000\n#> sample estimates:\n#> probability of success \n#>                   0.51"},{"path":"binomial-test.html","id":"lab-7-generalization-assignment","chapter":"7 Binomial Test","heading":"7.6 Lab 7 Generalization Assignment","text":"","code":""},{"path":"binomial-test.html","id":"instructions-6","chapter":"7 Binomial Test","heading":"7.6.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab7.Rmd”Use Lab7.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 7 blackboard.","code":""},{"path":"binomial-test.html","id":"problems-6","chapter":"7 Binomial Test","heading":"7.6.2 Problems","text":"test-taker answered 50 true/false questions received score 60% correct. Report results binomial test explain whether think test-score produced test-taker randomly guessing question. (2 points)test-taker answered 50 true/false questions received score 60% correct. Report results binomial test explain whether think test-score produced test-taker randomly guessing question. (2 points)examiner wants make TRUE/FALSE test, still deciding many questions include. want make sure difficult simply randomly guess able score higher 55% percent. many questions examiner need use confident scores 55% higher produced chance? (2 points)examiner wants make TRUE/FALSE test, still deciding many questions include. want make sure difficult simply randomly guess able score higher 55% percent. many questions examiner need use confident scores 55% higher produced chance? (2 points)test 5 TRUE/FALSE questions (one right answer) 5 multiple choice questions four choices (one right answer).test 5 TRUE/FALSE questions (one right answer) 5 multiple choice questions four choices (one right answer).create sampling distribution probability distribution illustrate random chance process perform test. (1 point)probability randomly guessing question allow person receive 75% greater test? (1 point)","code":""},{"path":"z-tests.html","id":"z-tests","chapter":"8 Z tests","heading":"8 Z tests","text":"“10/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"z-tests.html","id":"reading-3","chapter":"8 Z tests","heading":"8.1 Reading","text":"Vokey & Allen27, Chapter 12; Abdi, Edelman, Dowling, & Valentin28 Appendix D","code":""},{"path":"z-tests.html","id":"overview-7","chapter":"8 Z tests","heading":"8.2 Overview","text":"ReviewPractical : Z-scoresConcept : Central Limit TheoremPractical II: Z-tests","code":""},{"path":"z-tests.html","id":"review-and-reminder","chapter":"8 Z tests","heading":"8.3 Review and Reminder","text":"previous labs introduced concepts 1) normal distribution 2) sampling distributions. relate concepts . Specifically, central limit theorem shows sampling distributions mean generally shape normal distribution. result, properties normal distributions can substituted properties sampling distributions process statistical inference.Normal distributions assumptions routinely appear across many remaining statistical tests (e.g., t-tests, ANOVAs, linear regression, etc.) discussed across course. , important comfortable basic properties normal distribution.example, reminder, normal distribution two parameters, mean standard deviation. example, formula normal distribution (probability density function) :\\(f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-u}{\\sigma})^2}\\), two free parameters \\(u\\) (mean), \\(sigma\\) (standard deviation). calculating probability density particular score using formula R, show answer dnorm().second reminder normal distributions shape. important understand “shape” means. Let’s look two different normal distributions. first one mean = 0, sd = 1. second mean = 25, sd = 5. two graphs appear show different shapes, first taller narrower second.However, x-axes different graphs. allow x-axes vary freely, can “zoom” distribution way, now see:, distributions similar shape, “seemingly” exactly shape…second one shorter spread . Nevertheless, shape still EXACTLY , one important way. Specifically, probability getting scores particular ranges, expressed standard deviations, .probability distribution functions, area curve represents possible outcomes. , area curve range values (relative total area), represents probability observing values range. crucial point look comparable ranges standard deviations normal distribution, always get probabilities.example, consider range 1 2 first graph (mean = 0, sd = 1). comparable range 30 35 second graph (mean = 25, sd =5). represent interval 1 2 standard deviations. interval shaded red .\ncalculate area shaded regions relative total area properly perform integration. However, let’s skip part, something quick R. example, sum numbers two vectors first_one second_one, approximate “total area”. , vector, sum numbers (probability density values) range. divide sums approximate probability, check see .values approximate values pnorm() function.","code":"\nx <- 1\nu <- 0\nsigma <- 1\n\n1/(sigma*sqrt(2*pi)) * exp(1)^(-.5*((x-u)/sigma)^2)\n#> [1] 0.2419707\n\ndnorm(1,0,1)\n#> [1] 0.2419707\nfirst_one <- dnorm(seq(-4,4,length.out=100),0,1)\nsecond_one <- dnorm(seq(5,45,length.out=100),25,5) \n\nplot_df <- data.frame(score=c(first_one,second_one),\n                      x = c(seq(-4,4,length.out=100),seq(5,45,length.out=100)),\n                      zscore = c(seq(-4,4,length.out=100),(seq(5,45,length.out=100)-25)/5),\n                      dist = rep(c(\"first\",\"second\"),each=100))\n  \nlibrary(ggplot2)\n\nggplot(plot_df, aes(y=score,x=x))+\n  geom_line()+\n  facet_wrap(~dist)\nggplot(plot_df, aes(y=score,x=x))+\n  geom_line()+\n  facet_wrap(~dist, scales=\"free_x\")\nlibrary(dplyr)\nggplot(plot_df, aes(y=score,x=x))+\n  geom_line()+\n  facet_wrap(~dist, scales=\"free_x\")+\n  geom_ribbon(data = plot_df %>%filter(zscore >= 1,\n                                       zscore <= 2),\n              fill = \"red\",\n              aes(ymin=0,ymax=score))\nlibrary(dplyr)\n\nplot_df %>%\n  filter(dist == \"first\",\n         x > 1,\n         x <= 2) %>%\n  select(score) %>%\n  sum()/sum(first_one)\n#> [1] 0.1443879\n\nplot_df %>%\n  filter(dist == \"second\",\n         x > 30,\n         x <= 35) %>%\n  select(score) %>%\n  sum()/sum(second_one)\n#> [1] 0.1443879\npnorm(2,0,1) - pnorm(1,0,1)\n#> [1] 0.1359051\n\npnorm(35,25,5)- pnorm(30,25,5)\n#> [1] 0.1359051"},{"path":"z-tests.html","id":"convenience","chapter":"8 Z tests","heading":"8.3.1 Convenience","text":"heading main part lab ’ll suggest convenience often important statistics. real life, uncommon people use tools convenient use. normal distribution number conveniences, one reason widely used. can use concept convenience gauge understanding normal distributions. example, understand normal distributions able explain least reasons distributions convenient use.one convenience. described normal distributions shape. also looked “special” version normal, called unit normal, also called standard normal, z-distribution. defined normal distribution mean = 0 , sd = 1. unit normal can convenient quick estimation. see , let’s turn first practical section lab, z-scores.","code":""},{"path":"z-tests.html","id":"practical-i-z-scores","chapter":"8 Z tests","heading":"8.4 Practical I: z-scores","text":"Let admit often confused metric system imperial system, partly grew Canada live USA, also systems used inconsistently personal experience. example, even though grew metric system, know weight pounds, kilograms. Also, know height feet inches, meters. Temperature even confusing. ’m good cold temperatures celsius, use Fahrenheit hot temperatures. ’m still better kilometers miles. Oh well.","code":""},{"path":"z-tests.html","id":"linear-transformation","chapter":"8 Z tests","heading":"8.4.1 Linear Transformation","text":"bring issues differences metric imperial essentially inconsequential. matter whether use Celsius Fahrenheit, temperature measured . Similarly, distance two cities matter whether use miles kilometers. difference measurement scale. case, differences linear transformations , means one scale shifted constant amount relative scale.example, formula translate celsius fahrenheit :\\(F = C * 9/5 + 32\\)can use formula draw graph relating different values Celsius, different values Fahrenheit. “transform” Celsius values multiplying constant (9/5), adding constant (32), therefore relationship linear.Z-scores express raw scores normal distribution terms far away mean standard deviation units.Z-scores linear transformations. convert raw values normal distribution “unit normal” distribution. conversion like transformation Celsius Fahrenheit, nothing lost conversion, values refer underlying quantity.z-score transformation :\\(z_i = \\frac{x_i - u}{\\sigma}\\) \\(z_i = \\frac{\\text{score} - \\text{mean}}{\\text{standard deviation}}\\)two important transformations formula. subtraction centers scores mean. division expresses scores terms common unit (sometimes z-scores also called standard scores, “standardized”, case “standard deviation”).elaborate, top part formula centers scores around mean. mean normal distribution 25, sampled score 25, z-score 0 (25-25 = 0). indicates score deviate mean, score case mean (deviates 0 mean).score 30? score 30 5 mean (30-25 = 5). score 10 -15 mean. centering transformation causes values mean receive negative z-score, values mean positive z-score.next step formula divide. already centered scores using mean, division allows centered value brought common frame reference, unit. process also commonly termed “standardizing” “normalizing”. example, example, score 30, know 5 mean 25. However, said nothing standard deviation. 5 mean large deviation? small deviation? depends much spread normal distribution, controlled standard deviation \\(\\sigma\\) parameter normal distribution. standard deviation 1, far away score 30 mean (25)? five standard deviations, far away values distribution. standard deviation 10, far away score 30 mean (25)? .5 standard deviation, fairly close many numbers distribution.Let’s say normal distribution mean = 25, standard deviation = 5. score distribution can converted unit normal distribution 1) subtracting mean, dividing standard deviation. graph x-axis top representing scores raw distribution (centered 25), x axis bottom representing scores unit normal z-distribution.Z-scores convenient know interpret . example, find Fahrenheit convenient use hot temperatures. know 109 degrees Fahrenheit outside, know really hot. honest, don’t good intuitive feeling really hot Celsius. , don’t find Celsius convenient hot temperatures. become convenient got used practice. , already something works, stick (convenience inertia strong forces…).Z-scores can become convenient get used represent. example, statistics textbooks commonly present graph table like following:spend time unit normal distribution, learn basic properties, 95% scores fall -2 +2 standard deviations; 99% scores fall -3 3 standard deviations, 34% scores fall mean one standard deviation. memorized details, whenever three things future…1) score normal distribution, 2) mean, 3) standard deviation…can good intuitions whether score common rare…can convenient (happen interacting normal distributions regular basis).","code":"\n\ncelsius <- seq(-50,50,1)\nfahrenheit <- seq(-50,50,1) * (9/5) + 32\n\nplot_df <- data.frame(celsius,\n                       fahrenheit)\n\nggplot(plot_df, aes(x=celsius,y=fahrenheit))+\n  geom_line()+\n  scale_y_continuous(breaks=seq(-60,140,10))+\n  scale_x_continuous(breaks=seq(-50,50,10))\npdf <- dnorm(seq(-4,4,length.out=100),0,1)\nzscores <- seq(-4,4,length.out=100)\nraw <- zscores*10+100\nplot_df <- data.frame(pdf,\n                      zscores,\n                      raw)\n\nggplot(plot_df,aes(x=zscores,y=pdf))+\n  geom_line()+\n  scale_x_continuous(breaks=c(-4:4), \n                     sec.axis = sec_axis(~ .*5+25, name=\"Raw Scores\",breaks=seq(5,50,5)))\nknitr::include_graphics(\"imgs/norm_zscores.png\")"},{"path":"z-tests.html","id":"conceptual-i-central-limit-theorem","chapter":"8 Z tests","heading":"8.5 Conceptual I: Central Limit Theorem","text":"turns many common statistics rely upon normal distributions, common statistics, frequently use normal distributions. one good reason become familiar basic properties.normal distributions commonly used statistics? think multiple reasons, including convenience (math can done hand), inertia (normals can convenient work , people use , keep using ). However, also fundamental reason stemming central limit theorem.present central limit theorem terms one thing already learned , make grand claim .","code":""},{"path":"z-tests.html","id":"we-already-learned-about-sampling-distributions","chapter":"8 Z tests","heading":"8.5.1 We already learned about sampling distributions","text":"Remember, sampling distribution 1) take multiple samples, 2) calculate statistic like mean sample, 3) look distribution sample statistic, sample statistics look like. important remember statistic compute sample descriptive statistic, like mean, standard deviation, median, whatever want.","code":""},{"path":"z-tests.html","id":"claim-sampling-distributions-of-the-mean-are-normally-distributed","chapter":"8 Z tests","heading":"8.5.2 Claim: sampling distributions of the mean are normally distributed","text":"central limit theorem roughly two parts. main part sampling distributions mean normally distributed. second part “time”. central limit theorem therefore mostly let’s treat sampling distributions mean normal distributions. sampling distributions, especially sampling distributions mean, extremely common fundamental statistical inference, math behind normal distributions well understood, can blame central limit theorem frequency deal normal distributions statistics.","code":""},{"path":"z-tests.html","id":"implication","chapter":"8 Z tests","heading":"8.5.3 Implication","text":"major implication central limit theorem 1) even scores come non-normal distribution, 2) sampling distribution sample means approximately normally distributed.Let’s use R illustrate implication.First, imagine measurement involves taking scores uniform distribution (flat, definitely normal).histogram (sample means) much normal looking compared flat distribution individual scores came . course, done simulation, wonder simulated distribution sample means really approximate normal distribution? raises general question…distribution numbers, suspect normal distribution, can know distribution normal?Instead developing test normality, let’s try something pretty straightforward quick “gut check”. Based learned z-scores, know normal distributions shape. , probability getting score 0 1 standard deviations normal distributions.ask find proportion numbers falling 0 1 standard deviations sampling distribution created…see value pretty close.’ve just done rough checking found sampling distribution mean seems approximately normal (based minimal comparison). relationship often holds, common use math normal distributions work sampling distributions mean.see later, sampling distributions create normal distributions, special property sampling distributions sample means. later tests, look sampling distributions statistics t-statistic, F-statistic. distributed normally, different distributions (t F distributions) used place normal.","code":"\n# parent distribution is a uniform\nraw_scores <- runif(10000,min = 0,100)\nhist(raw_scores)\nsample_means <- replicate(10000,mean(runif(5,0,100)))\nhist(sample_means)\npnorm(1,0,1) - pnorm(0,0,1)\n#> [1] 0.3413447\nto_z <- (sample_means-mean(sample_means))/sd(sample_means)\nlength(to_z[to_z > 0 & to_z < 1])/10000\n#> [1] 0.3305"},{"path":"z-tests.html","id":"conceptual-ii-z-tests","chapter":"8 Z tests","heading":"8.6 Conceptual II: z-tests","text":"central limit theorem, long run, large enough sample-size, many statistical tests converge z-test. see later discussing t-test.z-test can used statistical inference population parameters set sample means 1) known normal, 2) mean standard deviation also known. WARNING: assumptions rarely met, facts rarely known. , z-tests commonly used compared statistical tests, especially psychological research usually impossible know properties “true” distribution measurements come .","code":""},{"path":"z-tests.html","id":"n1-example","chapter":"8 Z tests","heading":"8.6.1 N=1 example","text":"“z-test” “z-scores” N 1. example, know measurement involves sampling score normal distribution, know mean standard deviation distribution. can use knowledge normal distributions determine probability obtaining specified ranges scores.probability obtaining score larger 5 normal distribution mean = 1, sd = 3?","code":"\n# one-tailed\npnorm(5,1,3, lower.tail=FALSE)\n#> [1] 0.09121122\n\n# using zscores\npnorm((5-1)/3,0,1, lower.tail=FALSE)\n#> [1] 0.09121122\n\n# two-tailed\npnorm(5,1,3, lower.tail=FALSE)*2\n#> [1] 0.1824224"},{"path":"z-tests.html","id":"n-1-example","chapter":"8 Z tests","heading":"8.6.2 N > 1 example","text":"Z-tests become complicated sample-size greater 1. example, imagine take 10 scores (n=10) normal distribution mean = 55, sd = 5. kinds things happen ?Let’s focus question sample mean. sample n=10, sample mean ? sample mean 60 strange? can randomly sampling numbers case?first need compute sampling distribution mean, tell us kinds sample means (n=10) observed randomly sampling normal mean =55 sd =5.can estimate simulation:, use analytic formulas “know” (least long). example, expect likely sample mean mean population (remember sample mean unbiased estimator population mean). , mean sampling distribution expected 55.Remember previous lab sampling distributions mentioned “standard deviation sampling distribution sample means standard error mean”. case, standard error mean can computed directly:\\(\\text{SEM} = \\frac{\\sigma}{\\sqrt{N}}\\)\\(\\sigma\\) standard deviation parent population (5), N number observations sample (10). calculate directly, find value similar one found simulation., now found parameters normal distribution actually interested , sampling distribution sample means particular situation. , can z-tests want (using mean sd sampling distribution).probability getting sample mean higher 60, sample n=10, individual scores came normal distribution mean = 55 sd = 5.","code":"\nsample_means <- replicate(10000,mean(rnorm(10,55,5)))\nhist(sample_means)\nmean(sample_means)\n#> [1] 54.99784\nsd(sample_means)\n#> [1] 1.583107\n5/sqrt(10)\n#> [1] 1.581139\n# use mean and sd in pnorm\npnorm(60, 55, 5/sqrt(10), lower.tail=FALSE )\n#> [1] 0.0007827011\n\n# OR, convert to a zscore first\nzscore <- (60-55)/(5/sqrt(10))\npnorm(zscore,0,1, lower.tail = FALSE)\n#> [1] 0.0007827011\n\n# this is similar to what the simulation showed:\nlength(sample_means[sample_means > 60])/10000\n#> [1] 0.0014"},{"path":"z-tests.html","id":"example-3-differences-between-groups","chapter":"8 Z tests","heading":"8.6.3 Example 3: differences between groups","text":"standardized educational test known distributional properties. Average test performance distributed normally. mean normal distribution 55, standard deviation 5.experimenter want determine “special training” can improve test performance. create two groups randomly assign 10 participants group. One group gets TRAINING (control group), another group gets TRAINING.training works expect TRAINING group better TRAINING group averageYou recognize people can get different means test just chance alone.can set standards determine evaluate data? kind evidence convince training worked…caused difference BEYOND CHANCE PRODUCED?answer, least insight question, can answered z-test even conduct experiment.first consider question, “chance ?”. two groups, B, NEITHER received training. experiment, random assignment groups, manipulation plausibly change test scores. randomly put 10 people group , 10 people group B, make take test, measure mean difference group B, can happen chance alone?Let’s simulate sampling distribution mean differences situationWhen assume sample means group B come exact distribution, subtract means group B create sampling distribution mean differences, create null-distribution, null-hypothesis, shows kinds mean differences observed due random sampling. chance experiment. Without getting specific, thought “special training” improve test scores scenario 30%, way outside window, way chance alone (30% BEYOND chance can ). hand, ran experiment found training improved test performance 3%…well, often get difference 3% better chance? answer can found evaluating distribution:According simulation, scores 3% greater occur p <= 0.0943. “one-tailed” test. wanted know often get score large 3% away mean either direction, two-tailed test:two p-values close estimates p-values get z-test. little bit simulation slightly imperfect.z-test real, need observed mean difference two groups. Let’s say 3% difference test. also need mean standard deviation normal distribution 3% difference came . can calculate probability obtaining 3% difference larger chance. IMPORTANTLY, mean standard deviation need null-distribution specific difference two sample means, original parent distribution scores come .know individuals take test, test means come normal distribution mean = 55, sd =5.put 10 people group 10 group B, assume difference groups, expect test scores random samples parent distribution 1.result, expect chance alone, difference mean group mean group (sample size 10), variability.central limit theorem, sampling distribution mean differences shape normal distributionThe mean sampling distribution mean differences 0, average difference two samples taken population 0.standard deviation distribution mean differences? knew value, perform z-test.estimate value simulation calculating standard deviation:turns slightly different analytic formula standard error mean situation:\\(\\text{SEM} = \\frac{\\sqrt{2}\\sigma}{\\sqrt{N}}\\)formula required distribution mean differences subtraction mean group B, n=10. , expected variation changes little bit, account added \\(\\sqrt{2}\\) formula.","code":"\nmean_differences <- replicate(10000,mean(rnorm(10,55,5))-mean(rnorm(10,55,5)))\nhist(mean_differences)\nlength(mean_differences[mean_differences > 3])/10000\n#> [1] 0.0943\nlength(mean_differences[mean_differences > 3 | mean_differences < -3])/10000\n#> [1] 0.1843\nsd(mean_differences)\n#> [1] 2.248508\n(sqrt(2)*5)/sqrt(10)\n#> [1] 2.236068"},{"path":"z-tests.html","id":"doing-the-z-test","chapter":"8 Z tests","heading":"8.6.4 doing the z-test","text":", better 3%, showing group difference least 3% something happens often chance?can see, p-value found z-test basically one found simulation approach.","code":"\n#one-tailed test\nzscore <- (58-55)/((sqrt(2)*5)/sqrt(10))\npnorm(zscore,0,1, lower.tail = FALSE)\n#> [1] 0.08985625"},{"path":"z-tests.html","id":"lab-8-generalization-assignment","chapter":"8 Z tests","heading":"8.7 Lab 8 Generalization Assignment","text":"","code":""},{"path":"z-tests.html","id":"instructions-7","chapter":"8 Z tests","heading":"8.7.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab8.Rmd”Use Lab8.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 8 blackboard.","code":""},{"path":"z-tests.html","id":"problems-7","chapter":"8 Z tests","heading":"8.7.2 Problems","text":"Write function convert vector raw-scores z-scores. function inputs vector, mean sd normal distribution, return vector zscores. (1 point). Also, demonstrate function works correctly (1 point). make demonstration .Advanced: option function convert raw scores zscores one two ways:using user provided mean standard deviationusing calculated mean standard deviation raw scoresBase R function z-test. Write function accomplish one-sample z-test. Remember, one-sample z test used compare probability obtaining sample mean (larger smaller) came known normal distribution. (2 points).Use z-test function conduct test following. sample 25 scores taken. mean sample 50. sample assumed taken normal distribution mean 40 standard deviation 7. Report one-tailed z-test, examining probability obtaining sample greater 50 situation. Report results, give brief sentence explaining result inference make (2 points).","code":""},{"path":"chi-square.html","id":"chi-square","chapter":"9 Chi Square","heading":"9 Chi Square","text":"“10/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"chi-square.html","id":"reading-4","chapter":"9 Chi Square","heading":"9.1 Reading","text":"Vokey & Allen29, Chapter 13.","code":""},{"path":"chi-square.html","id":"overview-8","chapter":"9 Chi Square","heading":"9.2 Overview","text":"lab provides conceptual foundation understanding chi square test using R.","code":""},{"path":"chi-square.html","id":"background","chapter":"9 Chi Square","heading":"9.3 Background","text":"","code":""},{"path":"chi-square.html","id":"a-brief-history","chapter":"9 Chi Square","heading":"9.3.1 A brief history","text":"Karl Pearson described chi-square test 1900.30 Also see Plackett, R. L.31 additional context development test. Relatedly, Pearson undeniably large impact discipline statistics; although socio-historical account beyond scope lab, worth pointing Pearson (like many contemporaries) heavily involved eugenics movement,32 developed statistical techniques, also applied causes (interested readers see examples Pearson’s publications eugenics journals).","code":""},{"path":"chi-square.html","id":"chi-square-distributions-have-fundamental-properties-that-make-them-widespread-in-statistics","chapter":"9 Chi Square","heading":"9.3.2 Chi-square distributions have fundamental properties that make them widespread in statistics","text":"chi-square (\\(\\chi^2\\)) test, statistic, associated distribution fundamental many aspects statistics. full accounting many connections mathematical relationships beyond scope lab (see wikipedia articles chi-square test, chi-square distribution ).","code":""},{"path":"chi-square.html","id":"debate-about-correct-usage","chapter":"9 Chi Square","heading":"9.3.3 Debate about correct usage","text":"chi-square test multiple uses psychology, including tests independence goodness fit. “correct” usage chi-square tests without debate. example, roughly 50 years Pearson, Lewis, D., & Burke, C. J.33 wrote lengthy paper describing “uses misuses” chi-square tests psychology. several replies authors identified “misusing” chi-square test. recently, suggestions Lewis & Burke34 revisited Delucchi, K. L.35 papers scratch surface many uses misuses chi-square tests psychology. ongoing goal labs develop conceptual understanding statistics use can justify usage appropriate analysis.","code":""},{"path":"chi-square.html","id":"connection-to-previous-lab-concepts","chapter":"9 Chi Square","heading":"9.3.4 Connection to previous lab concepts","text":"previous labs conducted statistical inference adopting similar general procedures. obtain sample data. consider sample arisen random sampling, construct sampling distribution. compare sample data sampling distribution see likely unlikely produced chance. Sometimes simulated sampling distribution, times used formulas compute precise probabilities.chi-square test another specific example general procedure described . can obtain sample data, compute \\(\\chi^2\\) statistic sample data, compare statistic reference null distribution determine probability obtaining value chance.","code":""},{"path":"chi-square.html","id":"practical-i-chisq.test-in-r","chapter":"9 Chi Square","heading":"9.4 Practical I: chisq.test() in R","text":"Base R comes several functions chi-square tests, including chisq.test() family \\(\\chi^2\\) distribution functions: dchisqu(), pchisqu(), qchisqu(), rchisqu().","code":""},{"path":"chi-square.html","id":"chisq.test","chapter":"9 Chi Square","heading":"9.4.1 chisq.test()","text":"chisq.test() function performs basic tests independence vectors contingency tables.","code":"\n?chisq.test"},{"path":"chi-square.html","id":"test-for-a-frequency-vector","chapter":"9 Chi Square","heading":"9.4.1.1 Test for a frequency vector","text":"Inputting single vector values conducts chi-square test N-1 degrees freedom. test assumes equal probability outcome default, reports chi-square sample statistic, well p-value associated \\(\\chi^2\\) distribution N-1 degrees freedom.Toss coin 50 times receive 20 heads 30 tails. Conduct chi-square test independence, assume theoretically expected frequencies 25 25.possible specify different theoretical probabilities:vector can length, e.g., roll dice 120 times count number times number 1 6 occurs, conduct chi-square test independence:","code":"\nmy_vals <- c(20,30)\n(xsq <- chisq.test(my_vals))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  my_vals\n#> X-squared = 2, df = 1, p-value = 0.1573\n\nxsq$statistic\n#> X-squared \n#>         2\nxsq$observed\n#> [1] 20 30\nxsq$expected\n#> [1] 25 25\nxsq$residuals\n#> [1] -1  1\n\n\nsum(((xsq$observed - xsq$expected)^2) / xsq$expected)\n#> [1] 2\nmy_vals <- c(20,30)\nchisq.test(my_vals, p = c(.25,.75))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  my_vals\n#> X-squared = 6, df = 1, p-value = 0.01431\nmy_vals <- c(20,30,10,10,30,30)\nchisq.test(my_vals)\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  my_vals\n#> X-squared = 22.308, df = 5, p-value = 0.0004576"},{"path":"chi-square.html","id":"independence-test-for-a-contingency-table","chapter":"9 Chi Square","heading":"9.4.2 Independence test for a contingency table","text":"matrix describing contingency table positive values rows columns can also inputted directly function. , null hypothesis “joint distribution cell counts contingency table product row column marginals”. degrees freedom defined \\((r-1)(c-1)\\), \\(r\\) number rows \\(c\\) number columns.following example help file:","code":"\nM <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\ndimnames(M) <- list(gender = c(\"F\", \"M\"),\n                    party = c(\"Democrat\",\"Independent\", \"Republican\"))\n(Xsq <- chisq.test(M))  # Prints test summary\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  M\n#> X-squared = 30.07, df = 2, p-value = 2.954e-07\nXsq$observed   # observed counts (same as M)\n#>       party\n#> gender Democrat Independent Republican\n#>      F      762         327        468\n#>      M      484         239        477\nXsq$expected   # expected counts under the null\n#>       party\n#> gender Democrat Independent Republican\n#>      F 703.6714    319.6453   533.6834\n#>      M 542.3286    246.3547   411.3166\nXsq$residuals  # Pearson residuals\n#>       party\n#> gender   Democrat Independent Republican\n#>      F  2.1988558   0.4113702 -2.8432397\n#>      M -2.5046695  -0.4685829  3.2386734\nXsq$stdres     # standardized residuals\n#>       party\n#> gender   Democrat Independent Republican\n#>      F  4.5020535   0.6994517 -5.3159455\n#>      M -4.5020535  -0.6994517  5.3159455\nXsq$p.value\n#> [1] 2.953589e-07\nXsq$statistic\n#> X-squared \n#>  30.07015"},{"path":"chi-square.html","id":"conceptual-i-chi2-distribution-sample-statistic-and-test","chapter":"9 Chi Square","heading":"9.5 Conceptual I: \\(\\chi^2\\) distribution, sample statistic, and test","text":"Chi-square (\\(\\chi^2\\)) statistics can confusing \\(\\chi^2\\) can refer distributions, sample statistic, statistical inference tests.\\(\\chi^2\\) distribution family distributions arise sum squared values random samples unit normal distribution:\\(\\chi^2 = \\sum_{=1}^k Z_i^2\\), \\(Z_i^2\\) random deviate unit normal distribution (mean = 0, sd =1), \\(k\\) number random samples (also known degrees freedom, number samples free independently vary). \\(k=1\\), \\(\\chi^2\\) distribution unit normal distribution squared.\\(\\chi^2\\) sample statistic formula can applied frequency data summarize amount observed frequencies differ theoretically expected frequencies.\\(\\chi^2 = \\sum{\\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}}}\\)\\(\\chi^2 = \\sum_{=1}^n{\\frac{(\\text{O}_i - \\text{E}_i)^2}{\\text{E}_i}}\\)\\(\\chi^2\\) statistical tests (test independence, goodness fit) used inference role chance producing observed frequency data. process involves computing \\(\\chi^2\\) sample statistic observed frequency data, comparing obtained value \\(\\chi^2\\) distribution degrees freedom sample. probability obtaining \\(\\chi^2\\) sample statistic larger generally approximates probability independent random sampling process produced deviations expected frequencies large larger found sample.","code":""},{"path":"chi-square.html","id":"the-chi2-sample-statistic","chapter":"9 Chi Square","heading":"9.5.1 The \\(\\chi^2\\) sample statistic","text":"two ways writing formula \\(\\chi^2\\) sample statistic.\\(\\chi^2 = \\sum{\\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}}}\\)\\(\\chi^2 = \\sum_{=1}^n{\\frac{(\\text{O}_i - \\text{E}_i)^2}{\\text{E}_i}}\\)\\(\\chi^2\\) sample statistic used summarize obtained frequency data, specifically way relates obtained frequencies theoretically expected frequencies.example, tossed coin 50 times, found following observed frequencies heads tails, compare expected frequencies coin fair.Thus, outcome observed value (\\(O_i\\)) expected value (\\(E_i\\)), \\(\\chi^2\\) can computed:shows two ways compute \\(\\chi^2\\) example, including using base R function \\(chisq.test()\\). obtained value .32 case fairly small differences obtained expected frequencies fairly small. differences larger, \\(\\chi^2\\) sample statistic much larger, e.g:times base R function returned \\(\\chi^2\\) sample statistic (computed data assumed theoretical frequencies). function also return information degrees freedom (df), p-value. additional values refer information \\(\\chi^2\\) distribution. appropriate conditions, \\(\\chi^2\\) sample statistic can compared \\(\\chi^2\\) distribution purposes statistical inference.","code":"\ncoin_toss <- data.frame(outcome = c(\"H\",\"T\"),\n                        O = c(23,27),\n                        E = c(25,25))\nknitr::kable(coin_toss)\nlibrary(dplyr)\n\ncoin_toss <- coin_toss %>%\n  mutate(d = O - E) %>%\n  mutate(d_sq = d^2) %>%\n  mutate(div = d_sq/E )\n\nknitr::kable(coin_toss)\n\n# compute chi-square\nsum(coin_toss$div)\n#> [1] 0.32\n\n# compute chi-square \nO <- c(23,27)\nE <- c(25,25)\nsum(((O-E)^2)/E)\n#> [1] 0.32\n\n# compute chi-square\nchisq.test(x=c(23,27))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  c(23, 27)\n#> X-squared = 0.32, df = 1, p-value = 0.5716\nchisq.test(x=c(47,3))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  c(47, 3)\n#> X-squared = 38.72, df = 1, p-value = 4.892e-10"},{"path":"chi-square.html","id":"the-chi2-distribution","chapter":"9 Chi Square","heading":"9.5.2 The \\(\\chi^2\\) distribution","text":"shape \\(\\chi^2\\) distribution depends parameter called \\(k\\). , \\(k = 1\\), \\(\\chi^2\\) distribution defined unit normal distribution squared.R distribution functions \\(\\chi^2\\), including dchisq(), pchisq(), qchisq(), rchisq(). , sampled random deviates using rchisq(), k = 1 (equivalent df = 1), get histogram :, simplest, \\(\\chi^2\\) just normal distribution squared.\\(k > 1\\), \\(\\chi^2\\) distribution defined :\\(\\chi^2 = \\sum_{=1}^k Z_i^2\\), \\(Z_i\\) independent samples unit normal distribution. words, \\(\\chi^2\\) distribution sum squared values unit normal distribution, \\(k\\) number independent samples squared sum.clarify, let’s use R:can get glimpse \\(\\chi^2\\) distribution looks like across range \\(k\\) plotting pdf (probability density function), using dchisq().\n### Developing intuitions \\(\\chi^2\\)Let’s say randomly sampled 5 numbers unit normal distribution (mean = 0 sd =1), squared numbers, added . expect number ?answer number distributed \\(\\chi^2\\) \\(k=5\\), referring situation summing squares 5 samples unit normal distribution.\\(\\chi^2\\) distribution change shape \\(k\\) increases? \\(k\\) big number like 60, kind shape expect? know \\(k\\), expect mean \\(\\chi^2\\) distribution ? Answering questions requires building intuition \\(\\chi^2\\).following, take random samples unit normal distribution:expected mean unit normal distribution?square values sample unit normal distribution (equivalent \\(\\chi^2\\) \\(k=1\\)), mean squared values ?turns answer 1. negative values squaring everything. 68% values unit normal -1 1, squaring make values 0 1, rest values get increasingly bigger 1. balance 1, mean squared normal distribution. words, mean \\(\\chi^2\\) \\(k\\) parameter, also called degrees freedom.example, mean 10,000 numbers drawn \\(\\chi^2\\) k = 10, :Another way think recognize expected value (mean) squared unit normal distribution 1. , take 10 values distribution (.e., k = 10), planning sum 10 values get, expected value 1…summing 10 ones, gives 10. expectations can applied \\(\\chi^2\\) distributions \\(k\\).","code":"\n# normal histogram\nhist(rnorm(10000,0,1), breaks = 100)\n\n# chi-squared with k = 1\nhist(rnorm(10000,0,1)^2, breaks = 100)\nhist(rchisq(10000,1), breaks=100)\n# k = 1\nfrom_normal <- replicate(10000, rnorm(1,0,1)^2)\nfrom_chisq  <- rchisq(10000,1)\nplot_df <- data.frame(values = c(from_normal,\n                                 from_chisq),\n                      source = c(\"normal^2\",\"chisq\"))\n\nlibrary(ggplot2)\nggplot(plot_df, aes(x=values))+\n  geom_histogram(bins=100)+\n  ggtitle(\"k=1\")+\n  facet_wrap(~source)\n# k = 2\nfrom_normal <- replicate(10000, sum(rnorm(2,0,1)^2))\nfrom_chisq  <- rchisq(10000,2)\nplot_df <- data.frame(values = c(from_normal,\n                                 from_chisq),\n                      source = c(\"normal^2\",\"chisq\"))\n\nlibrary(ggplot2)\nggplot(plot_df, aes(x=values))+\n  geom_histogram(bins=100)+\n  ggtitle(\"k=2\")+\n  facet_wrap(~source)\n# k = 3\nfrom_normal <- replicate(10000, sum(rnorm(3,0,1)^2))\nfrom_chisq  <- rchisq(10000,3)\nplot_df <- data.frame(values = c(from_normal,\n                                 from_chisq),\n                      source = c(\"normal^2\",\"chisq\"))\n\nlibrary(ggplot2)\nggplot(plot_df, aes(x=values))+\n  geom_histogram(bins=100)+\n  ggtitle(\"k=3\")+\n  facet_wrap(~source)\n# k = 5\nfrom_normal <- replicate(10000, sum(rnorm(5,0,1)^2))\nfrom_chisq  <- rchisq(10000,5)\nplot_df <- data.frame(values = c(from_normal,\n                                 from_chisq),\n                      source = c(\"normal^2\",\"chisq\"))\n\nlibrary(ggplot2)\nggplot(plot_df, aes(x=values))+\n  geom_histogram(bins=100)+\n  ggtitle(\"k=5\")+\n  facet_wrap(~source)\n\nplot_df <- data.frame(values = c(dchisq(x=seq(0,20,length.out = 100), df=1),\n                                 dchisq(x=seq(0,20,length.out = 100), df=3),\n                                 dchisq(x=seq(0,20,length.out = 100), df=5),\n                                 dchisq(x=seq(0,20,length.out = 100), df=9),\n                                 dchisq(x=seq(0,20,length.out = 100), df=11)\n                                 ),\n                      x = rep(seq(0,20,length.out = 100),5),\n                      k = as.factor(rep(c(1,3,5,9,11), each = 100))\n                      )\n\nggplot(plot_df, aes(x = x, y=values, color=k, group=k))+\n  geom_line()+\n  ylab(\"density\")+\n  xlab(\"chi-squared\")+\n  scale_x_continuous(breaks=0:20)\na<- replicate(10000,sum(rnorm(5,0,1)^2))\nhist(a)\nmean(rnorm(10000,0,1))\n#> [1] -0.002371537\nmean(rnorm(10000,0,1)^2)\n#> [1] 1.000881\nmean(rchisq(10000,10))\n#> [1] 10.00974"},{"path":"chi-square.html","id":"conceptual-ii-examining-the-approximation","chapter":"9 Chi Square","heading":"9.6 Conceptual II: Examining the approximation","text":"lecture discussed binomial distribution converges normal distribution long run. one properties allows \\(\\chi^2\\) distribution approximate properties binomial distribution.First, can visually see binomial distribution becomes normally distributed long run simulation. simulation involves 10,000 sets coin flips. first histogram involves set 10 coin flips, displays frequency possible outcome (# heads)s. second histogram shows sets 100 coin flips. , range possible outcomes increases, appear distributed normally. number flips set increases, distribution possible outcomes approaches normal distribution.\nSecond, let’s develop sense idea \\(\\chi^2\\) test approximation binomial test.Consider another coin flipping scenario. Let’s say coin flipped 10 times, 2 heads. p-value two-tailed test, specifically probabiltiy getting 2 less heads, 8 heads.?use binomial test, compute exact probability.also use \\(\\chi^2\\) test approximation:case wouldn’t good reason use \\(\\chi^2\\) test, binomial test provides exact probability. Also, expected frequencies small , p-value \\(\\chi^2\\) test half small .However, consider sets coin flips much larger 10, increase expected frequencies (allows closer convergence normal distribution), \\(\\chi^2\\) binomial tests return p-values increasingly similar.","code":"\n# flip a coin 10 times\nhist(rbinom(10000,10,.5), breaks=seq(0,10,1))\n\n# flip a coin 100 times\nhist(rbinom(10000,100,.5), breaks=seq(20,80,1))\npbinom(2,10,.5, lower.tail = TRUE)*2\n#> [1] 0.109375\nchisq.test(c(2,8))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  c(2, 8)\n#> X-squared = 3.6, df = 1, p-value = 0.05778\npbinom(40,100,.5, lower.tail = TRUE)*2\n#> [1] 0.05688793\nchisq.test(c(40,60))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  c(40, 60)\n#> X-squared = 4, df = 1, p-value = 0.0455\npbinom(450,1000,.5, lower.tail = TRUE)*2\n#> [1] 0.001730536\nchisq.test(c(450,550))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  c(450, 550)\n#> X-squared = 10, df = 1, p-value = 0.001565\npbinom(4900,10000,.5, lower.tail = TRUE)*2\n#> [1] 0.04658553\nchisq.test(c(4900,5100))\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  c(4900, 5100)\n#> X-squared = 4, df = 1, p-value = 0.0455"},{"path":"chi-square.html","id":"lab-9-generalization-assignment","chapter":"9 Chi Square","heading":"9.7 Lab 9 Generalization Assignment","text":"","code":""},{"path":"chi-square.html","id":"instructions-8","chapter":"9 Chi Square","heading":"9.7.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab9.Rmd”Use Lab9.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 9 blackboard.","code":""},{"path":"chi-square.html","id":"problems-8","chapter":"9 Chi Square","heading":"9.7.2 Problems","text":"following paper links open data, describes design two chi-square tests performed Experiment 1 (copy paper made available).Silver, . M., Stahl, . E., Loiotile, R., Smith-Flores, . S., & Feigenson, L. (2020). Choosing Leads Liking: Choice-Induced Preference Infancy. Psychological Science, 0956797620954491.Obtain data online repository, show code loading R, conduct tests reported Experiment 1 authors conducted. include one binomial test, two chi-square tests. Briefly report re-analysis, discuss whether obtained values authors (6 points).Important Note: re-analysis able obtain values authors provided results section. However, view authors also misused chi-square test, especially test independence involving age. , ok unable reproduce analysis. However, instructive try reproduce authors form opinion whether test applied sound manner.Solution script: also providing .rmd lab 9 wrote solution video https://github.com/CrumpLab/psyc7709Lab/blob/master/lab_solutions/Lab9.Rmd.Update misuse chi-square test paper. discussed class solution video, appears accidentally found example recent literature chi-square test used incorrectly. solution video didn’t provide clear reason demonstrate chi-square test misused. , thought write addendum lab.recap, authors measured choice made 21 infants. age infant measured months (two decimal places). reported chi-square test independence determine whether age independent choice. example, reported: \\(\\chi^2\\) (19,N=21) =18.24,p=.506. values sense correct. example:However, appears authors treated infant’s ages, measured continuous variable, categorical variable. , happened, two infants happened exactly 11.66 months old. result, 21 infants, 20 different age categories. contingency table constructed represent 20 age categories, 2 choice options, get 20x2 table. table (20-1)(2-1) = 19 degrees freedom. solution video showed example authors might constructed table data, able obtain chi-square value reported, suggesting construct table.several problems . One problems focus conversion continuous age variable categorical variable. expecting authors bin ages, say two categories: younger vs. older. Instead, used infant’s age category level. happened case two infants exactly age (11.66 months), hadn’t happened, table 21 levels age (21 infants).Consider occurs treat subject unique level contingency table, especially experiment involves subject making single choice two alternatives. see answer , always get chi-square value. matter subjects .example contingency table 5 subjects. case, column represents subject. Row 1 represents choice , row 2 choice 2. subject makes choice, choice counted appropriate row.contingency table, possible compute chi-square test table.However, look happens. obtain chi-squared value 5. , always happen, matter choices subject makes:Every time function runs, choices made simulated subject randomized. However, matter happens, chi-square value always five.effectively authors . example, consider happen authors data excluded two infants exact age. table choices 19 infants.can simulate possible outcome way:matter choices infants make (either B), always make one , . , observed chi-square value always number subjects. , test confirm number infants used experiment assess independence age choices.Finally, create contingency table subject category level…conduct chi-square test independence see subjects independent choices made, think major problem subjects couldn’t independent choices made, ones making choices.","code":"\npchisq(18.24, 19, lower.tail = FALSE)\n#> [1] 0.5064639\nreplicate(5,sample(c(1,0),2))\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    1    1    1    0    1\n#> [2,]    0    0    0    1    0\nchisq.test(replicate(5,sample(c(1,0),2)))\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  replicate(5, sample(c(1, 0), 2))\n#> X-squared = 5, df = 4, p-value = 0.2873\nchisq.test(replicate(5,sample(c(1,0),2)))\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  replicate(5, sample(c(1, 0), 2))\n#> X-squared = 5, df = 4, p-value = 0.2873\nchisq.test(replicate(19,sample(c(1,0),2)))\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  replicate(19, sample(c(1, 0), 2))\n#> X-squared = 19, df = 18, p-value = 0.3918"},{"path":"t-tests.html","id":"t-tests","chapter":"10 T-tests","heading":"10 T-tests","text":"“10/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"t-tests.html","id":"reading-5","chapter":"10 T-tests","heading":"10.1 Reading","text":"Vokey & Allen36, Chapter 14; Crump, Navarro, & Suzuki37, Chapter 6","code":""},{"path":"t-tests.html","id":"overview-9","chapter":"10 T-tests","heading":"10.2 Overview","text":"lab demonstrates conduct one sample, paired sample, independent sample t-tests R, uses R tool develop insight conceptual foundations t-test.","code":""},{"path":"t-tests.html","id":"historical-background","chapter":"10 T-tests","heading":"10.3 Historical Background","text":"William Sealy Gosset published t-test pseudonym “Student”, test sometimes called “Student’s t-test.”38 dispute origin meaning \\(t\\). One hypothesis \\(s\\) commonly used time refer sample statistics, Gosset chose \\(t\\) next letter, perhaps indicating “step-” thinking sample statistics? Gosset published pseudonym employee Guinness Breweries time, hired examine issues making inferences small samples brewing beer. test developed intellectual property Guinness, Gosset thought test broadly used, published pseudonym protect job. Pearson, E. S.39 provides biography Gosset. Sawilowsky, S. S., & Blair, R. C.40 conduct simulations examine robust t-test violations assumptions.","code":""},{"path":"t-tests.html","id":"practical-i-t.test","chapter":"10 T-tests","heading":"10.4 Practical I: t.test()","text":"Base R includes t.test() function computes several forms t-tests.three quick examples computing one sample, paired sample independent sample t-tests using R.","code":"\n?t.test"},{"path":"t-tests.html","id":"one-sample-t-test","chapter":"10 T-tests","heading":"10.4.1 One-sample t-test","text":"","code":"\nsome_random_means <- rnorm(10,0,1)\nt.test(some_random_means, mu=0)\n#> \n#>  One Sample t-test\n#> \n#> data:  some_random_means\n#> t = -0.47971, df = 9, p-value = 0.6429\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.5577339  0.3625736\n#> sample estimates:\n#>   mean of x \n#> -0.09758017"},{"path":"t-tests.html","id":"paired-sample-t-test","chapter":"10 T-tests","heading":"10.4.2 Paired-sample t-test","text":"","code":"\nA_means <- rnorm(10,0,1)\nB_means <- rnorm(10,0,1)\n\nt.test(A_means,B_means,paired=TRUE)\n#> \n#>  Paired t-test\n#> \n#> data:  A_means and B_means\n#> t = -2.3541, df = 9, p-value = 0.04301\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.16192615 -0.04307544\n#> sample estimates:\n#> mean of the differences \n#>               -1.102501"},{"path":"t-tests.html","id":"independent-sample-t-test","chapter":"10 T-tests","heading":"10.4.3 Independent-sample t-test","text":"","code":"\nA_means <- rnorm(10,0,1)\nB_means <- rnorm(10,0,1)\n\nt.test(A_means,B_means, var.equal=TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  A_means and B_means\n#> t = -0.37344, df = 18, p-value = 0.7132\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.2643478  0.8827081\n#> sample estimates:\n#>  mean of x  mean of y \n#> -0.3786486 -0.1878288"},{"path":"t-tests.html","id":"formula-syntax-for-data-frames","chapter":"10 T-tests","heading":"10.4.4 formula syntax for data frames","text":"examples, t.test() function applied vectors containing sample means. also possible apply t.test() function long data frames using ~ syntax.","code":"\nmy_data <- data.frame(group = rep(c(\"A\",\"B\"), each=10),\n                      means = rnorm(20,0,1))\n\nt.test(means~group, var.equal=TRUE, data=my_data)\n#> \n#>  Two Sample t-test\n#> \n#> data:  means by group\n#> t = -2.9425, df = 18, p-value = 0.008707\n#> alternative hypothesis: true difference in means between group A and group B is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.7676000 -0.2949594\n#> sample estimates:\n#> mean in group A mean in group B \n#>      -0.8516509       0.1796288"},{"path":"t-tests.html","id":"one-or-two-sided-test","chapter":"10 T-tests","heading":"10.4.5 one or two-sided test","text":"default, t.test() function provides two sided test, options can specified using alternative = c(\"two.sided\", \"less\", \"greater\") input parameter.","code":"\nsome_random_means <- rnorm(10,0,1)\nt.test(some_random_means, mu=0, alternative = \"two.sided\")\n#> \n#>  One Sample t-test\n#> \n#> data:  some_random_means\n#> t = -2.2015, df = 9, p-value = 0.05521\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.01340444  0.01377459\n#> sample estimates:\n#>  mean of x \n#> -0.4998149\nt.test(some_random_means, mu=0, alternative = \"less\")\n#> \n#>  One Sample t-test\n#> \n#> data:  some_random_means\n#> t = -2.2015, df = 9, p-value = 0.0276\n#> alternative hypothesis: true mean is less than 0\n#> 95 percent confidence interval:\n#>         -Inf -0.08363359\n#> sample estimates:\n#>  mean of x \n#> -0.4998149\nt.test(some_random_means, mu=0, alternative = \"greater\")\n#> \n#>  One Sample t-test\n#> \n#> data:  some_random_means\n#> t = -2.2015, df = 9, p-value = 0.9724\n#> alternative hypothesis: true mean is greater than 0\n#> 95 percent confidence interval:\n#>  -0.9159963        Inf\n#> sample estimates:\n#>  mean of x \n#> -0.4998149"},{"path":"t-tests.html","id":"var.equal-and-welchs-correction","chapter":"10 T-tests","heading":"10.4.6 var.equal and Welch’s correction","text":"t.test() function also makes default assumptions indpendent samples test. , default applies correction called Welch’s correction. correction related assumption equal variances samples group.conduct t-test without correction set var.equal=TRUE. applies independent sample case two variances.","code":"\nA <- rnorm(10,0,1)\nB <- rnorm(10,0,1)\nt.test(A,B)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  A and B\n#> t = -0.93736, df = 17.819, p-value = 0.3611\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.4402070  0.5519985\n#> sample estimates:\n#>  mean of x  mean of y \n#> -0.2656442  0.1784601\nt.test(A,B, var.equal=TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  A and B\n#> t = -0.93736, df = 18, p-value = 0.361\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.4394817  0.5512733\n#> sample estimates:\n#>  mean of x  mean of y \n#> -0.2656442  0.1784601"},{"path":"t-tests.html","id":"t.test-contents","chapter":"10 T-tests","heading":"10.4.7 t.test() contents","text":"t.test() function two kinds outputs. First, prints results console (saw ). Second, outputs list containing components test. individual pieces t-test can saved accessed putting results t.test new variable.example:","code":"\n (my_results <- t.test(A,B, var.equal=TRUE))\n#> \n#>  Two Sample t-test\n#> \n#> data:  A and B\n#> t = -0.93736, df = 18, p-value = 0.361\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.4394817  0.5512733\n#> sample estimates:\n#>  mean of x  mean of y \n#> -0.2656442  0.1784601\nmy_results$statistic\n#>          t \n#> -0.9373613\nmy_results$parameter\n#> df \n#> 18\nmy_results$p.value\n#> [1] 0.3609806\nmy_results$estimate\n#>  mean of x  mean of y \n#> -0.2656442  0.1784601"},{"path":"t-tests.html","id":"papaja-reporting-with-apa_print","chapter":"10 T-tests","heading":"10.4.8 papaja reporting with apa_print()","text":"papaja package convenient functions automating writing t-test results.example, t-test result, \\(t(18) = -0.94\\), \\(p = .361\\), printed using r code snippet inserted text .Rmd.","code":"\nlibrary(papaja)\napa_print(my_results)\n#> $estimate\n#> [1] \"$\\\\Delta M = -0.44$, 95\\\\% CI $[-1.44, 0.55]$\"\n#> \n#> $statistic\n#> [1] \"$t(18) = -0.94$, $p = .361$\"\n#> \n#> $full_result\n#> [1] \"$\\\\Delta M = -0.44$, 95\\\\% CI $[-1.44, 0.55]$, $t(18) = -0.94$, $p = .361$\"\n#> \n#> $table\n#> A data.frame with 5 labelled columns:\n#> \n#>   estimate      conf.int statistic df p.value\n#> 1    -0.44 [-1.44, 0.55]     -0.94 18    .361\n#> \n#> estimate : $\\\\Delta M$ \n#> conf.int : 95\\\\% CI \n#> statistic: $t$ \n#> df       : $\\\\mathit{df}$ \n#> p.value  : $p$ \n#> attr(,\"class\")\n#> [1] \"apa_results\" \"list\""},{"path":"t-tests.html","id":"conceptual-i-simulating-the-t-test","chapter":"10 T-tests","heading":"10.5 Conceptual I: Simulating the t-test","text":"section create simulations various components independent samples t-test. One goal make code general, can simulate wide range designs.simulate experimental situation involving two groups B. assume equal number subjects (N), subject measured similar number times (X times). also simulate null-hypothesis, assumes experimental manipulation ineffective. result, assume subjects B randomly sampled underlying distribution. assume raw scores subject come normal distribution.","code":""},{"path":"t-tests.html","id":"simulating-a-single-experiment","chapter":"10 T-tests","heading":"10.5.1 Simulating a single experiment","text":"assumptions place, possible simulate results single experiment.one alternative, another example one-liner:","code":"\n#subjects per group\nN <- 10\n# measurements per subject\nX <- 2\n\n# distribution assumptions\nA_mean <- 100\nB_mean <- 100\n\nA_sd <- 25\nB_sd <- 25\nA_scores <- rnorm(N*X,A_mean,A_sd)\nB_scores <- rnorm(N*X,B_mean,B_sd)\n\nsim_data <- data.frame(groups = rep(c(\"A\",\"B\"),each = N*X),\n                       subjects = rep(rep(1:N,each = X),2),\n                       scores = c(A_scores,B_scores))\n\nlibrary(dplyr)\n\nsubject_means <- sim_data %>%\n  group_by(groups,subjects) %>%\n  summarize(means = mean(scores), .groups = 'drop')\n\nt.test(means~groups, var.equal =TRUE,data = subject_means)\n#> \n#>  Two Sample t-test\n#> \n#> data:  means by groups\n#> t = -0.024415, df = 18, p-value = 0.9808\n#> alternative hypothesis: true difference in means between group A and group B is not equal to 0\n#> 95 percent confidence interval:\n#>  -19.15564  18.71554\n#> sample estimates:\n#> mean in group A mean in group B \n#>        101.6500        101.8701\nt.test(replicate(N, mean(rnorm(X, A_mean, A_sd))),\n       replicate(N, mean(rnorm(X, B_mean, B_sd))),\n       var.equal = TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  replicate(N, mean(rnorm(X, A_mean, A_sd))) and replicate(N, mean(rnorm(X, B_mean, B_sd)))\n#> t = 0.67402, df = 18, p-value = 0.5089\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -14.41213  28.02779\n#> sample estimates:\n#> mean of x mean of y \n#>  98.22765  91.41983"},{"path":"t-tests.html","id":"simulating-distributions-of-experiments","chapter":"10 T-tests","heading":"10.5.2 Simulating distributions of experiments","text":"“experiment may regarded forming individual ‘population’ experiments might performed conditions. series experiments sample drawn population.” — William Sealy Gossett.41This quote first sentence Student’s formative paper t-test. previous section simulated single experiment. conduct simulation several thousand times, can create “population” experiments occurred conditions. general idea create sampling distribution experiments, compare results actual experiment distributions possible experiments.","code":""},{"path":"t-tests.html","id":"the-distribution-of-mean-differences","chapter":"10 T-tests","heading":"10.5.2.1 The distribution of mean differences","text":"experiment repeated 1000 times, time mean difference Group B. Thus, mean difference sample statistic used summarize result experiment, sampling distribution mean differences estimated simulation:","code":"\nsim_mean_diffs <- replicate(1000, mean(replicate(N, mean(rnorm(X, A_mean, A_sd)))) - mean(replicate(N, mean(rnorm(X, B_mean, B_sd)))))\n\nhist(sim_mean_diffs)"},{"path":"t-tests.html","id":"the-distribution-of-t","chapter":"10 T-tests","heading":"10.5.2.2 The distribution of t","text":"Student used \\(t\\) formula, rather mean differences summarize sample data two group experiment. \\(t\\) formula normalizes mean difference estimated standard error mean.\\(t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p\\sqrt{2/n}}\\)\\(s_p = \\sqrt{\\frac{s^2_{X_1} + s^2_{X_2}}{2}}\\)Although already distribution functions t-values, t-distribution constructed simulation. histogram 1000 \\(t\\) values happened.","code":"\nsim_ts <- replicate(1000, t.test(replicate(N, mean(rnorm(X, A_mean, A_sd))),\n                                 replicate(N, mean(rnorm(X, B_mean, B_sd))),\n                                 var.equal = TRUE)$statistic)\nhist(sim_ts)"},{"path":"t-tests.html","id":"t-distribution-functions","chapter":"10 T-tests","heading":"10.5.3 t distribution functions","text":"R comes dt, pt, qt, rt family function t-distributions well.example, rather simulating t values , sample 1000 \\(t\\) values t-distribution df = 18.\ndt function used draw probability density function t-distributions across range degrees freedom.discussed lecture, t-distribution approaches normal distribution degrees freedom increase. example, 95% values normal distribution smaller Z = 1.644854 standard deviations. corresponding 95% values t shown approach 1.644 degrees freedom increase.","code":"\nhist(rt(1000,df=18))\nlibrary(ggplot2)\nplot_df <- data.frame(values = c(dt(x=seq(-5,5,length.out = 100), df=1),\n                                 dt(x=seq(-5,5,length.out = 100), df=3),\n                                 dt(x=seq(-5,5,length.out = 100), df=5),\n                                 dt(x=seq(-5,5,length.out = 100), df=9),\n                                 dt(x=seq(-5,5,length.out = 100), df=11)\n                                 ),\n                      x = rep(seq(-5,5,length.out = 100),5),\n                      df = as.factor(rep(c(1,3,5,9,11), each = 100))\n                      )\n\nggplot(plot_df, aes(x = x, y=values, color=df, group=df))+\n  geom_line()+\n  ylab(\"density\")+\n  xlab(\"t\")+\n  scale_x_continuous(breaks=-5:5)\nqnorm(.95,0,1)\n#> [1] 1.644854\nqt(p=.95,df=c(1,5,10,100,1000))\n#> [1] 6.313752 2.015048 1.812461 1.660234 1.646379"},{"path":"t-tests.html","id":"conceptual-ii-simulating-power-curves","chapter":"10 T-tests","heading":"10.6 Conceptual II: Simulating power curves","text":"simulations assumed experimental manipulation effect, therefore expectation measurements groups taken distribution, differences groups explained random sampling.Another possibility experimental manipulation effective caused difference groups. case expectations measurements group different distributions. Specifically, causal force manipulation assumed change performance, resulting systematic changes scores group received manipulation. experimental manipulation principle change almost property distribution, differences means often focus research interest.","code":""},{"path":"t-tests.html","id":"what-proportion-of-experiments-would-be-significant","chapter":"10 T-tests","heading":"10.6.1 What proportion of experiments would be significant…?","text":"purpose conceptual section use simulation techniques ask proportion experiments significant (alpha level, say p<.05) experimental manipulation worked caused difference group means.First, confident basic answer. set alpha criterion p<.05, proportion experiments significant according null-hypothesis (difference groups)? can conduct simulation, see many experiments 1000 returned p-value less .05. answer , definition, approximately .05.next question something like, proportion experiments significant actually difference means group B. example, assume group mean 0, group B mean .25, distributions standard deviation (sd=1). design, N = 10 group, 5 scores measured per subject, probability getting p < .05 much higher 5%.","code":"\nsim_ps <- replicate(1000, t.test(replicate(10, mean(rnorm(5, 0, 1))),\n                                 replicate(10, mean(rnorm(5, 0, 1))),\n                                 var.equal = TRUE)$p.value)\nhist(sim_ps)\nlength(sim_ps[sim_ps < .05])/1000\n#> [1] 0.045\nsim_ps <- replicate(1000, t.test(replicate(10, mean(rnorm(5, 0, 1))),\n                                 replicate(10, mean(rnorm(5, .25, 1))),\n                                 var.equal = TRUE)$p.value)\nhist(sim_ps)\nlength(sim_ps[sim_ps < .05])/1000\n#> [1] 0.224"},{"path":"t-tests.html","id":"effect-size","chapter":"10 T-tests","heading":"10.6.2 Effect-size","text":"working toward plotting power-curves, show us probability getting significant result function design parameters like number subjects, number scores per subject (observations per cell), well assumed “effect-size” manipulation. Effect size regular meaning, idea experimental manipulation can cause change different amounts, manipulations might big effects (e.g., cause really big change mean), small effects.Effect-size can also refer specific statistical units. example, Cohen proposed effect-sizes mean differences two groups standardized. example, mean difference group B, clear big small difference. underlying distributions unit normal distributions, mean shift 1 large, represents shifting distribution whole standard deviation. underlying distributions large standard deviation, say 100, shifting mean 1 isn’t large respect total variability., Cohen’s effect size can expressed idea normalize mean difference standard deviation.\\(\\text{Cohen's d} = \\frac{\\text{mean difference}}{\\text{standard deviation}}\\)make use ideas power-curve simulations. Specifically, convenience, use unit normal distributions parent distributions group scores. , simulate differences mean groups. result, increase mean difference .25, .5, 1, another number, can interpret differences terms standard deviation units.","code":""},{"path":"t-tests.html","id":"power-curve-as-a-function-of-effect-size","chapter":"10 T-tests","heading":"10.6.3 Power-curve as a function of effect-size","text":"example conduct range simulations assuming effect-size increases 0 1.5, steps .1. simulations design parameters: N=10 subjects per group, X=5 scores taken subject. power curve shows proportion experiments return result p < .05 level effect-size. example, assumed effect-size 1, design detect effect size p<.05 level close 100% time. However, “power” design detect smaller effects decreases following curve.","code":"\neffect_sizes <- seq(0,1.5,.1)\nprop_significant <-c()\n\nfor(i in 1:length(effect_sizes)){\n  sim_ps <- replicate(1000, t.test(replicate(10, mean(rnorm(5, 0, 1))),\n                                   replicate(10, mean(rnorm(5, effect_sizes[i], 1))),\n                                   var.equal = TRUE)$p.value)\n  \n  prop_significant[i] <- length(sim_ps[sim_ps < .05])/1000\n}\n\nplot_df <- data.frame(effect_sizes,\n                      prop_significant)\n\nggplot(plot_df, aes(x=effect_sizes,y=prop_significant))+\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks=seq(0,1.5,.1))+\n  scale_y_continuous(breaks=seq(0,1,.1)) +\n  ylab(\"Proportion Significant\")"},{"path":"t-tests.html","id":"lab-10-generalization-assignment","chapter":"10 T-tests","heading":"10.7 Lab 10 Generalization Assignment","text":"","code":""},{"path":"t-tests.html","id":"instructions-9","chapter":"10 T-tests","heading":"10.7.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab10.Rmd”Use Lab10.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 10 blackboard.","code":""},{"path":"t-tests.html","id":"problems-9","chapter":"10 T-tests","heading":"10.7.2 Problems","text":"task obtain data following paper conduct reproducible analysis results.Rosenbaum, D., Mama, Y., & Algom, D. (2017). Stand Stroop: Standing Enhances Selective Attention Cognitive Control. Psychological science, 28(12), 1864-1867.Note, paper, data, existing reproducible analysis data available https://crumplab.github.io/statisticsLab/lab-10-factorial-anova.html#important-stuff-4The re-analysis focus Experiment 3. three main goalsReproduce much analysis possible using paired-sample t-tests. Note, authors reported 2x2 repeated measures ANOVA, consider questions answered t-tests (2 points)Reproduce graph means, like shown paper (2 points)Present power-curve analysis design. (2 points)Note copy R markdown document described solution video can found github repository course: https://github.com/CrumpLab/psyc7709Lab/tree/master/lab_solutions","code":""},{"path":"simulation-and-power-analysis.html","id":"simulation-and-power-analysis","chapter":"Simulation and Power Analysis","heading":"Simulation and Power Analysis","text":"“11/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"simulation-and-power-analysis.html","id":"reading-6","chapter":"Simulation and Power Analysis","heading":"10.8 Reading","text":"Crump, Navarro, & Suzuki42, Chapter 12 Effect size power","code":""},{"path":"simulation-and-power-analysis.html","id":"overview-10","chapter":"Simulation and Power Analysis","heading":"10.9 Overview","text":"Part semester long project conduct simulation-based power-analysis. document provides background power-analyses use simulations sample-size design planning. Attempts made integrate additional examples remaining labs semester next move complicated designs statistical tests.Develop deeper understanding assumptions behind statistical testsSample-size planning power-analysisUnderstand real data behave given assumptions making data.","code":""},{"path":"simulation-and-power-analysis.html","id":"null-hypothesis","chapter":"Simulation and Power Analysis","heading":"10.10 Null-hypothesis","text":"general, null-hypothesis hypothesis experimental manipulation didn’t work. , hypothesis differences.can simulate null-hypotheses R experimental design. following way:Use R generate sample data condition designMake sure sample data comes distribution conditions (ensure differences)Compute test-statistic simulation, save , repeat create sampling distribution test statistic.sampling distribution test-statistic null-distribution. use set alpha criterion, hypothesis testing.","code":""},{"path":"simulation-and-power-analysis.html","id":"null-for-a-t-test","chapter":"Simulation and Power Analysis","heading":"10.10.1 Null for a t-test","text":"ran simulation 1000 times. definition, get approximately 5% simulations returning p-value less .05. increase number simulations, get accurate answer converges 5% every time.","code":"\n# samples A and B come from the same normal distribution\nA <- rnorm(n=10,mean=10, sd=5)\nB <- rnorm(n=10,mean=10, sd=5)\n\n# the pvalue for this one pretend simulation\nt.test(A,B,var.equal=TRUE)$p.value\n#> [1] 0.292469\n\n# running the simulation\n# everytime we run this function we do one simulated experiment and return the p-value\nsim_null <- function(){\n  A <- rnorm(n=10,mean=10, sd=5)\n  B <- rnorm(n=10,mean=10, sd=5)\n  return(t.test(A,B,var.equal=TRUE)$p.value)\n}\n\n# use replicate to run the sim many times\noutcomes <- replicate(1000,sim_null())\n\n# plot the null-distribution of p-values\nhist(outcomes)\n\n# proportion of simulated experiments had a p-value less than .05\nlength(outcomes[outcomes<.05])/1000\n#> [1] 0.052"},{"path":"simulation-and-power-analysis.html","id":"alternative-hypothesis","chapter":"Simulation and Power Analysis","heading":"10.11 Alternative hypothesis","text":"general alternative null hypothesis (differences), often called alternative hypothesis: manipulation cause difference. specifically, infinite number possible alternative hypotheses, involving difference specific size.Consider . often find p-value less .05 mean difference 5 sample B. Let’s use parameters , except time sample different distributions B.programmed mean difference 5 sample B, found p-values less .05 much higher proportion time. sensible, really difference samples (put ).","code":"\n  A <- rnorm(n=10,mean=10, sd=5)\n  B <- rnorm(n=10,mean=15, sd=5)\n  t.test(A,B,var.equal=TRUE)$p.value\n#> [1] 0.06901522\n\n# make the mean for B 15 (5 more than A)\nsim_alternative <- function(){\n  A <- rnorm(n=10,mean=10, sd=5)\n  B <- rnorm(n=10,mean=15, sd=5)\n  return(t.test(A,B,var.equal=TRUE)$p.value)\n}\n\n# use replicate to run the sim many times\noutcomes <- replicate(1000,sim_alternative())\n\n# plot the distribution of p-values\nhist(outcomes)\n\n# proportion of simulated experiments had a p-value less than .05\nlength(outcomes[outcomes<.05])/1000\n#> [1] 0.595"},{"path":"simulation-and-power-analysis.html","id":"power-and-effect-size","chapter":"Simulation and Power Analysis","heading":"10.12 Power and Effect Size","text":"Power: probability rejecting null-hypothesis, given true effect size.Effect-size: general, ’s assumed size difference. example, assumed difference 5, assumed effect-size 5.many ways define measure effect-size. Perhaps common way Cohen’s D. Cohen’s D expresses mean difference terms standard deviation units. example, distributions standard deviation 5. mean 10, mean B 15. Using Cohen’s D, effect-size 1. 15 1 standard deviation away 10 (standard deviation also 5).calculated proportion simulations returned p-value less .05, found power design detect effect-size 1.Power depends three major things:Sample-sizeEffect-sizeAlpha-criterionPower property design. power design increases sample-size increases. power design increases actual true effect-size increases. power design increases alpha criterion increases (e.g, going .05 .1, making easier reject null).","code":""},{"path":"simulation-and-power-analysis.html","id":"power-analysis-with-r","chapter":"Simulation and Power Analysis","heading":"10.13 Power analysis with R","text":"many packages functions power analysis. Power analysis important planning design. example, can determine many subjects need order high probability detecting true effect (particular size) really .","code":""},{"path":"simulation-and-power-analysis.html","id":"pwr-package","chapter":"Simulation and Power Analysis","heading":"10.13.1 pwr package","text":"example using pwr package find power independent sample t-test, n=10, detect effect-size 1. answer similar simulations answer. simulation converge answer increased number simulations.mentioned many functions directly computing power R. Feel free use . class, learn use simulation conduct power analyses. can redundant approach necessary, given functions can use. Additionally, get exact solutions (approximate ones). Nevertheless, existing power functions can limited may apply design. simulation approach can extended design. Learning run simulations also improve statistical sensibilities, power calculations become less black box.Two things moving onto simulation: power-curves, sample-size planning.","code":"\nlibrary(pwr)\npwr.t.test(n=10,\n           d=1,\n           sig.level=.05,\n           type=\"two.sample\",\n           alternative=\"two.sided\")\n#> \n#>      Two-sample t test power calculation \n#> \n#>               n = 10\n#>               d = 1\n#>       sig.level = 0.05\n#>           power = 0.5620066\n#>     alternative = two.sided\n#> \n#> NOTE: n is number in *each* group"},{"path":"simulation-and-power-analysis.html","id":"power-curves","chapter":"Simulation and Power Analysis","heading":"10.13.2 power-curves","text":"design’s power relation true effect-size. design different levels power detect different sized effects. Let’s make power curve see power t-test independent samples (n=10) detect range effect-sizesThis power curve applies independent-sample t-tests n=10. property, fact designs. Every design ’s power curve. power curve shows us happen (average), true state world involves effects different sizes.know power-curve design, know sensitive design detecting effects particular sizes. might accidentally using powered design, small chance detecting effect size interested .know power-curve design, better position plan experiment, example modifying number subjects run.","code":"\neffect_sizes <- seq(.1,2,.1)\npower <- sapply(effect_sizes, \n          FUN = function(x) {\n            pwr.t.test(n=100,\n            d=x,\n            sig.level=.05,\n            type=\"two.sample\",\n            alternative=\"two.sided\")$power})\nplot_df <- data.frame(effect_sizes,power)\n\nlibrary(ggplot2)\nggplot(plot_df, aes(x=effect_sizes,\n                    y=power))+\n  geom_point()+\n  geom_line()"},{"path":"simulation-and-power-analysis.html","id":"sample-size-planning","chapter":"Simulation and Power Analysis","heading":"10.13.3 Sample-size planning","text":"one way plan number subjects need find effect interest.Establish smallest effect-size interestCreate curve showing power design function number subjects detect smallest effect-size interest.’s clear establish smallest effect-size interest. let’s say interested detecting effect least d = .2. means two conditions differ least .2 standard deviation shift. find something smaller , let’s say wouldn’t care wouldn’t big enough care. many subjects need high powered design, one almost always reject null-hypothesis?independent samples t-test:Well, looks like need many subjects high power. example, want detect effect 95% time, need around 650 subjects. ’s worth kind analysis see design checks . don’t want waste time running experiment designed fail (even true effect real).","code":"\nnum_subjects <- seq(10,1000,10)\npower <- sapply(num_subjects, \n          FUN = function(x) {\n            pwr.t.test(n=x,\n            d=.2,\n            sig.level=.05,\n            type=\"two.sample\",\n            alternative=\"two.sided\")$power})\nplot_df <- data.frame(num_subjects,power)\n\nlibrary(ggplot2)\nggplot(plot_df, aes(x=num_subjects,\n                    y=power))+\n  geom_point()+\n  geom_line()"},{"path":"simulation-and-power-analysis.html","id":"simulation-approach-to-power-calculations","chapter":"Simulation and Power Analysis","heading":"10.14 Simulation approach to power calculations","text":"simulation approach power analysis involves steps:Use R sample numbers condition design.can set properties (e.g., n, mean, sd, kind distribution etc.) sample condition, mimic type expected patternAnalyze simulated data obtain p-value (use analysis appropriate design)Repeat many times, save p-valuesCompute power determining proportion simulated p-values less alpha criterion.simulations, increasing number simulations improve accuracy results. use 1000 simulations throughout. 10,000 better, might take just little bit longer.","code":""},{"path":"simulation-and-power-analysis.html","id":"simulated-power-for-a-t-test","chapter":"Simulation and Power Analysis","heading":"10.14.1 Simulated power for a t-test","text":"power curve n=10.case, obvious benefit computing power-curve simulation. answer get similar answer got using pwr package, simulation answer noisy. bother simulation?One answer bother question can simulate deeper aspects design get refined answers without work math.","code":"\n# function to run a simulated t-test\nsim_power <- function(x){\n  A <- rnorm(n=10,mean=0, sd=1)\n  B <- rnorm(n=10,mean=(0+x), sd=1)\n  return(t.test(A,B,var.equal=TRUE)$p.value)\n}\n\n# vector of effect sizes\neffect_sizes <- seq(.1,2,.1)\n# run simulation for each effect size 1000 times\npower <- sapply(effect_sizes, \n          FUN = function(x) {\n            sims <- replicate(1000,sim_power(x))\n            sim_power <- length(sims[sims<.05])/length(sims)\n            return(sim_power)})\n# combine into dataframe\nplot_df <- data.frame(effect_sizes,power)\n\n# plot the power curve\nggplot(plot_df, aes(x=effect_sizes,\n                    y=power))+\n  geom_point()+\n  geom_line()"},{"path":"simulation-and-power-analysis.html","id":"simulating-cell-size","chapter":"Simulation and Power Analysis","heading":"10.14.2 Simulating cell-size","text":"Many experimental designs involve multiple measurements, trials, subject condition. many trials require subject condition? Traditional power analysis doesn’t make easy answer question. However, power design depend number subjects, also number trials used estimate mean subject condition.Consider simple Stroop experiment. researcher interested measuring Stroop effect least d=.1 (e.g., difference mean congruent trials .1 standard deviations smaller mean incongruent trials). many subjects required? , many trials subject perform congruent incongruent conditions? Let’s use simulation find .eye, looks like 30 subjects 100 trials condition give high power find Stroop effect d=.1.","code":"\n# function to run a simulated t-test\n# nsubs sets number of subjects\n# ntrials to change number of trials\n# d sets effect size\n# this is a paired sample test to model Stroop\nsim_power <- function(nsubs,ntrials,d){\n  A <- replicate(nsubs,mean(rnorm(n=ntrials,mean=0, sd=1)))\n  B <- replicate(nsubs,mean(rnorm(n=ntrials,mean=d, sd=1)))\n  return(t.test(A,B,paired=TRUE)$p.value)\n}\n\n# vectors for number of subjects and trials\nn_subs_vector <- c(10,20,30,50)\nn_trials_vector <- c(10,20,30,50,100)\n\n# a loop to run all simulations\npower <- c()\nsubjects <- c()\ntrials <- c()\ni <- 0 # use this as a counter for indexing\nfor(s in n_subs_vector){\n  for(t in n_trials_vector){\n    i <- i+1\n    sims <- replicate(1000,sim_power(s,t,.1))\n    power[i] <- length(sims[sims<.05])/length(sims)\n    subjects[i] <- s\n    trials[i] <- t\n  }\n}\n\n# combine into dataframe\nplot_df <- data.frame(power,subjects,trials)\n\n# plot the power curve\nggplot(plot_df, aes(x=subjects,\n                    y=power,\n                    group=trials,\n                    color=trials))+\n  geom_point()+\n  geom_line()\n\n# a vectorized version of the loop\n# run simulation for each effect size 1000 times\n\n# power <- outer(n_subs_vector,\n#                n_trials_vector,\n#                FUN = Vectorize(function(x,y) {\n#                   sims <- replicate(100,sim_power(x,y))  \n#                   sim_power <- length(sims[sims<.05])/length(sims)\n#                   return(sim_power)\n#                }))"},{"path":"simulation-and-power-analysis.html","id":"closing-thoughts","chapter":"Simulation and Power Analysis","heading":"10.15 Closing thoughts","text":"simulation approach powerful flexible. can applied whenever can formalize assumptions data. , simulations can highly customized account kinds nuances, like different numbers subjects, different distributions, assumptions noise, etc. wondering design can , maybe simulate .","code":""},{"path":"simulation-and-power-analysis.html","id":"more-examples","chapter":"Simulation and Power Analysis","heading":"10.16 More example(s)","text":"find time try add examples , especially box examples illustrate simulation can applied.","code":""},{"path":"simulation-and-power-analysis.html","id":"one-way-anova","chapter":"Simulation and Power Analysis","heading":"10.16.1 One-Way ANOVA","text":"Let’s extend simulation based approach one-way ANOVA. Let’s assume -subjects design, one factor four levels: , B, C, D. 20 subjects group. power curve design detect effects various size? Immediately, situation becomes complicated, numerous ways means , B, C, D vary. Let’s assume simplest case, three , one different amount standard deviations. compute main effect, report proportion significant experiments increase effect size fourth group.looks like design (1 factor, -subjects, 20 subjects per group) high power detect effect d=1, specifically one groups differs others d=1.However, effects psychology smalls, d=.2 common. , many subjects design require high power (let’s say .95, although people use .8) detect small effect?simulations suggests need 560 subjects group power .95 detect effect (d=.2). ’s total 2240 subjects. Reality can surprising comes power analysis. better surprised design run experiment, .","code":"\n# function to run a simulated t-test\nsim_power_anova <- function(x){\n  A <- rnorm(n=20,mean=0, sd=1)\n  B <- rnorm(n=20,mean=0, sd=1)\n  C <- rnorm(n=20,mean=0, sd=1)\n  D <- rnorm(n=20,mean=(0+x), sd=1)\n  df <- data.frame(condition = as.factor(rep(c(\"A\",\"B\",\"C\",\"D\"),each=20)),\n                   DV = c(A,B,C,D))\n  aov_results <- summary(aov(DV~condition,df))\n  #return the pvalue\n  return(aov_results[[1]]$`Pr(>F)`[1])\n}\n\n# vector of effect sizes\neffect_sizes <- seq(.1,2,.1)\n# run simulation for each effect size 1000 times\npower <- sapply(effect_sizes, \n          FUN = function(x) {\n            sims <- replicate(1000,sim_power_anova(x))\n            sim_power <- length(sims[sims<.05])/length(sims)\n            return(sim_power)})\n# combine into dataframe\nplot_df <- data.frame(effect_sizes,power)\n\n# plot the power curve\nggplot(plot_df, aes(x=effect_sizes,\n                    y=power))+\n  geom_point()+\n  geom_line()\nsim_power_anova <- function(x){\n  A <- rnorm(n=x,mean=0, sd=1)\n  B <- rnorm(n=x,mean=0, sd=1)\n  C <- rnorm(n=x,mean=0, sd=1)\n  D <- rnorm(n=x,mean=.2, sd=1)\n  df <- data.frame(condition = as.factor(rep(c(\"A\",\"B\",\"C\",\"D\"),each=x)),\n                   DV = c(A,B,C,D))\n  aov_results <- summary(aov(DV~condition,df))\n  #return the pvalue\n  return(aov_results[[1]]$`Pr(>F)`[1])\n}\n\n# vector of effect sizes\nsubjects <- seq(10,1000,50)\n# run simulation for each effect size 1000 times\npower <- sapply(subjects, \n          FUN = function(x) {\n            sims <- replicate(1000,sim_power_anova(x))\n            sim_power <- length(sims[sims<.05])/length(sims)\n            return(sim_power)})\n# combine into dataframe\nplot_df <- data.frame(subjects,power)\n\n# plot the power curve\nggplot(plot_df, aes(x=subjects,\n                    y=power))+\n  geom_point()+\n  geom_line()"},{"path":"simulation-and-power-analysis.html","id":"correlation-between-traits-and-behavior","chapter":"Simulation and Power Analysis","heading":"10.16.2 Correlation between traits and behavior","text":"common research strategy measure putative correlations people’s traits behavior. example, researcher might prepare questionnaire several questions. Perhaps questions political views, life satisfaction, anything else. research might ask group people answer questions, also perform task. end experiment, might ask different kinds people (measured questionnaire) perform differently behavioral measure. example, might people answer several questions openness new experiences, might also measure performance working memory task. research question might , people open experience perform better working memory task? answer lie correlation answers questions, performance task. Let’s simulate kind situation, say 20 subjects. subject answers 20 questions, perform behavioral task. end experiment, correlate answers question, performance task. kind correlations expect chance alone? Let’s say participants random robots. answer question randomly, performance behavioral task sampled randomly normal distribution. everything random, shouldn’t expect find correlations?Details:Let’s assume question involves likert scale 1 7. person randomly picks number 1 7 question. Let’s assume performance behavioral task sampled normal distribution mean = 0, sd = 1.histogram shows range correlations individual questions behavior can emerge just chance alone. run code times, see histogram changes bit random chance.Oftentimes researchers might know question questionnaire best question. , one best correlates behavior. Consider researcher computes correlations question behavior, chooses question highest correlation (positive negative) best question. , highest correlation. choosing question, might suggest behavior strongly correlated people answer question.Let’s try find simulation kinds large correlations can occur just chance alone. run many times, time save absolute value largest correlation question behavior. Just large can correlations chance alone?simulation shows chance alone situation can produce large correlations, large .8 .9 (although often).situation changes somewhat many subjects run. Let’s , run 200 subjects, rather 20.Now, chance doesn’t much better .25.","code":"\n# get 20 random answers for all 20 subjects and 20 questions\n# columns will be individual subjects 1 to 20\n# rows will be questions 1 to 20\nquestionnaire <- matrix(sample(1:7,20*20, replace=TRUE),ncol=20)\n# get 20 measures of performance on behavioral task\nbehavioral_task <- rnorm(20,0,1)\n\n# correlate behavior with each question\nsave_correlations <-c()\nfor(i in 1:20){\n  save_correlations[i] <- cor(behavioral_task,questionnaire[i,])\n}\n\n# show histogram of 20 correlations\nhist(save_correlations)\nsave_max <- c()\nfor( j in 1:10000){\n  questionnaire <- matrix(sample(1:7,20*20, replace=TRUE),ncol=20)\n  behavioral_task <- rnorm(20,0,1)\n  \n  save_correlations <-c()\n  for(i in 1:20){\n    save_correlations[i] <- cor(behavioral_task,questionnaire[i,])\n  }\n  save_max[j] <- max(abs(save_correlations))\n}\n\nhist(save_max)\nsave_max <- c()\nfor( j in 1:10000){\n  questionnaire <- matrix(sample(1:7,200*20, replace=TRUE),ncol=200)\n  behavioral_task <- rnorm(200,0,1)\n  \n  save_correlations <-c()\n  for(i in 1:20){\n    save_correlations[i] <- cor(behavioral_task,questionnaire[i,])\n  }\n  save_max[j] <- max(abs(save_correlations))\n}\n\nhist(save_max)"},{"path":"correlation.html","id":"correlation","chapter":"11 Correlation","heading":"11 Correlation","text":"“10/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"correlation.html","id":"reading-7","chapter":"11 Correlation","heading":"11.1 Reading","text":"Vokey & Allen43, Chapter 7; Abdi, Edelman, Dowling, & Valentin44, Chapter 2; Crump, Navarro, & Suzuki45, Chapter 3","code":""},{"path":"correlation.html","id":"overview-11","chapter":"11 Correlation","heading":"11.2 Overview","text":"lab contains practical section two concept sections correlations R.Practical : compute correlations R using cor() cor.test()Concepts : behavior measures covarianceConcepts II: statistical inference correlation using permutation tests","code":""},{"path":"correlation.html","id":"historical-background-1","chapter":"11 Correlation","heading":"11.3 Historical Background","text":"Sir Francis Galton often credited inventing measures correlation,46 developed interests eugenics quantifying heritability traits parents offspring.47 Galton’s protege, Karl Pearson, credited popularizing formalizing measures correlation; example, correlation coefficient, \\(r\\), often referred Pearson’s \\(r\\) (AKA Pearson’s correlation coefficient, Pearson’s product-moment correlation coefficient). Pearson published work brief note Royal Society.48 Despite recognition Galton Pearson receive, seems August Bravais described correlation coefficients .49 Pearson recounts history development correlation coefficient, discusses missed Bravais’ work.50 recently, Stigler discussed history Galton’s account development correlation coefficient.51 note: papers included class zotero group.Finally, Lee Rodgers, J., & Nicewander, W. .52 fantastic article thirteen ways conceptualizing correlation coefficient. also begin article brief discussion history development ideas around correlation.","code":""},{"path":"correlation.html","id":"additional-reading","chapter":"11 Correlation","heading":"11.4 Additional Reading","text":"additional reading introductory background concept correlation, see chapter correlation Answering questions data: https://crumplab.github.io/statistics/Correlation.htmlThe animated gif shows examples observing random correlations chance alone. See link example R code generate gifs like one.","code":"\nknitr::include_graphics(\"imgs/corNormFourNs-1.gif\")"},{"path":"correlation.html","id":"practical-i-cor-and-cor.test","chapter":"11 Correlation","heading":"11.5 Practical I: cor() and cor.test()","text":"","code":""},{"path":"correlation.html","id":"cor","chapter":"11 Correlation","heading":"11.5.1 cor()","text":"Base R comes cor() function computing Pearson’s correlation coefficients.cor() function can take vectors x y variables inputs return correlation.x y inputs can also matrices. case, correlation column vector B computedIf x y matrices, correlation column X Y computed.","code":"\n?cor\nA <- c(1,2,3,4,5,6,7,8,9,10) \nB <- c(1,3,2,4,3,5,4,5,6,7)\n\nplot(A,B)\ncor(A,B)  \n#> [1] 0.9246348\nA <- matrix(rnorm(100,0,1),ncol=10,nrow=10) \nB <- c(1,3,2,4,3,5,4,5,6,7)\ncor(A,B)\n#>                [,1]\n#>  [1,] -0.0007732836\n#>  [2,]  0.2105462061\n#>  [3,]  0.0498890881\n#>  [4,]  0.3139424573\n#>  [5,] -0.5678677690\n#>  [6,] -0.1178132763\n#>  [7,]  0.4675903979\n#>  [8,] -0.0862016646\n#>  [9,] -0.2831217608\n#> [10,] -0.4679394212\nA <- matrix(rnorm(25,0,1),ncol=5,nrow=5) \nB <- matrix(rnorm(25,0,1),ncol=5,nrow=5) \ncor(A,B)\n#>            [,1]       [,2]        [,3]       [,4]        [,5]\n#> [1,]  0.6750004  0.9164679  0.43895000 -0.6522803 -0.07008638\n#> [2,]  0.3156052  0.6585476 -0.01190223 -0.5233832 -0.03164248\n#> [3,] -0.7339743 -0.1991054 -0.83370085  0.2719532  0.33902507\n#> [4,] -0.1355138 -0.8903007  0.11685418  0.9238322  0.55263043\n#> [5,] -0.8205371 -0.4681444 -0.60486866  0.2312924 -0.21254416"},{"path":"correlation.html","id":"cor-and-n-1","chapter":"11 Correlation","heading":"11.5.2 cor and n-1","text":"’s worth noting cor() divides n-1, function computing correlation coefficient sample.","code":"\nA <- c(1,2,3,4,5)\nB <- c(5,2,3,1,4)\ncor(A,B)\n#> [1] -0.3\n\n# long-form using z-score method\n\nA_z <- (A-mean(A))/sd(A)\nB_z <- (B-mean(B))/sd(B)\nsum(A_z * B_z) / 4 # n-1, 5-1 = 4\n#> [1] -0.3"},{"path":"correlation.html","id":"additional-cor-functionality","chapter":"11 Correlation","heading":"11.5.3 Additional cor() functionality","text":"review help file cor() shows number uses. example, default method compute Pearson correlation coefficient, function also used compute kendall spearman’s coefficient (yet discussed class).Another advanced feature handling missing values. example, variable B contains NA, missing value fifth position. default, cor() return NA situation.However, use= option can set handle missing data different ways. example, complete.obs option removes fifth pair altogether, compues correlation remaining pairs complete cases.","code":"\nA <- c(1,2,3,4,5)\nB <- c(5,2,3,1,NA)\ncor(A,B)\n#> [1] NA\ncor(A,B,use=\"complete.obs\")\n#> [1] -0.8315218"},{"path":"correlation.html","id":"cor.test","chapter":"11 Correlation","heading":"11.5.4 cor.test()","text":"cor() function returns correlation coefficients, however cor.test() function can used return \\(r\\) value, well \\(p\\)-value.help file cor.test(), “method ”pearson”, test statistic based Pearson’s product moment correlation coefficient cor(x, y) follows t distribution length(x)-2 degrees freedom samples follow independent normal distributions. least 4 complete pairs observation, asymptotic confidence interval given based Fisher’s Z transform.”cor.test() also return list object can saved accessed later point.","code":"\n?cor.test\nA <- c(1,2,3,4,5)\nB <- c(5,2,3,1,4)\ncor.test(A,B)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  A and B\n#> t = -0.5447, df = 3, p-value = 0.6238\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.9348345  0.7918544\n#> sample estimates:\n#>  cor \n#> -0.3\nA <- c(1,2,3,4,5)\nB <- c(5,2,3,1,4)\nresults <- cor.test(A,B)\n\nresults$statistic\n#>          t \n#> -0.5447048\nresults$parameter\n#> df \n#>  3\nresults$p.value\n#> [1] 0.6238377\nresults$estimate\n#>  cor \n#> -0.3"},{"path":"correlation.html","id":"conceptual-i-the-simple-cross-product-as-a-measure-of-correlation","chapter":"11 Correlation","heading":"11.6 Conceptual I: The simple cross product as a measure of correlation","text":"","code":""},{"path":"correlation.html","id":"correlation-basics","chapter":"11 Correlation","heading":"11.6.1 Correlation basics","text":"correlation coefficient convenient measure association two variables. One convenient aspect resulting value limited range -1 1, can aid interpreting value. +1 means perfect positive correlation, 0 means correlation, -1 means perfect negative correlation.section use R look basic math behind correlation coefficient, way focus general concept.example, general concept positive correlation pair measures tends go together. value X small, paired value Y usually small. value X large, paired value Y usually large. words, variation X matches well variation Y;, X Y co-vary together way.perfect positive example :negative correlation pair measures tends go opposite directions . value X small, paired value Y usually large. value X large, paired value Y usually small. , X Y co-vary, except opposite ways. perfect negative example :idea zero correlation isn’t association paired values. value X goes , paired value Y whatever wants.Everytime code chunk runs, randomly shuffle Y, resulting correlation average 0 (everytime due chance).","code":"\nX <- 1:10\nY <- 1:10\n\nplot(X,Y)\ncor(X,Y)\n#> [1] 1\nX <- 1:10\nY <- 10:1\n\nplot(X,Y)\ncor(X,Y)\n#> [1] -1\nX <- 1:10\nY <- sample(10:1)\n\nplot(X,Y)\ncor(X,Y)\n#> [1] -0.369697"},{"path":"correlation.html","id":"crossproducts-and-correlation","chapter":"11 Correlation","heading":"11.6.2 Crossproducts and correlation","text":"Pearson’s \\(r\\) also sometimes called product moment correlation coefficient. refers idea \\(r\\) sum cross products standardized.section look basic cross-product operation. example, cross product involves multiplying values two variables X Y together.sum cross products involves adding values:sum crossproducts also measure correlation association variables X Y. However, range values can take depend values X Y (standardized).Consider questions, assume X contains values 1 10, Y.largest value sum crossproducts X Y can take?Notice arrangement, smallest value X Y (1) paired together, next largest value (2) paired, 10. pairing creates largest sum crossproducts. also represents perfect positive correlation.smallest value sum crossproducts X Y can take?numbers arranged produce perfect negative correlation, sum cross products ’s minimum possible value.X contains number 1 10 order, Y , kinds sums cross-products can occur?","code":"\nX <- 1:10\nY <- 1:10\n\nX*Y\n#>  [1]   1   4   9  16  25  36  49  64  81 100\nsum(X*Y)\n#> [1] 385\nX <- 1:10\nY <- 1:10\nsum(X*Y)\n#> [1] 385\nX <- 1:10\nY <- 10:1\nsum(X*Y)\n#> [1] 220\nsum(sample(1:10)*sample(1:10))\n#> [1] 305\nsim_sums <- replicate(1000,sum(sample(1:10)*sample(1:10)))\nhist(sim_sums)\nmin(sim_sums)\n#> [1] 230\nmax(sim_sums)\n#> [1] 376"},{"path":"correlation.html","id":"conceptual-ii-statistical-inference-for-correlation","chapter":"11 Correlation","heading":"11.7 Conceptual II: Statistical inference for correlation","text":"look concept null-distribution correlation co-efficients two different ways, first randomly samply values normal distributions, second permutation test.","code":""},{"path":"correlation.html","id":"random-correlations","chapter":"11 Correlation","heading":"11.7.1 “Random” correlations","text":"totally possible apply Pearson’s \\(r\\) formula two variables conceptually 100% uncorrelated independent , still find “correlations”, largish values \\(r\\).example, randomly sample 10 values normal distribution X, another 10 values normal distribution Y, expect average 0 correlation X Y. , selected values completely random.happens 10 times?1000 times?sense simulation creates null-distribution sorts, sampling distribution \\(r\\) values expected number paired scores 10, drawn randomly independently unit normal distributions. ’s clear case chance alone possible get wide range correlation coefficients.","code":"\nX <- rnorm(10,0,1)\nY <- rnorm(10,0,1)\ncor(X,Y)\n#> [1] 0.1720688\nreplicate(10,cor(rnorm(10,0,1),rnorm(10,0,1)))\n#>  [1] -0.16575280  0.11104389 -0.57530133 -0.19034062 -0.08043619  0.19232185\n#>  [7] -0.11890749  0.67469078 -0.38367144 -0.37141058\nrand_1000 <- replicate(1000,cor(rnorm(10,0,1),rnorm(10,0,1)))\nhist(rand_1000)\nmean(rand_1000)\n#> [1] -0.00662184\nmax(rand_1000)\n#> [1] 0.7796334\nmin(rand_1000)\n#> [1] -0.8187007"},{"path":"correlation.html","id":"sample-size-matters","chapter":"11 Correlation","heading":"11.7.2 Sample-size matters","text":"Briefly, kinds correlations can produced chance limited sample-size. example, consider happens range simulated \\(r\\) values number paired scores increased 10 100.","code":"\nrand_1000 <- replicate(1000,cor(rnorm(100,0,1),rnorm(100,0,1)))\nhist(rand_1000)\nmean(rand_1000)\n#> [1] 0.000502127\nmax(rand_1000)\n#> [1] 0.3591474\nmin(rand_1000)\n#> [1] -0.3453971"},{"path":"correlation.html","id":"permutation-test","chapter":"11 Correlation","heading":"11.7.3 Permutation test","text":"class discussed sample data suggesting length word negatively correlated number meanings word. Example data showing negative correlation shown (taken R workbook Abdi textbook).According cor.test() function, correlation sample data unlikely produced chance alone.Instead using cor.test() function, can use concept permutation test construct null distribution. basic idea imagine values Length Meanings variables randomly repaired, new correlation coefficient measured. procedure several thousand times create null distribution representing kinds \\(r\\) values obtained chance.","code":"\nlibrary(ggplot2)\nlibrary(ggrepel)\nWords  = c('bag','buckle','on','insane','by','monastery',\n'relief','slope','scoundrel','loss','holiday','pretentious',\n'solid','time','gut','tarantula','generality','arise','blot','infectious')\nLength=c(3,6,2,6,2,9,6,5,9,4,7,11,5,4,3,9,10,5,4,10)\nMeanings=c(8,4,10,1,11,1,4,3,1,6,2,1,9,3,4,1,3,3,3,2)\n\nall <- data.frame(Words,Length,Meanings)\nknitr::kable(all)\n\nggplot(all,aes(x=Length,y=Meanings))+\n  geom_point()+\n  geom_text_repel(aes(label=Words))\ncor.test(Length,Meanings)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  Length and Meanings\n#> t = -4.5644, df = 18, p-value = 0.0002403\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.8873588 -0.4289759\n#> sample estimates:\n#>        cor \n#> -0.7324543\ncor(sample(Length),sample(Meanings))\n#> [1] -0.1250532\nsim_rs <- replicate(1000,cor(sample(Length),sample(Meanings)))\nhist(sim_rs)\n\nlength(sim_rs[sim_rs <= cor(Length,Meanings)])/1000\n#> [1] 0"},{"path":"correlation.html","id":"lab-11-generalization-assignment","chapter":"11 Correlation","heading":"11.8 Lab 11 Generalization Assignment","text":"","code":""},{"path":"correlation.html","id":"instructions-10","chapter":"11 Correlation","heading":"11.8.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab11.Rmd”Use Lab11.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 11 blackboard.","code":""},{"path":"correlation.html","id":"problems-10","chapter":"11 Correlation","heading":"11.8.2 Problems","text":"X Y variable contain numbers.\n. Compute Pearson’s \\(r\\) report associated p-value using cor.test() function. (2 points)\nB. Use permutation test create null-distribution, report p-value getting observed correlation larger using simulated null-distribution. (2 points)X Y variable contain numbers.. Compute Pearson’s \\(r\\) report associated p-value using cor.test() function. (2 points)B. Use permutation test create null-distribution, report p-value getting observed correlation larger using simulated null-distribution. (2 points)Using variables X Y , assuming values re-ordered way, report following:\n. smallest possible sum cross-products (1 point)\nB. largest possible sum cross-products (1 point)Using variables X Y , assuming values re-ordered way, report following:. smallest possible sum cross-products (1 point)B. largest possible sum cross-products (1 point)","code":"\nX <- c(1,4,3,2,5,4,3,6,7,8)\nY <- c(1,3,6,7,8,2,8,7,6,9)"},{"path":"regression.html","id":"regression","chapter":"12 Regression","heading":"12 Regression","text":"“10/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"regression.html","id":"readings-2","chapter":"12 Regression","heading":"12.1 Readings","text":"Vokey & Allen53, Chapter 8; Abdi, Edelman, Dowling, & Valentin54, Chapters 3 4; Crump, Navarro, & Suzuki55, 3.5.","code":""},{"path":"regression.html","id":"overview-12","chapter":"12 Regression","heading":"12.2 Overview","text":"Practical: Performing linear regressions RConcept : best fit line least squaresConcept II: \\(R^2\\) \\(F\\)Advanced Concept: Best fit line brute force search","code":""},{"path":"regression.html","id":"practical-i-simple-linear-regression-in-r","chapter":"12 Regression","heading":"12.3 Practical I: Simple linear Regression in R","text":"questions ’ll answer section:conduct linear regression R?find best fit line R?graph best fit line ggplot2?","code":""},{"path":"regression.html","id":"lm","chapter":"12 Regression","heading":"12.3.1 lm()","text":"Base R comes lm() function, short linear models, can perform linear regression among analyses. come back lm() later labs.basic examples computing linear regression lm(). step pieces code chunk.","code":"\n?lm\n# Dataframe with dependent and independent variables\nsome_data <- data.frame(Y_dv= c(1,2,4,3,5,4,6,5),\n                        X_iv= c(3,5,4,2,6,7,8,9))\n\n#compute linear regression,\nlm(Y_dv ~ X_iv, data=some_data)\n#> \n#> Call:\n#> lm(formula = Y_dv ~ X_iv, data = some_data)\n#> \n#> Coefficients:\n#> (Intercept)         X_iv  \n#>         1.0          0.5\n\n# saving and output\nsave_model <- lm(Y_dv ~ X_iv, data=some_data)\nsummary(save_model)$r.squared\n#> [1] 0.5384615\n\n# r and r^2\ncor.test(some_data$Y_dv,some_data$X_iv)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  some_data$Y_dv and some_data$X_iv\n#> t = 2.6458, df = 6, p-value = 0.03825\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.06031428 0.94817953\n#> sample estimates:\n#>       cor \n#> 0.7337994\nsummary(save_model)$r.squared\n#> [1] 0.5384615\nsqrt(summary(save_model)$r.squared)\n#> [1] 0.7337994"},{"path":"regression.html","id":"use-long-data-format","chapter":"12 Regression","heading":"12.3.1.1 Use long-data format","text":"lm() requires data long-format (one observation per row), data.frame similar (e.g., data.table, tibble also work). convention, use Y refer dependent variable, X refer independent variable.","code":"\n# Dataframe with dependent and independent variables\nsome_data <- data.frame(Y_dv= c(1,2,4,3,5,4,6,5),\n                        X_iv= c(3,5,4,2,6,7,8,9))"},{"path":"regression.html","id":"specify-the-formula","chapter":"12 Regression","heading":"12.3.1.2 Specify the formula","text":"linear regression performed . two basic inputs lm(formula = , data=). formula supplied Y_dv ~ X_iv. dependent variable always placed left side (Y_dv), independent variable placed right side (X_dv). Order matters. case, explaining/predicting variation Y_dv function variation X_dv.class learned Pearson’s correlation coefficient, \\(r\\), related coefficient determination, \\(R^2\\), measures proportion variance explained. lm() returns \\(R^2\\), \\(r\\). quick demonstration square-rooting \\(R^2\\) lm(), gives value \\(r\\) cor.test().output prints y-intercept slope needed draw best-fit line regression.","code":"\n#compute linear regression,\nlm(Y_dv ~ X_iv, data=some_data)\n#> \n#> Call:\n#> lm(formula = Y_dv ~ X_iv, data = some_data)\n#> \n#> Coefficients:\n#> (Intercept)         X_iv  \n#>         1.0          0.5\n# r and r^2\ncor.test(some_data$Y_dv,some_data$X_iv)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  some_data$Y_dv and some_data$X_iv\n#> t = 2.6458, df = 6, p-value = 0.03825\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.06031428 0.94817953\n#> sample estimates:\n#>       cor \n#> 0.7337994\nsummary(save_model)$r.squared\n#> [1] 0.5384615\nsqrt(summary(save_model)$r.squared)\n#> [1] 0.7337994"},{"path":"regression.html","id":"model-summary","chapter":"12 Regression","heading":"12.3.1.3 Model summary","text":"addition, output assigned object, can inspected manipulated . example, additional information can printed summary function.","code":"\nsave_model <- lm(Y_dv ~ X_iv, data=some_data)\nsummary(save_model)\n#> \n#> Call:\n#> lm(formula = Y_dv ~ X_iv, data = some_data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -1.50  -0.75   0.25   1.00   1.00 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)    1.000      1.126   0.888   0.4087  \n#> X_iv           0.500      0.189   2.646   0.0382 *\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.225 on 6 degrees of freedom\n#> Multiple R-squared:  0.5385, Adjusted R-squared:  0.4615 \n#> F-statistic:     7 on 1 and 6 DF,  p-value: 0.03825\nsummary(save_model)$r.squared\n#> [1] 0.5384615"},{"path":"regression.html","id":"ggplot2-1","chapter":"12 Regression","heading":"12.3.2 ggplot2","text":"Best fit lines can easily displayed scatterplots ggplot2, basic example. Note supplied dataframe X Y variables. geom_point() adds dots, geom_smooth() computes regression adds line using lm() method described . possible add confidence intervals setting se=TRUE, ’s clear intervals computed, might decide use default.","code":""},{"path":"regression.html","id":"geom_smooth","chapter":"12 Regression","heading":"12.3.2.1 geom_smooth","text":"many sets X Y data specified data frame can create multiple scatterplots, best fit line, using facet_wrap().","code":"\nlibrary(ggplot2)\n\nggplot(some_data, aes(x=X_iv, y=Y_dv))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)\nsome_data2 <- data.frame(Y = rnorm(4*10,0,1),\n                         X = rnorm(4*10,0,1),\n                        set = rep(1:4,each=10)) \n\nggplot(some_data2, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  facet_wrap(~set)"},{"path":"regression.html","id":"add-equations-and-model-info","chapter":"12 Regression","heading":"12.3.2.2 Add equations and model info","text":"ggplot2 automatically show regression equation \\(R^2\\) values, can added annotations. , find extension package might thing looking . example, numerous extensions ggplot2, https://exts.ggplot2.tidyverse.org/gallery/. One ggpmisc, can help adding details plot.ggpmisc::stat_poly_eq() can add \\(R^2\\) value plot., print regression line equation, . ’ll refer examples info using ggpmisc.","code":"\nlibrary(ggpmisc)\nformula <- y ~ x\nggplot(some_data, aes(x= X_iv, y = Y_dv)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = formula) +\n  stat_poly_eq(formula = formula, parse = TRUE)\nggplot(some_data, aes(x= X_iv, y = Y_dv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = formula) +\n  stat_poly_eq(aes(label = stat(eq.label)), formula = formula,\n               parse = TRUE)"},{"path":"regression.html","id":"concept-i-the-best-fit-line-and-least-squares","chapter":"12 Regression","heading":"12.4 Concept I: The best fit line and least squares","text":"code chunk generates linear regression animation using ggplot2 gganimate.animation shows two key concepts reinforce. First, regression line “best fit line”. , “best fit line” one minimizes sum squared deviations point line.blue line represents best fit line found linear regression. roving black line represents possible lines. simplicity, black line slope, intercept changes moves . black line moves, little red lines show error black line data point. shaded rectangle dot represents squared error. shaded error squares…black line moves away true regression line, error squares get bigger bigger, redder redder. indicates fit line getting worse (error getting bigger), line moves away best line. black line moves toward best fit line, can see error squares getting smaller, shaded less red, visually indicate less overall error. black line converges blue line, found best fit line, one minimizes sum squared deviations.","code":"\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(dplyr)\n\nd <- mtcars\nfit <- lm(mpg ~ hp, data = d)\nd$predicted <- predict(fit)   # Save the predicted values\nd$residuals <- residuals(fit) # Save the residual values\n\ncoefs<-coef(lm(mpg ~ hp, data = mtcars))\ncoefs[1]\ncoefs[2]\n\nx<-d$hp\nmove_line<-c(seq(-6,6,.5),seq(6,-6,-.5))\ntotal_error<-length(length(move_line))\ncnt<-0\nfor(i in move_line){\n  cnt<-cnt+1\n  predicted_y <- coefs[2]*x + coefs[1]+i\n  error_y <- (predicted_y-d$mpg)^2\n  total_error[cnt]<-sqrt(sum(error_y)/32)\n}\n\nmove_line_sims<-rep(move_line,each=32)\ntotal_error_sims<-rep(total_error,each=32)\nsims<-rep(1:50,each=32)\n\nd<-d %>% slice(rep(row_number(), 50))\n\nd<-cbind(d,sims,move_line_sims,total_error_sims)\n\nanim<-ggplot(d, aes(x = hp, y = mpg)) +\n  geom_abline(intercept = 30.09886+move_line_sims, slope = -0.06822828, aes(linetype='d'), color= 'red')+\n  lims(x = c(0,400), y = c(-10,40))+\n  geom_segment(aes(xend = hp, yend = predicted+move_line_sims, color=\"red\"), alpha = .5) + \n  geom_point() +\n  geom_rect(aes(ymin=predicted+move_line_sims, ymax=mpg, xmin=hp, xmax=hp+abs(predicted)+abs(residuals)+abs(move_line_sims), fill = total_error_sims), alpha = .2)+\n  scale_fill_gradient(low=\"lightgrey\", high=\"red\")+\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") + \n  theme_classic()+\n  theme(legend.position=\"none\")+\n  xlab(\"X\")+ylab(\"Y\")+\n  transition_manual(frames=sims)+\n  enter_fade() + \n  exit_fade()+\n  ease_aes('sine-in-out')\n\nanimate(anim,fps=5)\nknitr::include_graphics(\"imgs/regression_squares.gif\")"},{"path":"regression.html","id":"illustrating-regression-concepts-with-r","chapter":"12 Regression","heading":"12.4.1 Illustrating regression concepts with R","text":"animation example modified Crump, Navarro, & Suzuki56 (added little rectangles indicate squared error).concept section use R create graph like animation. goal see can use R visually illustrate concepts linear regression. example, can already plot data points scatterplot, can plot regression line. nice visualize error data point line, perhaps also see error smallest use best fit line, line.","code":""},{"path":"regression.html","id":"create-a-scatterplot","chapter":"12 Regression","heading":"12.4.2 Create a scatterplot","text":"sample data using :","code":"\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9))\n\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()"},{"path":"regression.html","id":"compute-regression-equation","chapter":"12 Regression","heading":"12.4.3 Compute regression equation","text":"can compute regression line using lm() get y-intercept slope.regression line represents best predictions values Y function X.","code":"\nlm(Y~X, data=some_data)\n#> \n#> Call:\n#> lm(formula = Y ~ X, data = some_data)\n#> \n#> Coefficients:\n#> (Intercept)            X  \n#>         1.0          0.5"},{"path":"regression.html","id":"generate-predicted-values","chapter":"12 Regression","heading":"12.4.4 Generate predicted values","text":"can use predict.lm() function compute predictions us. Thus, X values 1 8, see following predictions Y.Let’s add predictions graph. adding new column data.frame. Using mutate() can run prediction function existing columns X Y.","code":"\npredict.lm(lm(Y~X, data=some_data))\n#>   1   2   3   4   5   6   7   8 \n#> 2.5 3.5 3.0 2.0 4.0 4.5 5.0 5.5\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X)))\n\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_point(aes(y=Y_pred), color='red')"},{"path":"regression.html","id":"add-regression-line","chapter":"12 Regression","heading":"12.4.5 Add regression line","text":"Lets’s also add regression line :","code":"\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_point(aes(y=Y_pred), color='red')"},{"path":"regression.html","id":"visualize-residual-error","chapter":"12 Regression","heading":"12.4.6 Visualize residual error","text":"next step visually depict error black dot (data point), blue line. involve drawing vertical lines black dot, red dot representing prediction blue line. can use geom_segment() draw lines.blue line happens best fit line, one minimizes sum squared deviations. drawn deviations little lines black dot line.","code":"\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X))) %>%\n  mutate(Y_error = Y - Y_pred)\n\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_point(aes(y=Y_pred), color='red') +\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)"},{"path":"regression.html","id":"visualize-error-as-squares","chapter":"12 Regression","heading":"12.4.7 Visualize error as squares","text":"Next, let’s illustrate deviations squares, better depict actual quantity summed . use geom_rect() draw rectangles.look like squares, don’t X Y axes aren’t exactly scale, figure height width aren’t exactly square :","code":"\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X))) %>%\n  mutate(Y_error = Y - Y_pred)\n\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_point(aes(y=Y_pred), color='red') +\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+\n  geom_rect(aes(ymin=Y, \n                ymax=Y_pred, \n                xmin=X,\n                xmax=X+Y_error), \n            alpha = .2)\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_point(aes(y=Y_pred), color='red') +\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+\n  geom_rect(aes(ymin=Y, \n                ymax=Y_pred, \n                xmin=X,\n                xmax=X+Y_error), \n            alpha = .2)+\n  coord_cartesian(xlim=c(0,10),\n                  ylim=c(0,10))"},{"path":"regression.html","id":"visualize-error-for-worse-fit-lines","chapter":"12 Regression","heading":"12.4.8 Visualize error for “worse” fit lines","text":"Finally, let’s try one two graphs illustrate drew different lines, best fit lines. drew “worse” line, see altogether, squared error boxes get bigger bigger. example, new red line go black dots (line shifted ), squared errors much larger ones blue best fit line.Let’s see can make faceted graph, showing red line moving away blue upward direction four steps. provide good visual evidence error squares growing size.","code":"\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X))+2) %>%\n  mutate(Y_error = Y - Y_pred)\n\nggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_point(aes(y=Y_pred), color='red') +\n  geom_line(aes(x=X,y=Y_pred), color='red')+\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+\n  geom_rect(aes(ymin=Y, \n                ymax=Y_pred, \n                xmin=X,\n                xmax=X+Y_error), \n            alpha = .2)+\n  coord_cartesian(xlim=c(0,10),\n                  ylim=c(0,10))\n\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X)))\n\nsome_data <- rbind(some_data,\n                   some_data,\n                   some_data,\n                   some_data) %>%\n  mutate(step = rep(1:4,each = 8),\n         Y_pred = Y_pred + rep(c(.5,1,1.5,2), each = 8)) %>%\n  mutate(Y_error = Y - Y_pred)\n\nggplot(some_data, aes(x=X, y=Y))+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_point(aes(y=Y_pred), color='red') +\n  geom_line(aes(x=X,y=Y_pred), color='red')+\n  geom_point()+\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+\n  geom_rect(aes(ymin=Y, \n                ymax=Y_pred, \n                xmin=X,\n                xmax=X+Y_error), \n            alpha = .2)+\n  coord_cartesian(xlim=c(0,10),\n                  ylim=c(0,10)) +\n  facet_wrap(~step)\n\nslope = 2\nb = 1\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = slope*X+b) %>%\n  mutate(error = Y-Y_pred)\n\nsum(some_data$error^2)\n#> [1] 648"},{"path":"regression.html","id":"concept-ii-r2-f-and-variance-explained","chapter":"12 Regression","heading":"12.5 Concept II: \\(R^2\\), F, and variance explained","text":"Wright, S.57 credited wikipedia coining term coefficient determination \\(R^2\\). interesting read paper think term coefficient determination. Wright discussed plain old correlations obviously caused multiple causal variables, one reason correlation equal causation. sought methods break correlation component causes paths, seems inspire rather loaded inspirational term coefficient determination. say term inspirational squaring \\(r\\)-value using deterministic language doesn’t magically allow one identify causal forces correlation.coin term less hubris, call \\(R^2\\) co-efficient composition. , see, \\(R^2\\) just squaring correlation coefficient…another (equivalent) definition based concept variance can partitioned different parts. Throughout course general idea turn something called General Linear Model. now, idea variation measurement can broken separate components, original data can re-expressed “re-composed” terms sum parts.linear regression variation Y variable, plot data X variable. draw best fit line. next sections, see data points Y decomposed different parts. total variation Y expressed squared differences Y value mean Y. total variation partitioned two parts. One part “explained” regression line, part leftover bits, called residuals.","code":""},{"path":"regression.html","id":"explaining-variance","chapter":"12 Regression","heading":"12.5.1 Explaining variance","text":"Imagine trying explain complicated thing, want keep track much thing explained. use formula like :\\(\\text{Total stuff explain} = \\text{Explained stuff} + \\text{Unexplained stuff}\\)conceptual formula partitions total stuff needing explanation two parts, explained unexplained. assign numbers formula, keep track quantities explanation. \\(R^2\\) \\(F\\) values related statistics give ratio quantity relating amount explained stuff unexplained stuff.","code":""},{"path":"regression.html","id":"r2-proportion-of-variance-explained","chapter":"12 Regression","heading":"12.5.2 \\(R^2\\) proportion of variance explained","text":"Imagine wanted know proportion explained stuff: ratio explained stuff total amount explained. proportion 0 haven’t explained anything, go 1 explain everything explained.\\(1 = \\frac{\\text{Explained} + \\text{Unexplained}}{\\text{Total stuff explain}}\\)\\(\\text{Proportion explained} = \\frac{\\text{Explained}}{\\text{Total}} =(1- \\frac{\\text{Unexplained}}{\\text{Total}})\\)\\(R^2\\) calculated. quantities replaced sum squares (\\(SS\\)) representing total amount variance explained. \\(SS_{Total}\\) split parts “can explained” (e.g., regression line), parts remain “unexplained” (residual error).\\(R^2 = 1 - \\frac{SS_{unexplained}}{SS_{Total}} = \\frac{SS_{explained}}{SS_{Total}}\\)Note, \\(R^2\\) must go 0 1, set ratio part whole (explained part), whole (things needing explaining). explained part can go anywhere 0 (explaining nothing) 100% (explaining everything), proportion must go 0 1.","code":""},{"path":"regression.html","id":"f-statistic-explained-over-unexplained","chapter":"12 Regression","heading":"12.5.3 F-statistic, explained over unexplained","text":"F statistic another way use general formula create ratio gets larger able explain can’t.\\(F = \\frac{Explained}{Unexplained}\\)Consider F ratio works. bounded 0 1. , number 1 still important, tipping point explanation.\\(explained = unexplained\\) , \\(F =1\\).\\(explained = unexplained\\) , \\(F =1\\).\\(explained > unexplained\\) , \\(F > 1\\);\\(explained > unexplained\\) , \\(F > 1\\);\\(explained < unexplained\\) , \\(F < 1\\).\\(explained < unexplained\\) , \\(F < 1\\).F-statistics use measures variance, sums squares divided relevant degrees freedom.\\(F = \\frac{SS_{Explained}/df1}{SS_{Unexplained}/df2} = \\frac{MS_{Explained}}{MS_{Unexplained}}\\)","code":""},{"path":"regression.html","id":"transforming-r2-to-f","chapter":"12 Regression","heading":"12.5.4 Transforming \\(R^2\\) to \\(F\\)","text":"Abdi, Edelman, Dowling, & Valentin58 provide formula (chapter 3) shows Fs can converted \\(R^2\\).\\(F = \\frac{R^2}{1-R^2} \\times (S-2)\\)numerator \\(R^2\\), previously defined proportion variance explained; denominator proporition variance unexplained. , formula presents F ratio explained unexplained variance.","code":""},{"path":"regression.html","id":"visualizing-the-sums-of-squares","chapter":"12 Regression","heading":"12.5.5 Visualizing the sums of squares","text":"Let’s try fill blanks can see \\(R^2\\) way:\\(R^2 = (1- \\frac{\\text{Unexplained}}{\\text{Total}}) = \\frac{\\text{Explained}}{\\text{Total}}\\)begin Y variable X variable. going use variability X “explanation” variability Y.data relate formula task explanation? trying explain? case, Y variable taking place dependent measure. Y variable numbers different, variation. general language, trying “explain” variation Y.","code":"\nY <- c(1,2,4,3,5,4,6,5)\nX <- c(3,5,4,2,6,7,8,9)"},{"path":"regression.html","id":"ss-total","chapter":"12 Regression","heading":"12.5.6 SS total","text":"order put number denominator general formula, need quantify total amount variation Y. two ways done previously, using sums squares, variance (divides sums squares). \\(SS_{total}\\) uses mean Y starting point, sums squared deviations mean. :\\(SS_{total} = \\sum{(Y_i - \\bar{Y})^2}\\)scatterplot visualizes \\(SS_total\\); , instead regression line saw earlier, red line represents mean Y. , deviations dot red line represent components added produce \\(SS_total\\) Y. Specifically, sum area error squares taken total amount stuff needs explained. Slightly formally, squared deviations represent departures mean Y. data Y number, departures mean, mean “explain everything”, leaving nothing left variable explain.moving , let’s tie word “explanation” idea prediction. trying make guesses numbers Y use mean Y best guess prediction. According least squares criterion, mean center data, value produces smallest sum squared deviations. guessed mean Y every time sampled number Y, ’d amount time, squaring summing little errors give total say represents variation accounted mean.linear regression ask whether can improve prediction beyond using mean Y. example, another variable like X, make use variable improve predictions Y? words, way can line Y values X values way reduces sum squared errors. achieved, knowing something mean Y knowing something X reduce prediction error.","code":"\nSS_total = sum((Y-mean(Y))^2)\nSS_total\n#> [1] 19.5\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = mean(Y)) %>%\n  mutate(Y_error = Y - Y_pred)\n\n(total_plot <- ggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_line(aes(y=Y_pred), color='black')+\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+\n  geom_rect(aes(ymin=Y, \n                ymax=Y_pred, \n                xmin=X,\n                xmax=X+Y_error), \n            alpha = .2)+\n  coord_cartesian(xlim=c(0,10),\n                  ylim=c(0,10))+\n  theme_classic()+\n  ggtitle(\"SS Total\")\n  )"},{"path":"regression.html","id":"ss-unexplained-residuals","chapter":"12 Regression","heading":"12.5.7 SS unexplained (residuals)","text":"Consider now regression line, squared deviations produced first concept section. colored error squares red flag fact blue regression line explain deviations blue line.squared errors represent deviations data points best fit regression line derived X. sum squares also represents quantity errors explained (often termed residuals). words, looking “unexplained” part general formula.\\(R^2 = (1- \\frac{\\text{Unexplained}}{\\text{Total}}) = \\frac{\\text{Explained}}{\\text{Total}}\\)specifically, looking another sum squares, call \\(SS_\\text{residuals}\\).\\(SS_\\text{residuals} = \\sum{(Y_i - Y'_i)^2}\\), \\(Y'_i\\) refers predicted Y values get regression equation.can quick check show can compute \\(R^2\\) \\(R^2 = (1- \\frac{\\text{Unexplained}}{\\text{Total}}) = \\frac{\\text{Explained}}{\\text{Total}}\\)\\(R^2 = (1- \\frac{\\text{SS_residual}}{\\text{SS_total}})\\)","code":"\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X))) %>%\n  mutate(Y_error = Y - Y_pred)\n\n(res_plot <- ggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+\n  geom_rect(aes(ymin=Y, \n                ymax=Y_pred, \n                xmin=X,\n                xmax=X+Y_error), \n            alpha = .5,\n            fill = \"red\")+\n  coord_cartesian(xlim=c(0,10),\n                  ylim=c(0,10))+\n  theme_classic()+\n  ggtitle(\"SS Unexplained (residual)\")\n  )\nY_predicted = predict.lm(lm(Y~X))\nY_predicted\n#>   1   2   3   4   5   6   7   8 \n#> 2.5 3.5 3.0 2.0 4.0 4.5 5.0 5.5\n\nSS_residuals = sum((Y-Y_predicted)^2)\nSS_residuals\n#> [1] 9\n\n1- ((SS_residuals/(8-2)) / (SS_total/(7)))\n#> [1] 0.4615385\n\n\ncor(Y,X)^2\n#> [1] 0.5384615\n#quick check\n1-(9/19.5)\n#> [1] 0.5384615\ncor(Y,X)^2\n#> [1] 0.5384615"},{"path":"regression.html","id":"ss-explained-regression","chapter":"12 Regression","heading":"12.5.8 SS explained (regression)","text":"Let’s finally look idea explained variance. Previously found \\(SS_{total}\\) \\(SS_{residual}\\), haven’t directly computed \\(SS_{explained}\\):\\(R^2 = (1- \\frac{\\text{Unexplained}}{\\text{Total}}) = \\frac{\\text{Explained}}{\\text{Total}}\\)\\(R^2 = (1- \\frac{\\text{SS}_{residuals}}{\\text{SS}_{Total}}) = \\frac{\\text{SS}_{Explained}}{\\text{SS}_{Total}}\\)use algebra find \\(SS_{Explained}\\):\\(SS_{Total} = \\text{SS}_{Explained} + \\text{SS}_{residuals}\\)\\(\\text{SS}_{Explained} = SS_{Total} -\\text{SS}_{residuals}\\)\\(\\text{SS}_{Explained} = 19.5 - 9 = 10.5\\)think interesting show sum squares \\(\\text{SS}_{Explained}\\) squares scatterplots making. “explained” variance associated squared areas black line mean Y, blue regression line. colored squared areas blue.sum squares refers variance Y “explained” combination using mean Y predictor, regression line X predictor.Previously, said \\(R^2\\) proportion variance explained. :\\(R^2 = \\frac{SS_{Explained}}{SS_{Total}}\\)sum blue squares equivalent \\(SS_{explained}\\). also write:\\(SS_{Explained} = \\sum (Y'_i - \\bar{Y})^2\\)means sum squared differences predicted Y values based regression line mean Y (e..g, sum area blue squares).","code":"\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = predict.lm(lm(Y~X)),\n         Y_mean = mean(Y)) %>%\n  mutate(Y_error = Y - Y_pred,\n         Y_total = Y-Y_mean)\n\n(exp_plot <- ggplot(some_data, aes(x=X, y=Y))+\n  geom_point()+\n  geom_line(aes(y=Y_mean), color='black')+\n  geom_smooth(method='lm', se=FALSE)+\n  geom_segment(aes(xend = X, y = Y_mean, yend = Y_pred), color='black')+\n  geom_rect(aes(ymin=Y_mean, \n                ymax=Y_pred, \n               xmin=X,\n                xmax=X+(Y_pred - Y_mean)), \n            alpha = .5,\n            fill = \"blue\")+\n  coord_cartesian(xlim=c(0,10),\n                  ylim=c(0,10))+\n  theme_classic()+\n  ggtitle(\"SS Explained (by Regression of X)\"))\nSS_explained = sum((predict.lm(lm(Y~X)) - mean(Y))^2)\nSS_explained\n#> [1] 10.5\n\n#from before\nSS_total\n#> [1] 19.5\n\n#calculate R^2 as a proportion of total SS\nSS_explained/SS_total\n#> [1] 0.5384615\n\n(SS_explained/8) / (SS_total/8)\n#> [1] 0.5384615\n\n# compare to r^2\ncor(Y,X)^2\n#> [1] 0.5384615"},{"path":"regression.html","id":"all-together","chapter":"12 Regression","heading":"12.5.9 All together","text":"just created three different scatterplots illustrating \\(SS_{total} = SS_{explained} + SS_{unexplained}\\). ease looking, ’d like see one place. accomplish goal multiple ways, ’d like introducing handy ggplot library called patchwork makes easy combine multiple plots together.","code":"\nlibrary(patchwork)\n\n(total_plot +plot_spacer())/(exp_plot+res_plot)+\n  plot_annotation(title = 'SStotal = SSexplained + SSunexplained')"},{"path":"regression.html","id":"advanced-concept-best-fit-line-by-brute-force-search","chapter":"12 Regression","heading":"12.6 Advanced Concept: Best fit line by brute force search","text":"didn’t know formulas compute slope y-intercept best fit line? knew definition best fit line, line minimizes sum squared deviations point line (\\(SS_{residual}\\)), potentially find best line trial error.example, pick slope y-intercept calculate \\(SS_{residual}\\), change slope intercept , see \\(SS_{residual}\\) get’s smaller…keep can’t make \\(SS_{residual}\\) smaller. end found correct parameters best fit line.quick example forcing R try find correct values slope intercept trial error. Sometimes converges right answer 1000 attempts, sometimes doesn’t. basic idea :choose starting slope intercept (b)compute \\(SS_{residual}\\) points linerandomly change slope intercept small value (.01)compute \\(SS_{residual}\\) .\\(SS_{residual}\\) smaller , save new slope intercept, keep repeatingIf \\(SS_{residual}\\) larger , forget new slope intercept, keep randomly changing produce smaller \\(SS_{residual}\\).long time…algorithm allows changes slope intercept make \\(SS_{residual}\\) smaller, time, value converge minimum possible value…one get best fit line.","code":"\nstep_size <- .01\n\nslope = 2\nb = 1\nsome_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = slope*X+b) %>%\n  mutate(error = Y-Y_pred)\n\nsum_deviations <- sum(some_data$error^2)\n\nfor(i in 1:1000){\n  \n  new_slope <- slope + step_size * sample(c(-1,1), 1)\n  new_b <- b + step_size * sample(c(-1,1), 1)\n  \n  some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),\n                        X= c(3,5,4,2,6,7,8,9)) %>%\n  mutate(Y_pred = new_slope*X+new_b) %>%\n  mutate(error = Y-Y_pred)\n\nnew_sum_deviations <- sum(some_data$error^2)\n\nif(new_sum_deviations < sum_deviations) {\n  sum_deviations <- new_sum_deviations\n  slope <- new_slope\n  b <- new_b\n}\n  \n}\n\nslope\n#> [1] 0.51\nb\n#> [1] 0.95"},{"path":"regression.html","id":"lab-12-generalization-assignment","chapter":"12 Regression","heading":"12.7 Lab 12 Generalization Assignment","text":"","code":""},{"path":"regression.html","id":"instructions-11","chapter":"12 Regression","heading":"12.7.1 Instructions","text":"general, labs present discussion problems issues example code like , students tasked completing generalization assignments, showing can work concepts tools independently.assignment instructions following:Work inside R project “StatsLab1” usingCreate new R Markdown document called “Lab12.Rmd”Use Lab12.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 12 blackboard.","code":""},{"path":"regression.html","id":"problems-11","chapter":"12 Regression","heading":"12.7.2 Problems","text":"linear regression data? Explain.","code":"\nY <- c(10,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7,8,9,10)\nX <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19)"},{"path":"regression.html","id":"references","chapter":"12 Regression","heading":"12.8 References","text":"","code":""},{"path":"semester-1-project.html","id":"semester-1-project","chapter":"Semester 1 project","heading":"Semester 1 project","text":"“11/8/2020 | Last Compiled: 2022-04-26”","code":""},{"path":"semester-1-project.html","id":"overview-13","chapter":"Semester 1 project","heading":"12.9 Overview","text":"semester project assignment due end semester submitted link Blackboard.discuss project across semester. overview, demonstrating can conduct reproducible analysis, analysis data independently verifiable. example, someone else obtain data code independently reproduce analysis.complete three related parts.Reproducible Report: Obtain open-data existing psych paper, load data R, attempt reproduce statistical analysis original authors reported.Reproducible Report: Obtain open-data existing psych paper, load data R, attempt reproduce statistical analysis original authors reported.APA paper: Learn use papaja package allows compile .Rmd files APA style manuscripts pdf form. , write short APA-style research report describes reproducible analysis.APA paper: Learn use papaja package allows compile .Rmd files APA style manuscripts pdf form. , write short APA-style research report describes reproducible analysis.Simulation based power analysis: Include simulation based power analysis end APA paper.Simulation based power analysis: Include simulation based power analysis end APA paper.","code":""},{"path":"semester-1-project.html","id":"part-1-reproducible-report-10-points","chapter":"Semester 1 project","heading":"12.10 Part 1: Reproducible report (10 points)","text":"","code":""},{"path":"semester-1-project.html","id":"finding-a-paper-with-data","chapter":"Semester 1 project","heading":"12.10.1 Finding a paper with data","text":"tips finding psych paper open data. important, assignment need re-analyze data particular paper. Many papers multiple experiments, multiple analyses, including analyses may familiar . can restrict re-analysis portion paper. example, might re-analyze results one experiment, perhaps results relevant one tests reported experiment. can limit re-analyses tests covered lecture lab.https://osf.io open science framework contains many repositories open data part published papersSome journals, including Psychological Science, put badges papers open data. Look blue open-data badge. usually find link open-data paper.https://crumplab.github.io/statisticsLab/ lab manual use teaching R undergraduates. labs (especially 6 11) involve paper Psych science open-data.","code":""},{"path":"semester-1-project.html","id":"loading-the-data-into-r","chapter":"Semester 1 project","heading":"12.10.2 Loading the data into R","text":"data find many different formats. possible load R transform data format/organization need complete analysis.","code":""},{"path":"semester-1-project.html","id":"re-analysis-of-original-data","chapter":"Semester 1 project","heading":"12.10.3 Re-analysis of original data","text":"Focus single analysis relevant one research questions. example, analysis involved several t-tests:Conduct report t-testsReport table meansReport graph means","code":""},{"path":"semester-1-project.html","id":"write-a-reproducible-report","chapter":"Semester 1 project","heading":"12.10.4 Write a reproducible report","text":"concept reproducible report someone else exactly reproduce analysis given report. easy make reproducible reports using R markdown. write report .Rmd file, file includes scripts loading analyzing data, sharing .rmd file, people can exactly reproduce report.report include following (points add 10 part 1).brief description research question experiment (citation paper, link find data) (3 points)brief description research question experiment (citation paper, link find data) (3 points)R code chunks necessary complete re-analysis (3 points).R code chunks necessary complete re-analysis (3 points).write-re-analysis results. (3 points)write-re-analysis results. (3 points)brief discussion whether successful . (1 point)brief discussion whether successful . (1 point)","code":""},{"path":"semester-1-project.html","id":"part-2-apa-paper-in-r-markdown-10-points","chapter":"Semester 1 project","heading":"12.11 Part 2: APA paper in R markdown (10 points)","text":"","code":""},{"path":"semester-1-project.html","id":"papaja-1","chapter":"Semester 1 project","heading":"12.11.1 Papaja","text":"part 2, learn use papaja package create APA style manuscripts using R markdown. discuss use papaja class. create new .rmd file using papaja template, transfer reproducible report format. write brief sections :abstract (50-100 words) (1 point)introduction (1 two paragraphs) (1 point)methods (1 paragraph) (1 point)results (re-analysis results) (3 point)must include R code chunks analysisfull points reporting results also reproducible (hand), example (see ).discussion (brief, 1 paragraph) (1 point)references (cite paper, anything else want cite) (1 point)completed .Rmd file succesfully compiled .pdf using papaja (2 points), purpose write full-length APA paper, get experience using papaja package.","code":""},{"path":"semester-1-project.html","id":"part-3-power-analysis-8-points","chapter":"Semester 1 project","heading":"12.12 Part 3: Power analysis (8 points)","text":"","code":""},{"path":"semester-1-project.html","id":"simulation-based-power-analysis","chapter":"Semester 1 project","heading":"12.12.1 Simulation based power analysis","text":"part 3 add simulation-based power analysis APA-style manuscript. Specifically, report graph showing power-curve design. discuss conduct simulation based power analyses class.following included general discussion APA-paper (part 2).R code chunk conducting power analysis (3 points)paragraph two discussing explaining power analysis reader, well reporting results power analysis. (3 points)graph depicting power-curve design. (2 points)statement design might changed achieve desired preferred amount power. (2 points)","code":""},{"path":"semester-1-project.html","id":"example-of-a-completed-project","chapter":"Semester 1 project","heading":"12.13 Example of a completed project","text":"give better idea looking completed project .source code located github repository https://github.com/CrumpLab/psyc7709Lab/tree/master/semester_project.repository contains:Part 1: .rmd file reproduces analysis Experiment 3 Rosenbaum, Mama, Algom (2017)Part 2: APA-style manuscript version reproducible report using papajaPart 3: section end APA-style manuscript conducts simulation based power-analysis design.","code":""},{"path":"semester-1-project.html","id":"semester-project-submission","chapter":"Semester 1 project","heading":"12.14 Semester Project Submission","text":"Create new git-enabled R project semester project, upload githubWork folder, complete Parts 1 3. finished minimum:folder data obtainedA compiled .Rmd html file part 1A compiled .Rmd .pdf part 2 3a bib file referencesSubmit link github repository Blackboard assignment semester long project","code":""},{"path":"shaping-data.html","id":"shaping-data","chapter":"13 Shaping Data","heading":"13 Shaping Data","text":"","code":""},{"path":"shaping-data.html","id":"overview-14","chapter":"13 Shaping Data","heading":"13.1 Overview","text":"Welcome back. lab overviews practical aspects form shape data can take. use coding concepts R mostly familiar last semester, use lab opportunity little bit review. readings textbook lab, may find following links generally helpful:dplyrData-transformation ChapterApologies videos two parts…couldn’t compete vacuum cleaner.","code":""},{"path":"shaping-data.html","id":"background-1","chapter":"13 Shaping Data","heading":"13.2 Background","text":"structure research designs imply produced data particular properties, data can saved different formats, data can arrive different shapes sizes, must transformed specific shapes order conduct specific analyses. Thus, shape data, shaping data, part parcel research, beginning end. spend good deal time discussing data organization manipulation. conversation unfold semester. remaining part background section ’m going identify places data organization important. topics easily expanded whole chapter. now, goal alert . cover topics detail lab.","code":""},{"path":"shaping-data.html","id":"data-creation","chapter":"13 Shaping Data","heading":"13.2.1 Data-Creation","text":"save data?kind file formats saving data?Perhaps discipline Psychology large varied slow adopt widespread standards formatting data. Certainly, many kinds data standards one research project might apply another. considerations keep mind:decisions make save data consequences later analysis. save format analysis-ready, won’t transform data laterSave data machine-readable transformable format. Machine-readable means can input data program language like R, transformable means can reshape data preserving inherent factor structure (lab).numerous file formats. typically reading .txt (text files), .csv (comma separated value) files, sometimes .xlsx (excel) proprietary files.","code":""},{"path":"shaping-data.html","id":"reproducible-analysis-pipeline","chapter":"13 Shaping Data","heading":"13.2.2 Reproducible analysis pipeline","text":"Inputting data RRe-shaping analysisData-shaping practical part data-analysis R. Using scripts handle data input analysis allows reproducible pipeline. reproducible helpful others. analysis pipeline reproducible, may able fix mistakes make. example, accidentally delete something, move something around hand, may record performed operation, forget , may never able go back fix errors. use script analysis never “touch” data hand. Instead, actions taken script. Even script makes mistake, mistake least identifiable fixable. someone else raw data, analysis script, input data script, output analysis reported. nuts bolts analysis script often include many transformations data: raw data inputted R, might saved different variables, pre-processed various ways, reformatted sliced diced meet input requirements R statistical analysis functions.","code":""},{"path":"shaping-data.html","id":"knowing-your-design-and-analysis","chapter":"13 Shaping Data","heading":"13.2.3 Knowing your design and analysis","text":"Design implied shapeSimulated data analysisA major overarching goal end course understand create statistical analyses customized exactly nuances research designs. words, WYOR (Write Recipes) statistics. order get , important recognize fundamental connections research design shape data collected design. lab, use R conceptual practical tool illustrate simulated data can created particular designs. simulated data, can test planned analysis advance obtaining real data.","code":""},{"path":"shaping-data.html","id":"concept-i-transformability","chapter":"13 Shaping Data","heading":"13.3 Concept I: Transformability","text":"short concept section illustrate concept transformability. basic idea transformable data can arranged different shapes back without losing information. practical matter, many functions depend inputs formatted particular way. Thus, data transformation often required get data shape can inputted function.","code":""},{"path":"shaping-data.html","id":"long-vs.-wide-data","chapter":"13 Shaping Data","heading":"13.3.1 Long vs. wide data","text":"general rule, functions statistical tests R require data organized long-format. personally find convenient means :can get data long format, thenI can just anything want using R functions analysis visualizationLet’s look example wide data. Imagine five people, measured many times check phone, morning, afternoon, evening.5 people, collect measures three times , must 5 x 3 total cells. wide version data 5x3 matrix 5 rows (different people), 3 columns (morning, afternoon, evening). matrix 15 cells , capable representing complete cases data. Wide-data perfectly fine way represent data, nothing inherently wrong wide-data. However, mentioned , many functions R written assumption data shaped long-data.example thing long-format:can see, long-data format, data gets really long. rule dependent measure (e.g., counts phone looking) listed single row. ar 5 x 3 = 15 total individual measures, must 15 rows long-form representation.","code":"\nwide_data <- data.frame(person = 1:5,\n                        Morning = c(1,3,2,4,3),\n                        Afternoon = c(3,4,5,4,7),\n                        Evening = c(7,8,7,6,9))\nknitr::kable(wide_data)\nlong_data <- data.frame(person = rep(1:5, each=3),\n                        time_of_day = rep(c(\"Morning\", \"Afternoon\", \"Evening\"),5),\n                        counts = c(1,3,7,3,4,8,2,5,7,4,4,6,3,7,9))\nknitr::kable(long_data)\n\n\nperson <- rep(1:5,3)\ntime_of_day <- rep(c(\"Morning\", \"Afternoon\", \"Evening\"),each =5)\ncounts <- c(1,3,7,3,4,8,2,5,7,4,4,6,3,7,9)\n\ntest <- data.frame(person,time_of_day,counts)"},{"path":"shaping-data.html","id":"converting-wide-to-long","chapter":"13 Shaping Data","heading":"13.3.2 Converting Wide to long","text":"Sometimes might receive data wide format need convert long format. can accomplished R multiple different ways. can write custom code , can try using various existing functions. Google “wide long R”, might come across curious history functions developed provide function. One part history functions keep getting re-written “clear” work. ’ll admit found functions confusing , usually find messing around conversion ’m looking . case, example pivoting wide long using tidyr","code":"\nlibrary(tidyr)\n\npivot_longer(data = wide_data, \n             cols = !person,\n             names_to = \"time_of_day\",\n             values_to = \"counts\")\n#> # A tibble: 15 × 3\n#>    person time_of_day counts\n#>     <int> <chr>        <dbl>\n#>  1      1 Morning          1\n#>  2      1 Afternoon        3\n#>  3      1 Evening          7\n#>  4      2 Morning          3\n#>  5      2 Afternoon        4\n#>  6      2 Evening          8\n#>  7      3 Morning          2\n#>  8      3 Afternoon        5\n#>  9      3 Evening          7\n#> 10      4 Morning          4\n#> 11      4 Afternoon        4\n#> 12      4 Evening          6\n#> 13      5 Morning          3\n#> 14      5 Afternoon        7\n#> 15      5 Evening          9"},{"path":"shaping-data.html","id":"custom-to-long","chapter":"13 Shaping Data","heading":"13.3.3 Custom to long","text":"grim reality data made take huge number different formats. , get R, may wrangle shape can proceed analyze . example, show strange data format, write custom code wrangle long-format. just illustrate general idea sometimes may custom data shaping.Consider following format. subject’s phone checking count number separated commas. first number always morning, second afternoon, third evening. Individual subjects separated semi-colons. Thus, first three numbers subject 1, next three subject 2, . can see, data perfectly preserved, one line., custom format , now need get long format. Unfortunately, tidy-verse functions shaping weird custom data formats. , someone pull tricks hat.","code":"\nthe_data<-\"1,3,7;3,4,8;2,5,7;4,4,6;3,7,9\"\nlibrary(dplyr)\n\n\nsubjects <- unlist(strsplit(the_data, split = \";\"))\nsubjects\n#> [1] \"1,3,7\" \"3,4,8\" \"2,5,7\" \"4,4,6\" \"3,7,9\"\nsubjects <- strsplit(subjects,split=\",\")\nsubjects\n#> [[1]]\n#> [1] \"1\" \"3\" \"7\"\n#> \n#> [[2]]\n#> [1] \"3\" \"4\" \"8\"\n#> \n#> [[3]]\n#> [1] \"2\" \"5\" \"7\"\n#> \n#> [[4]]\n#> [1] \"4\" \"4\" \"6\"\n#> \n#> [[5]]\n#> [1] \"3\" \"7\" \"9\"\n\nsubjects <- t(data.frame(subjects))\nsubjects\n#>                  [,1] [,2] [,3]\n#> c..1....3....7.. \"1\"  \"3\"  \"7\" \n#> c..3....4....8.. \"3\"  \"4\"  \"8\" \n#> c..2....5....7.. \"2\"  \"5\"  \"7\" \n#> c..4....4....6.. \"4\"  \"4\"  \"6\" \n#> c..3....7....9.. \"3\"  \"7\"  \"9\"\ncolnames(subjects) <- c(\"Morning\",\"Afternoon\",\"Evening\")\nsubjects\n#>                  Morning Afternoon Evening\n#> c..1....3....7.. \"1\"     \"3\"       \"7\"    \n#> c..3....4....8.. \"3\"     \"4\"       \"8\"    \n#> c..2....5....7.. \"2\"     \"5\"       \"7\"    \n#> c..4....4....6.. \"4\"     \"4\"       \"6\"    \n#> c..3....7....9.. \"3\"     \"7\"       \"9\"\nrow.names(subjects) <- 1:5\nsubjects <- as.data.frame(subjects) %>%\n  mutate(person=1:5)\n\npivot_longer(data = subjects, \n             cols = 1:3,\n             names_to = \"time_of_day\",\n             values_to = \"counts\")\n#> # A tibble: 15 × 3\n#>    person time_of_day counts\n#>     <int> <chr>       <chr> \n#>  1      1 Morning     1     \n#>  2      1 Afternoon   3     \n#>  3      1 Evening     7     \n#>  4      2 Morning     3     \n#>  5      2 Afternoon   4     \n#>  6      2 Evening     8     \n#>  7      3 Morning     2     \n#>  8      3 Afternoon   5     \n#>  9      3 Evening     7     \n#> 10      4 Morning     4     \n#> 11      4 Afternoon   4     \n#> 12      4 Evening     6     \n#> 13      5 Morning     3     \n#> 14      5 Afternoon   7     \n#> 15      5 Evening     9"},{"path":"shaping-data.html","id":"practical-i-simulated-data-for-different-designs","chapter":"13 Shaping Data","heading":"13.4 Practical I: Simulated data for different designs","text":"purpose section focus process creating data-structures R following properties:appropriately represent data necessary particular designThey formatted R functions statistical tests can performed ","code":""},{"path":"shaping-data.html","id":"one-sample-t-test-1","chapter":"13 Shaping Data","heading":"13.4.1 One-sample t-test","text":"one-sample t-test involves vector means. , vector means created sampling 10 values unit normal distribution.Consider design 50 participants. participant takes TRUE/FALSE quiz 10 questions. researcher wants apply one-sample t-test test whether participants performed better chance.Create example raw data represents subjects’ answer question\n50 participants x 10 questions, must 500 cells\nsample 1s 0s binomial indicate correct vs incorrect one question\nCreate example raw data represents subjects’ answer questionThere 50 participants x 10 questions, must 500 cellsThere 50 participants x 10 questions, must 500 cellsI sample 1s 0s binomial indicate correct vs incorrect one questionI sample 1s 0s binomial indicate correct vs incorrect one questionCreate summary vector means suitable t.test functionCreate summary vector means suitable t.test functionRun t.testRun t.test","code":"\ndv <- rnorm(10,0,1)\nt.test(dv)\n#> \n#>  One Sample t-test\n#> \n#> data:  dv\n#> t = 4.2546, df = 9, p-value = 0.002128\n#> alternative hypothesis: true mean is not equal to 0\n#> 95 percent confidence interval:\n#>  0.43278 1.41551\n#> sample estimates:\n#> mean of x \n#> 0.9241449\nsubject_data <- matrix( rbinom(50*10,1,.5), ncol=10, nrow=50)\nsubject_means <- rowMeans(subject_data)\nt.test(subject_means, mu=.5)\n#> \n#>  One Sample t-test\n#> \n#> data:  subject_means\n#> t = -0.44361, df = 49, p-value = 0.6593\n#> alternative hypothesis: true mean is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.4446992 0.5353008\n#> sample estimates:\n#> mean of x \n#>      0.49"},{"path":"shaping-data.html","id":"paired-sample-t-test-1","chapter":"13 Shaping Data","heading":"13.4.2 Paired sample t-test","text":"Consider design measuring fluctuations weight function weekday vs. weekend. Researchers 25 people weigh 5 times throughout day Wednesday, 5 times throughout day Sunday. Create data frame represents situation, conduct paired sample t-test.break , create long data.frame four columns: Subject number, Day, measurement number, weight. many rows must ? 25 people, 5 measurements per day, two days measurements. long-format, one measure per row. Therefore, 25 x 5 x 2 = 250 rows.repeat number 1 25, 10 times .day column two levels, Wednesday vs. Sunday. level appear 5 times subject.need variable represent five measurements taken per day. Let’s call measurement_numberWe need pretend measurements. now, let’s just choose random numbers normal distribution. need 250 numbers.Next, let’s combine vectors data.frameNote, defined everything inside single data.frameThe data.frame weight_data now represents complete shape data implied research design. However, data yet ready t.test function. t.test function assumes inputs means participant condition. , raw data must summarized first. words, must find mean weight within day participant measured. continue use dplyr syntax group summarize data.Finally, can run t.test","code":"\nsubject_number <- rep(1:25, each=10)\n#day <- rep(c(\"Wednesday\",\"Sunday\"), each = 5) # makes one subject\nday <- rep(rep(c(\"Wednesday\",\"Sunday\"), each = 5), 25)\nmeasurement_number <- rep(1:5, 2*25)\nweights <- rnorm(250, 100, 25)\nweight_data <- data.frame(subject_number,\n                          day,\n                          measurement_number,\n                          weights)\nhead(weight_data)\n#>   subject_number       day measurement_number   weights\n#> 1              1 Wednesday                  1  79.23932\n#> 2              1 Wednesday                  2 114.84009\n#> 3              1 Wednesday                  3  78.59676\n#> 4              1 Wednesday                  4 150.13053\n#> 5              1 Wednesday                  5 105.23875\n#> 6              1    Sunday                  1 101.74751\nweight_data <- data.frame(subject_number = rep(1:25, each=10),\n                          day = rep(rep(c(\"Wednesday\",\"Sunday\"), each = 5), 25),\n                          measurement_number = rep(1:5, 2*25),\n                          weights = rnorm(250, 100, 25))\nsubject_means <- weight_data %>%\n  group_by(subject_number,day) %>%\n  summarize(mean_weight = mean(weights), .groups = \"drop\")\n\nhead(subject_means)\n#> # A tibble: 6 × 3\n#>   subject_number day       mean_weight\n#>            <int> <chr>           <dbl>\n#> 1              1 Sunday           82.8\n#> 2              1 Wednesday        88.1\n#> 3              2 Sunday          101. \n#> 4              2 Wednesday       106. \n#> 5              3 Sunday           87.4\n#> 6              3 Wednesday        92.2\nt.test(mean_weight~day, paired=TRUE, data=subject_means)\n#> \n#>  Paired t-test\n#> \n#> data:  mean_weight by day\n#> t = 0.076271, df = 24, p-value = 0.9398\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -6.621660  7.129843\n#> sample estimates:\n#> mean of the differences \n#>               0.2540919"},{"path":"shaping-data.html","id":"independent-sample-t-test-1","chapter":"13 Shaping Data","heading":"13.4.3 Independent sample t-test","text":"researcher gives 10 subjects recall memory test. read 50 words later memory test. short break half participants put noisy room, half put quiet room. given piece paper 50 lines asked write memory words can remember. raw data coded 1s 0s, 1 representing correctly recalled word 0 represent incorrectly recalled word. researcher wants t-test number correctly recalled words noisy vs quiet room.Note, switch using tibble instead data.frame.","code":"\n\nsubjects <- rep(1:10, each = 50)\nroom <- rep(c(\"Noisy\",\"Quiet\"), each = 50*5)\nwords <- rep(1:50, 10)\ncorrect <- rbinom(500,1,.5)\n\nrecall_data <- tibble(subjects,\n                      room,\n                      words,\n                      correct)\n\nrecall_data\n#> # A tibble: 500 × 4\n#>    subjects room  words correct\n#>       <int> <chr> <int>   <int>\n#>  1        1 Noisy     1       1\n#>  2        1 Noisy     2       1\n#>  3        1 Noisy     3       1\n#>  4        1 Noisy     4       1\n#>  5        1 Noisy     5       1\n#>  6        1 Noisy     6       1\n#>  7        1 Noisy     7       1\n#>  8        1 Noisy     8       1\n#>  9        1 Noisy     9       1\n#> 10        1 Noisy    10       0\n#> # … with 490 more rows\n\ncount_data <- recall_data %>%\n  group_by(subjects,room) %>%\n  summarize(number_correct = sum(correct), .groups=\"drop\")\n\ncount_data\n#> # A tibble: 10 × 3\n#>    subjects room  number_correct\n#>       <int> <chr>          <int>\n#>  1        1 Noisy             25\n#>  2        2 Noisy             24\n#>  3        3 Noisy             20\n#>  4        4 Noisy             25\n#>  5        5 Noisy             19\n#>  6        6 Quiet             24\n#>  7        7 Quiet             26\n#>  8        8 Quiet             29\n#>  9        9 Quiet             27\n#> 10       10 Quiet             31\n\nt.test(number_correct~room, var.equal=TRUE, data=count_data)\n#> \n#>  Two Sample t-test\n#> \n#> data:  number_correct by room\n#> t = -2.7175, df = 8, p-value = 0.02635\n#> alternative hypothesis: true difference in means between group Noisy and group Quiet is not equal to 0\n#> 95 percent confidence interval:\n#>  -8.8732154 -0.7267846\n#> sample estimates:\n#> mean in group Noisy mean in group Quiet \n#>                22.6                27.4"},{"path":"shaping-data.html","id":"simple-linear-regression","chapter":"13 Shaping Data","heading":"13.4.4 Simple linear regression","text":"100 people write height centimeters, day month born. Conduct linear regression see day month explains variation height.","code":"\n\npeople <- tibble(height = rnorm(100, 90, 10),\n                 day = sample(1:31, 100, replace=TRUE))\n\npeople\n#> # A tibble: 100 × 2\n#>    height   day\n#>     <dbl> <int>\n#>  1  112.      5\n#>  2   91.9    20\n#>  3  101.     31\n#>  4   98.6    17\n#>  5   77.5     8\n#>  6   78.4     8\n#>  7   85.7    14\n#>  8   90.2     9\n#>  9   63.2    30\n#> 10   81.6    12\n#> # … with 90 more rows\n\nlm.out <- lm(height~day, data= people)\nlm.out\n#> \n#> Call:\n#> lm(formula = height ~ day, data = people)\n#> \n#> Coefficients:\n#> (Intercept)          day  \n#>    87.59461     -0.01262\nsummary(lm.out)\n#> \n#> Call:\n#> lm(formula = height ~ day, data = people)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -26.7687  -6.4261  -0.1001   6.9333  24.4441 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 87.59461    2.16125  40.530   <2e-16 ***\n#> day         -0.01262    0.11110  -0.114     0.91    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.906 on 98 degrees of freedom\n#> Multiple R-squared:  0.0001316,  Adjusted R-squared:  -0.01007 \n#> F-statistic: 0.0129 on 1 and 98 DF,  p-value: 0.9098"},{"path":"shaping-data.html","id":"one-way-anova-1","chapter":"13 Shaping Data","heading":"13.4.5 One-way ANOVA","text":"haven’t yet covered one-way ANOVA course. may already familiar ANOVA. now, can think kind extension t-tests deal single-factor designs one level. example, consider extending paired t-test sample . example, 25 people weighed 5 times throughout day Wednesday, 5 times throughout day Sunday. independent variable day, two levels (Wednesday vs Sunday). stop Wednesday vs. Sunday? five days week. Maybe day influence weight.Consider simulating data one-factor design 7 levels, one day week. N=25, person measures 5 times day.see later labs ANOVA, analysis can performed one-line aov functionAnd, also learn class, ANOVA Linear Regression fundamentally analysis, also use lm function treat analysis regression.","code":"\nweight_data <- tibble(subject_number = rep(1:25, each=5*7),\n                          day = rep(rep(c(\"S\",\"M\",\"T\",\"W\",\"Th\",\"F\",\"Sa\"),\n                                      each = 5), 25),\n                          measurement_number = rep(1:5, 7*25),\n                          weights = rnorm(25*5*7, 100, 25))\nsubject_means <- weight_data %>%\n  group_by(subject_number,day) %>%\n  summarize(mean_weight = mean(weights), .groups=\"drop\")\n\nsubject_means\n#> # A tibble: 175 × 3\n#>    subject_number day   mean_weight\n#>             <int> <chr>       <dbl>\n#>  1              1 F           102. \n#>  2              1 M           108. \n#>  3              1 S           107. \n#>  4              1 Sa          116. \n#>  5              1 T            97.7\n#>  6              1 Th           95.5\n#>  7              1 W            88.3\n#>  8              2 F            85.0\n#>  9              2 M            85.1\n#> 10              2 S           108. \n#> # … with 165 more rows\n\naov.out <- aov(mean_weight ~ day, data = subject_means)\nsummary(aov.out)\n#>              Df Sum Sq Mean Sq F value Pr(>F)\n#> day           6    668   111.4   0.759  0.603\n#> Residuals   168  24655   146.8\nsubject_means <- weight_data %>%\n  group_by(subject_number,day) %>%\n  summarize(mean_weight = mean(weights), .groups=\"drop\")\n\nsubject_means\n#> # A tibble: 175 × 3\n#>    subject_number day   mean_weight\n#>             <int> <chr>       <dbl>\n#>  1              1 F           102. \n#>  2              1 M           108. \n#>  3              1 S           107. \n#>  4              1 Sa          116. \n#>  5              1 T            97.7\n#>  6              1 Th           95.5\n#>  7              1 W            88.3\n#>  8              2 F            85.0\n#>  9              2 M            85.1\n#> 10              2 S           108. \n#> # … with 165 more rows\n\nlm.out <- lm(mean_weight ~ day, data = subject_means)\nsummary(lm.out)\n#> \n#> Call:\n#> lm(formula = mean_weight ~ day, data = subject_means)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -29.6835  -8.5188   0.1274   8.4696  29.0607 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 100.3022     2.4229  41.398   <2e-16 ***\n#> dayM         -1.4732     3.4264  -0.430    0.668    \n#> dayS          0.5147     3.4264   0.150    0.881    \n#> daySa         2.0101     3.4264   0.587    0.558    \n#> dayT          2.3145     3.4264   0.675    0.500    \n#> dayTh        -3.4167     3.4264  -0.997    0.320    \n#> dayW          1.9340     3.4264   0.564    0.573    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.11 on 168 degrees of freedom\n#> Multiple R-squared:  0.0264, Adjusted R-squared:  -0.008373 \n#> F-statistic: 0.7592 on 6 and 168 DF,  p-value: 0.603"},{"path":"shaping-data.html","id":"factorial-anova","chapter":"13 Shaping Data","heading":"13.4.6 Factorial ANOVA","text":"one-way ANOVA extends t-test design terms number levels single factor. also possible run experiments multiple independent variables, multiple levels.example, let’s continue design make minor changes becomes factorial 7x2 design. 7x2 design two independent variables. first one 7 levels, second one 2 levels.modify example include time day factor. example, people measure weight two times morning, two times evening. Thus, two independent variables. Day (S, M, T, W, Th, F, Sa) time day (morning, evening). keep number subjects 25.Just like can treat one-way ANOVA regression, can also treat Factorial ANOVA multiple regression:","code":"\nweight_data <- tibble(subject_number = rep(1:25, each=4*7),\n                          day = rep(rep(c(\"S\",\"M\",\"T\",\"W\",\"Th\",\"F\",\"Sa\"),\n                                      each = 4), 25),\n                          time_of_day = rep(c(\"Morning\",\"Morning\",\n                                              \"Evening\",\"Evening\"),7*25),\n                          measurement_number = rep(rep(1:2, 2), 7*25),\n                          weights = rnorm(25*4*7, 100, 25))\nsubject_means <- weight_data %>%\n  group_by(subject_number,day, time_of_day) %>%\n  summarize(mean_weight = mean(weights), .groups=\"drop\")\n\nsubject_means\n#> # A tibble: 350 × 4\n#>    subject_number day   time_of_day mean_weight\n#>             <int> <chr> <chr>             <dbl>\n#>  1              1 F     Evening            65.5\n#>  2              1 F     Morning            90.9\n#>  3              1 M     Evening            84.9\n#>  4              1 M     Morning            84.5\n#>  5              1 S     Evening           101. \n#>  6              1 S     Morning           104. \n#>  7              1 Sa    Evening           112. \n#>  8              1 Sa    Morning           116. \n#>  9              1 T     Evening            99.8\n#> 10              1 T     Morning           117. \n#> # … with 340 more rows\n\naov.out <- aov(mean_weight ~ day*time_of_day, data = subject_means)\nsummary(aov.out)\n#>                  Df Sum Sq Mean Sq F value Pr(>F)\n#> day               6   2039   339.9   1.038  0.401\n#> time_of_day       1    156   156.3   0.477  0.490\n#> day:time_of_day   6   1443   240.6   0.734  0.622\n#> Residuals       336 110072   327.6\nsubject_means <- weight_data %>%\n  group_by(subject_number,day, time_of_day) %>%\n  summarize(mean_weight = mean(weights), .groups=\"drop\")\n\nsubject_means$day <-as.factor(subject_means$day)\nsubject_means$time_of_day <-as.factor(subject_means$time_of_day)\n\nsubject_means\n#> # A tibble: 350 × 4\n#>    subject_number day   time_of_day mean_weight\n#>             <int> <fct> <fct>             <dbl>\n#>  1              1 F     Evening            65.5\n#>  2              1 F     Morning            90.9\n#>  3              1 M     Evening            84.9\n#>  4              1 M     Morning            84.5\n#>  5              1 S     Evening           101. \n#>  6              1 S     Morning           104. \n#>  7              1 Sa    Evening           112. \n#>  8              1 Sa    Morning           116. \n#>  9              1 T     Evening            99.8\n#> 10              1 T     Morning           117. \n#> # … with 340 more rows\n\nlm.out <- lm(mean_weight ~ day*time_of_day, data = subject_means)\nsummary(lm.out)\n#> \n#> Call:\n#> lm(formula = mean_weight ~ day * time_of_day, data = subject_means)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -51.947 -11.853  -0.171  11.607  54.663 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               92.2268     3.6199  25.478   <2e-16 ***\n#> dayM                      10.6690     5.1193   2.084   0.0379 *  \n#> dayS                       5.9800     5.1193   1.168   0.2436    \n#> daySa                      6.6601     5.1193   1.301   0.1942    \n#> dayT                      10.8615     5.1193   2.122   0.0346 *  \n#> dayTh                      3.8682     5.1193   0.756   0.4504    \n#> dayW                       8.0789     5.1193   1.578   0.1155    \n#> time_of_dayMorning         4.1781     5.1193   0.816   0.4150    \n#> dayM:time_of_dayMorning   -9.6100     7.2398  -1.327   0.1853    \n#> dayS:time_of_dayMorning    3.0051     7.2398   0.415   0.6783    \n#> daySa:time_of_dayMorning  -3.0615     7.2398  -0.423   0.6727    \n#> dayT:time_of_dayMorning   -6.0307     7.2398  -0.833   0.4054    \n#> dayTh:time_of_dayMorning   0.7375     7.2398   0.102   0.9189    \n#> dayW:time_of_dayMorning   -4.9324     7.2398  -0.681   0.4962    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 18.1 on 336 degrees of freedom\n#> Multiple R-squared:  0.032,  Adjusted R-squared:  -0.00545 \n#> F-statistic: 0.8545 on 13 and 336 DF,  p-value: 0.6019\nanova(lm.out)\n#> Analysis of Variance Table\n#> \n#> Response: mean_weight\n#>                  Df Sum Sq Mean Sq F value Pr(>F)\n#> day               6   2039  339.90  1.0376 0.4006\n#> time_of_day       1    156  156.27  0.4770 0.4903\n#> day:time_of_day   6   1443  240.56  0.7343 0.6223\n#> Residuals       336 110072  327.60"},{"path":"shaping-data.html","id":"lab-1-generalization-assignment-1","chapter":"13 Shaping Data","heading":"13.5 Lab 1 Generalization Assignment","text":"NOTE: Spring 2022, following video semester long project tab setting new R project github repo. making .Rmd, adding vignettes folder, displaying work pkgdown website.","code":""},{"path":"shaping-data.html","id":"instructions-12","chapter":"13 Shaping Data","heading":"13.5.1 Instructions","text":"assignment instructions following:Work inside new R project createdCreate new R Markdown document called “Lab1.Rmd”, inside vignettes folderUse Lab1.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .finished, compile pkgdown website running pkgdown::build_site() lab work displayed website.Push everything github, make sure github repo public, github pages enabled. Make sure can view website. Submit URL website blackboard.","code":""},{"path":"shaping-data.html","id":"problem-6-points","chapter":"13 Shaping Data","heading":"13.5.2 Problem (6 points)","text":"Download Lab1_data.xlsx data file. file contains fake data 2x3x2 repeated measures design, 10 participants. data wide format. link.https://github.com/CrumpLab/rstatsmethods/raw/master/vignettes/Stats2/Lab1_data.xlsxYour task convert data long format, store long-format data data.frame tibble. Print long-form data lab1.Rmd, show make appropriate conversion. extra fun, show two different ways solve problem.need modify excel hand help solve problem OK, just make note lab work.","code":""},{"path":"shaping-data.html","id":"references-1","chapter":"13 Shaping Data","heading":"13.6 References","text":"","code":""},{"path":"multiple-regression-i.html","id":"multiple-regression-i","chapter":"14 Multiple Regression I","heading":"14 Multiple Regression I","text":"","code":""},{"path":"multiple-regression-i.html","id":"readings-3","chapter":"14 Multiple Regression I","heading":"14.1 Readings","text":"Chapters 4 5 Abdi, Edelman, Dowling, & Valentin59.","code":""},{"path":"multiple-regression-i.html","id":"overview-15","chapter":"14 Multiple Regression I","heading":"14.2 Overview","text":"purpose lab show simple linear regression (using line) can extended multiple linear regression (using plane hyperplane).begin, let’s take stock far got last semester. Remember, entire purpose course understand statistical inference applied experimental designs. experimental designs, researchers interested whether manipulations one experimental variable cause change measured dependent variable. Last semester considered simple situations involving one independent variable (manipulation) two levels, one dependent variable (measure. design like analyzed t-test.Note also, end last semester began discussing another relatively simple situation involving two variables, predictor variable dependent variable; , saw linear regression applied examine potential linear relationships predictor variable dependent variable. figure implies, can relationship simple situations. lab, show t-test simple linear regression can equivalent. equivalence also exists complicated designs, throughout semester show ANOVAs Multiple Linear Regression different expressions analysis.entire rest semester focuses expansions simplest experimental design (one independent variable two levels, one dependent variable). example, one way expand design increase number levels single independent variable. see designs multiple levels can analyzed ANOVA (Analysis Variance) Linear Regression, produce results.Another way expand simple design increase number manipulations, independent variables. Designs multiple IVs, multiple levels, can analyzed ANOVA Multiple Linear Regression, produce results.","code":""},{"path":"multiple-regression-i.html","id":"concept-i-t-tests-and-simple-regression","chapter":"14 Multiple Regression I","heading":"14.3 Concept I: t-tests and simple regression","text":"Throughout course mostly focusing extensions t-test form ANOVA (Analysis Variance). extensions allow analyses complicated designs multiple independent variables multiple levels. However, discuss ANOVA, spend lab next one discussing multiple linear regression. linear regression can also extended handle complicated designs. extension termed multiple linear regression. Last, ANOVA multiple linear regression fundamentally analysis, produce results.relationship holds true simple designs . , head multiple linear regression, let’s quickly revisit simple linear regression see can t-test.","code":""},{"path":"multiple-regression-i.html","id":"independent-sample-t-test-for-two-groups","chapter":"14 Multiple Regression I","heading":"14.3.1 Independent sample t-test for two groups","text":"Let’s quickly make sample data experiment two groups analyzed t-test.’s table data (arranged long-format):’s bar graph showing means group, data-points subject overlaid top bars.’s t-test results:","code":"\nlibrary(tibble)\nsimple_design <- tibble(group = rep(c(0,1), each=10),\n                        DV = c(1,3,2,4,3,4,5,6,5,4,5,4,3,2,3,4,5,6,8,9))\nknitr::kable(simple_design)\nlibrary(ggplot2)\n\nggplot(simple_design, aes(x=group, y=DV))+\n  geom_bar(stat = \"summary\", fun = \"mean\", position=\"dodge\")+\n  geom_point()\nt.test(DV~group, var.equal=TRUE, data=simple_design)\n#> \n#>  Two Sample t-test\n#> \n#> data:  DV by group\n#> t = -1.412, df = 18, p-value = 0.175\n#> alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.9854404  0.5854404\n#> sample estimates:\n#> mean in group 0 mean in group 1 \n#>             3.7             4.9"},{"path":"multiple-regression-i.html","id":"linear-regression-for-two-groups","chapter":"14 Multiple Regression I","heading":"14.3.2 Linear regression for two groups","text":"common apply linear regression situation involving two continuous variables. However, can also apply simple linear regression situation. example, predictor variable ‘group’, ranges 0 1. dependent variable ‘DV’.can add regression line graph. Notice regression line goes across boundaries 0 1 (even though data 0 1). Also, notice regression line crosses exactly mean group. regression line line goes means two levels.Next, conduct linear regression data.Notice intercept 3.7. value DV line group = 0, also mean value DV group 0. Notice coefficient group 1.2. represents idea line goes 1, value DV go 1.2. , go group = 0, group =1, go 3.7 3.7+1.2 = 4.9. Notice, 4.9 mean group 1.Last, can look summary linear regression. Notice p-value .175, t-test . Notice t-value , however square root F-value 1.994 1.412, t-value t-test.","code":"\nggplot(simple_design, aes(x=group, y=DV))+\n  geom_bar(stat = \"summary\", fun = \"mean\", position=\"dodge\")+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=FALSE)\nlm(DV~group, data=simple_design)\n#> \n#> Call:\n#> lm(formula = DV ~ group, data = simple_design)\n#> \n#> Coefficients:\n#> (Intercept)        group  \n#>         3.7          1.2\nsummary(lm(DV~group, data=simple_design))\n#> \n#> Call:\n#> lm(formula = DV ~ group, data = simple_design)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -2.90  -1.10   0.10   1.15   4.10 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   3.7000     0.6009   6.157 8.18e-06 ***\n#> group         1.2000     0.8498   1.412    0.175    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.9 on 18 degrees of freedom\n#> Multiple R-squared:  0.09972,    Adjusted R-squared:  0.04971 \n#> F-statistic: 1.994 on 1 and 18 DF,  p-value: 0.175"},{"path":"multiple-regression-i.html","id":"concept-ii-adding-more-levels-is-still-simple-linear-regression","chapter":"14 Multiple Regression I","heading":"14.4 Concept II: Adding more levels is still simple linear regression","text":"ended last lab assignment read Slamecka, N. J.60 research design paper used chapter 561 explain multiple orthogonal linear regression. working toward explaining example. full design two independent variables, three levels. take look , let’s consider issue extending number levels one independent variable.","code":""},{"path":"multiple-regression-i.html","id":"a-simplified-memory-experiment","chapter":"14 Multiple Regression I","heading":"14.4.1 A simplified memory experiment","text":"Let’s simplify Slamecka62 experiment moment. Read following sentence one time:“must postulate strictly semantic points vantage, confusions communication revolve inadequate stipulation meaning.”Now, imagine 5 minutes try recall entire sentence memory. original sentence 20 words. many think can correctly recall?Slamecka63, subjects given sentences 20 words , different amounts practice, termed Original Learning (OL). subjects got 2, 4, 8 trials practice sentence. Thus, first Independent variable OL (amount practice), three levels, 2, 4, 8.expectation manipulating amount practice change memory performance. Specifically, practice sentence, able remember . created quick sketch illustrate basic predictions.Notice prediction graph terribly different t-test graph previous example. Instead groups 0 1 x-axis, three levels practice, 2, 4, 8. ran experiment, might expect memory performance goes amount practice goes , put line data basic way see amount practice actually effect. practice effect, line flat. effect, line go like yellow line graph., let’s quickly reproduce situation R. ’ll imagine data 3 subjects practice condition. , make numbers reflect idea practice lead words recalled. Finally, fit regression line.tiny wrinkles note. look closely, regression line go exactly means level practice. Also, bar 8 trials practice far away bar 4 trials practice.happening levels practice variable treated continuous variable. independent variable treated like continuous predictor variable, regression ANOVA , regression line may go means level.also possible treat independent variable categorical variable. can redefining practice variable factor R. couple different consequences. First, bar 8 trials practice now located beside 4. now considered one category away, four steps away, 4. Also, ggplot2 show regression line even though asked draw one. , results linear regression ANOVA different (although ).","code":"\nrecall_design <- tibble(practice = rep(c(2,4,8), each=3),\n                        subjects = 1:9,\n                        recall = c(5,7,8,\n                                   8,10,12,\n                                   12,15,17))\nknitr::kable(recall_design)\n\nggplot(recall_design, aes(x=practice, y=recall))+\n  geom_bar(stat = \"summary\", fun = \"mean\", position=\"dodge\")+\n  geom_point()+\n  geom_smooth(method=\"lm\", formula = y~x, se=FALSE)\nsummary(lm(recall~practice, data=recall_design))\n#> \n#> Call:\n#> lm(formula = recall ~ practice, data = recall_design)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.8095 -1.5714  0.1905  1.0476  2.4286 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)   4.3333     1.3678   3.168  0.01575 * \n#> practice      1.3095     0.2585   5.066  0.00145 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.934 on 7 degrees of freedom\n#> Multiple R-squared:  0.7857, Adjusted R-squared:  0.7551 \n#> F-statistic: 25.67 on 1 and 7 DF,  p-value: 0.001453\nsummary(aov(recall~practice, data=recall_design))\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> practice     1  96.03   96.03   25.67 0.00145 **\n#> Residuals    7  26.19    3.74                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nrecall_design$practice <- as.factor(recall_design$practice)\n\nggplot(recall_design, aes(x=practice, y=recall))+\n  geom_bar(stat = \"summary\", fun = \"mean\", position=\"dodge\")+\n  geom_point()+\n  geom_smooth(method=\"lm\", formula = y~x, se=FALSE)\n\nsummary(lm(recall~practice, data=recall_design))\n#> \n#> Call:\n#> lm(formula = recall ~ practice, data = recall_design)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.6667 -1.6667  0.3333  1.3333  2.3333 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)    6.667      1.186   5.620  0.00136 **\n#> practice4      3.333      1.678   1.987  0.09413 . \n#> practice8      8.000      1.678   4.768  0.00310 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.055 on 6 degrees of freedom\n#> Multiple R-squared:  0.7927, Adjusted R-squared:  0.7236 \n#> F-statistic: 11.47 on 2 and 6 DF,  p-value: 0.008905\nsummary(aov(recall~practice, data=recall_design))\n#>             Df Sum Sq Mean Sq F value Pr(>F)   \n#> practice     2  96.89   48.44   11.47 0.0089 **\n#> Residuals    6  25.33    4.22                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"multiple-regression-i.html","id":"concept-iii-multiple-orthogonal-linear-regression","chapter":"14 Multiple Regression I","heading":"14.5 Concept III: Multiple orthogonal linear regression","text":"Slamecka64 manipulated second independent variable addition practice. second variable called interpolated learning (IL), also three levels (0,4,8). ’s happened. subjects practiced learning sentence learn sentences. additional sentences likely distracting effect.well remember sentence…“must postulate strictly semantic points vantage, confusions communication revolve inadequate stipulation meaning.”’s (0 distraction sentences afterwards). well remember sentence practice new sentence four times:“Communicators can exercise latitude specifying meaning however choose, provided definitions correspond somewhat closely customary usage.”practice new sentence 8 times? expectation memory first sentence get worse practiced additional sentences probably distract first one.design 3x3 design. first independent variable amount practice first sentence (2,4,8 trials original learning). second independent variable ’ll call amount distraction sentences (interpolated learning: 0,4,8). design fully crossed, means level first independent variable paired level second. 3x3 = 9 total cells design. general expectations data:Notice 9 dots representing rough predictions memory recall 9 conditions (cells) design. practice variable indicated x-axis. distraction variable indicated different colors connected lines.Just like Concept section II, expect increasing practice increase number words recalled. can see prediction practice variable (IV) colored lines. move right, indicating practice sentence, expect remember words sentence.predictions second IV, distraction variable can see looking colored lines. 0 distraction, expect best performance overall, blue line representing 0 distraction top. 4 trials distraction, expect memory performance go , green line lower blue line. Finally, 8 trials distraction, expect memory performance go even . , yellow lines green line.trends referred “main effects”. expect main effect practice increase memory performance. expect main effect distraction decrease memory performance. expect things can happen time, total memory performance basically sum practice minus distraction.follow along textbook, can visualize predictions performance design terms 3-D space. One dimension represents DV (less memory recall), two dimensions represent IVs (less practice, less distraction). one IV, look best fit line relating IV DV. Now, two IVs add dimension line. two-dimensional line plane. , two IVs, can find best fit plane runs means cell design.Multiple linear regression can extended many IVs like. However, becomes difficult visualize extensions. can go line plane, ’s harder visualize happens add third IV mix. require four-dimensional graph hard draw. , extend plane just like , best fit cube. keep adding IVs, need geometrical dimensions can’t visualize. keep things “simple”, situations say fitting “hyperplane” data.Summary: main purpose preceding section illustrate multiple linear regression geometric interpretation, geometric interpretation can extended simple 2-D space involving one IV one DV, 3-D space involving 2 IVs one DV, beyond.","code":""},{"path":"multiple-regression-i.html","id":"practical-i-simulating-slamecka-1960","chapter":"14 Multiple Regression I","heading":"14.6 Practical I: Simulating Slamecka (1960)","text":"purpose section conduct Slamecka65 example textbook R. Note, original design IL (number interpolated lists) three levels 0, 4, 8. textbook, 0 2. align textbook, use 2 .","code":""},{"path":"multiple-regression-i.html","id":"representing-the-data-in-long-form","chapter":"14 Multiple Regression I","heading":"14.6.1 Representing the data in long-form","text":"","code":"\nslamecka_design <- tibble(number_of_learning_trials = rep(c(2,4,8), each=6),\n                          number_of_IL = rep(rep(c(2,4,8), 2), 3),\n                          subjects = 1:18,\n                          recall = c(35,21,6,\n                                   39,31,8,\n                                   40,34,18,\n                                   52,42,26,\n                                   61,58,46,\n                                   73,66,52\n                                   )\n                          )\n\nknitr::kable(slamecka_design)"},{"path":"multiple-regression-i.html","id":"visualizing-the-data","chapter":"14 Multiple Regression I","heading":"14.6.2 Visualizing the data","text":"","code":"\nggplot(slamecka_design,aes(x=number_of_IL,\n                           group = number_of_learning_trials,\n                           y=recall))+\n  geom_line(stat = \"summary\", fun = \"mean\")+\n  geom_point(stat = \"summary\", fun = \"mean\",)+\n  theme_classic()"},{"path":"multiple-regression-i.html","id":"running-the-multiple-regression-in-r","chapter":"14 Multiple Regression I","heading":"14.6.3 Running the multiple regression in R","text":"Note, main purpose example show correspondence fully worked example textbook, output R.","code":"\nlm(recall~ number_of_learning_trials + number_of_IL,data = slamecka_design)\n#> \n#> Call:\n#> lm(formula = recall ~ number_of_learning_trials + number_of_IL, \n#>     data = slamecka_design)\n#> \n#> Coefficients:\n#>               (Intercept)  number_of_learning_trials  \n#>                        30                          6  \n#>              number_of_IL  \n#>                        -4\n\nsummary(lm(recall~ number_of_learning_trials + number_of_IL,data = slamecka_design))\n#> \n#> Call:\n#> lm(formula = recall ~ number_of_learning_trials + number_of_IL, \n#>     data = slamecka_design)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>   -9.0   -4.0    0.5    4.0    6.0 \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                30.0000     3.3993   8.825 2.52e-07 ***\n#> number_of_learning_trials   6.0000     0.4818  12.453 2.60e-09 ***\n#> number_of_IL               -4.0000     0.4818  -8.302 5.44e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.099 on 15 degrees of freedom\n#> Multiple R-squared:  0.9372, Adjusted R-squared:  0.9289 \n#> F-statistic:   112 on 2 and 15 DF,  p-value: 9.609e-10"},{"path":"multiple-regression-i.html","id":"practical-ii-graphing-designs-with-multiple-ivs-and-levels","chapter":"14 Multiple Regression I","heading":"14.7 Practical II: Graphing designs with multiple IVs and levels","text":"section provides several example code snippets using ggplot2 graph means designs multiple IVs levels.Notice textbook version Slamecka results plotted number interpolated learning trials x-axis, whereas concept section III, plotted amount practice x-axis. ’s easy swap around, assigning different variables x group.","code":"\nggplot(slamecka_design,aes(x=number_of_IL,\n                           group = number_of_learning_trials,\n                           y=recall))+\n  geom_line(stat = \"summary\", fun = \"mean\")+\n  geom_point(stat = \"summary\", fun = \"mean\",)+\n  theme_classic()\nslamecka_design$number_of_IL <- as.factor(slamecka_design$number_of_IL)\n\nggplot(slamecka_design,aes(x= number_of_learning_trials,\n                           group = number_of_IL,\n                           y=recall))+\n  geom_line(stat = \"summary\", fun = \"mean\")+\n  geom_point(stat = \"summary\", fun = \"mean\", aes(shape=number_of_IL))+\n  theme_classic()"},{"path":"multiple-regression-i.html","id":"facet-wrapping","chapter":"14 Multiple Regression I","heading":"14.7.1 facet-wrapping","text":"ggplot2 useful function called facet_wrap() can used graph complicated designs different ways.Facets created level independent variable. , facets level distraction (IL) variable.Let’s imagine 6 subjects experiment, assigned conditions following manner:made change illustrate facet_wrap can create facets combine two variables, practice individual subjects:","code":"\nggplot(slamecka_design,aes(x= number_of_learning_trials,\n                           y=recall))+\n  geom_line(stat = \"summary\", fun = \"mean\")+\n  geom_point(stat = \"summary\", fun = \"mean\")+\n  theme_classic() +\n  facet_wrap(~number_of_IL)\nslamecka_design <- tibble(number_of_learning_trials = rep(c(2,4,8), each=6),\n                          number_of_IL = rep(rep(c(2,4,8), 2), 3),\n                          subjects = rep(1:6, each=3),\n                          recall = c(35,21,6,\n                                   39,31,8,\n                                   40,34,18,\n                                   52,42,26,\n                                   61,58,46,\n                                   73,66,52\n                                   )\n                          )\n\nknitr::kable(slamecka_design)\nggplot(slamecka_design,aes(x= number_of_IL,\n                           y=recall))+\n  geom_line()+\n  geom_point()+\n  theme_classic() +\n  facet_wrap(~number_of_learning_trials*subjects, ncol=2)"},{"path":"multiple-regression-i.html","id":"lab-2-generalization-assignment-1","chapter":"14 Multiple Regression I","heading":"14.8 Lab 2 Generalization Assignment","text":"","code":""},{"path":"multiple-regression-i.html","id":"instructions-13","chapter":"14 Multiple Regression I","heading":"14.8.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab2.Rmd”Use Lab2.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 2 blackboard.","code":""},{"path":"multiple-regression-i.html","id":"problems-12","chapter":"14 Multiple Regression I","heading":"14.8.2 Problems","text":"(3 points) following code roughly reproduces figure 5.5 textbook.66 Modify ggplot code graph looks close possible figure textbook:change x-axis titlechange y-axis titlemake x-axis y axis ticks sameinclude different symbols differentiate lines(3 points) Slamecka design 3x3 design. practice variable three levels (2, 4, 8), assumed increase memory recall increasing amount practice. interpolated learning (IL distraction) variable three levels (0, 4, 8), assumed decrease memory recall increasing amounts distraction.Imagine Slamecka included third independent variable also three levels. new IV expected increase memory recall. example, maybe third IV amount reward given subjects (0, 50 dollars, 1 million dollars) completing study. amount expected reward increased, subjects motivated remember words.Use R create dataframe represent predictions new 3x3x3 design. use ggplot facet wrapping show predicted means condition. hint, provided drawing expect ggplot graph look something like drawing.","code":"\nslamecka_design <- tibble(number_of_learning_trials = rep(c(2,4,8), each=6),\n                          number_of_IL = rep(rep(c(2,4,8), 2), 3),\n                          subjects = 1:18,\n                          recall = c(35,21,6,\n                                   39,31,8,\n                                   40,34,18,\n                                   52,42,26,\n                                   61,58,46,\n                                   73,66,52\n                                   )\n                          )\n\nggplot(slamecka_design,aes(x=number_of_IL,\n                           group = number_of_learning_trials,\n                           y=recall))+\n  geom_line(stat = \"summary\", fun = \"mean\")+\n  geom_point(stat = \"summary\", fun = \"mean\",)+\n  theme_classic()"},{"path":"multiple-regression-i.html","id":"references-2","chapter":"14 Multiple Regression I","heading":"14.9 References","text":"","code":""},{"path":"multiple-regression-ii.html","id":"multiple-regression-ii","chapter":"15 Multiple Regression II”","heading":"15 Multiple Regression II”","text":"","code":""},{"path":"multiple-regression-ii.html","id":"readings-4","chapter":"15 Multiple Regression II”","heading":"15.1 Readings","text":"Chapters 6 semi-partial correlation Abdi, Edelman, Dowling, & Valentin67.","code":""},{"path":"multiple-regression-ii.html","id":"overview-16","chapter":"15 Multiple Regression II”","heading":"15.2 Overview","text":"overarching goal semester discuss experimental research designs statistical tools combined together test causal hypotheses psychological phenomena. part, use ANOVA statistical tool experimental research designs multiple IVs. Currently, discussing regression en route discussing ANOVA. One reason ANOVA regression fundamentally analyses, important recognize equivalence. focus today’s lab concept semi-partial correlation context non-orthogonal multiple regression. statistical tool often used non-experimental research, useful discuss uses limitations prepare ANOVA next week.","code":""},{"path":"multiple-regression-ii.html","id":"a-note-on-explanation","chapter":"15 Multiple Regression II”","heading":"15.3 A note on Explanation","text":"Briefly, using word explanation many times throughout lab next ones. part talking explanation highly restricted statistical sense. example, \\(R^2\\), termed co-efficient determination, often described quantity explanation; specifically, much variation one variable explains variation another. \\(F\\) ratio “explained” variance divided “unexplained” variance. mainly deal linear models, “explanation” always refers geometrical account data, wherein data points described one part falls line (explained part), leftover part (error, unexplained part). Although use word explanation, term imply anything theoretical causal explanation.","code":""},{"path":"multiple-regression-ii.html","id":"concept-i-explaining-variance-with-multiple-variables","chapter":"15 Multiple Regression II”","heading":"15.4 Concept I: Explaining variance with multiple variables","text":"multiple linear regression one dependent variable multiple predictor variables. variation dependent variable “explained” terms combinations linear relationships variables.One issue multiple linear regression adding predictor variables generally increases amount variation explained. quickly illustrate :First, create matrix random values normal distribution. 26 variables, labeled z. pick DV, use random vectors predictor variables explain variation .First, everything random, variables “ortho-normal expectation”. , expect variables correlated . Let’s find true:general, distribution correlations centered 0. , “correlations” caused chance (except identities, must one…correlated 1, ).Now, let’s try predict values random vectors b z, see happens… notice total \\(R^2\\) keeps increasing.example uses + add new variables linear regression formula. adds variable predictor add potential interactions variables. discussed concept statistical interaction yet course, coming lectures. Nevertheless, consider happens \\(R^2\\) * used formula.* adds interactions variables, effectively increases number random predictor variables regression. can see, 5 predictor variables, include potential interactions, grows many linearly independent variables (15 case):predictor variables rows data DV, overfit data ways explaining data lines data points. , 6 predictor variables, interactions, multiple \\(R^2\\) value goes 1.Consider explained . DV generated random, nothing explain first place. predictor variables also generated random, shouldn’t explain variance. Yet, use enough random vectors “explain” variation another random vector, can perfectly, explaining 100% variance. sense explained everything statistically, without explained anything . numeric realities always play analyses real data.","code":"\n\nrandom_vectors <- matrix(rnorm(20*26,0,1), nrow=20, ncol=26)\ncolnames(random_vectors) <- letters\nrandom_vectors <- as.data.frame(random_vectors)\nhist(cor(random_vectors))\n\nsummary(lm(a~b,data=random_vectors))\n#> \n#> Call:\n#> lm(formula = a ~ b, data = random_vectors)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.2576 -0.6878  0.0128  0.5432  1.4502 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -0.05156    0.17567  -0.293    0.773\n#> b            0.09756    0.17086   0.571    0.575\n#> \n#> Residual standard error: 0.7838 on 18 degrees of freedom\n#> Multiple R-squared:  0.01779,    Adjusted R-squared:  -0.03678 \n#> F-statistic: 0.326 on 1 and 18 DF,  p-value: 0.5751\n\nsummary(lm(a~b,data=random_vectors))$r.squared\n#> [1] 0.01778961\nsummary(lm(a~b+c,data=random_vectors))$r.squared\n#> [1] 0.02377437\nsummary(lm(a~b+c+d,data=random_vectors))$r.squared\n#> [1] 0.1699112\nsummary(lm(a~b+c+d+e,data=random_vectors))$r.squared\n#> [1] 0.2029563\nsummary(lm(a~b+c+d+e+f,data=random_vectors))$r.squared\n#> [1] 0.3896615\nsummary(lm(a~b+c+d+e+f+g,data=random_vectors))$r.squared\n#> [1] 0.6958678\nsummary(lm(a~b+c+d+e+f+g+h,data=random_vectors))$r.squared\n#> [1] 0.6963296\nsummary(lm(a~b,data=random_vectors))$r.squared\n#> [1] 0.01778961\nsummary(lm(a~b*c,data=random_vectors))$r.squared\n#> [1] 0.3995137\nsummary(lm(a~b*c*d,data=random_vectors))$r.squared\n#> [1] 0.5424997\nsummary(lm(a~b*c*d*e,data=random_vectors))$r.squared\n#> [1] 0.8313199\nsummary(lm(a~b*c*d*e*f,data=random_vectors))$r.squared\n#> [1] 1\nsummary(lm(a~b*c*d*e,data=random_vectors))\n#> \n#> Call:\n#> lm(formula = a ~ b * c * d * e, data = random_vectors)\n#> \n#> Residuals:\n#>         1         2         3         4         5         6         7         8 \n#>  0.652511 -0.024001 -0.098379  0.116578 -0.204449  0.004711 -0.020795 -0.168447 \n#>         9        10        11        12        13        14        15        16 \n#>  0.271648 -0.001472 -0.400620 -0.145215  0.300497  0.513969 -0.190503  0.045478 \n#>        17        18        19        20 \n#>  0.232799  0.221906 -0.640894 -0.465322 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -0.13734    0.34838  -0.394    0.714\n#> b            0.31257    0.42573   0.734    0.504\n#> c            0.08714    0.33773   0.258    0.809\n#> d            0.55964    0.43853   1.276    0.271\n#> e           -0.66598    0.74829  -0.890    0.424\n#> b:c         -0.27126    0.43110  -0.629    0.563\n#> b:d         -0.46982    0.56138  -0.837    0.450\n#> c:d          0.43685    0.40141   1.088    0.338\n#> b:e          1.80883    1.43068   1.264    0.275\n#> c:e          0.36233    0.29596   1.224    0.288\n#> d:e         -0.37405    0.41018  -0.912    0.413\n#> b:c:d       -0.22123    0.31566  -0.701    0.522\n#> b:c:e       -0.74718    0.56292  -1.327    0.255\n#> b:d:e        0.30411    0.62552   0.486    0.652\n#> c:d:e        0.86560    0.55083   1.571    0.191\n#> b:c:d:e      0.34905    0.44491   0.785    0.477\n#> \n#> Residual standard error: 0.689 on 4 degrees of freedom\n#> Multiple R-squared:  0.8313, Adjusted R-squared:  0.1988 \n#> F-statistic: 1.314 on 15 and 4 DF,  p-value: 0.4333\nsummary(lm(a~b*c*d*e*f,data=random_vectors))\n#> \n#> Call:\n#> lm(formula = a ~ b * c * d * e * f, data = random_vectors)\n#> \n#> Residuals:\n#> ALL 20 residuals are 0: no residual degrees of freedom!\n#> \n#> Coefficients: (12 not defined because of singularities)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -0.15626        NaN     NaN      NaN\n#> b            0.53401        NaN     NaN      NaN\n#> c           -0.17132        NaN     NaN      NaN\n#> d            1.25351        NaN     NaN      NaN\n#> e           -0.88703        NaN     NaN      NaN\n#> f           -0.73748        NaN     NaN      NaN\n#> b:c         -0.19513        NaN     NaN      NaN\n#> b:d         -1.18972        NaN     NaN      NaN\n#> c:d          0.94783        NaN     NaN      NaN\n#> b:e          1.99134        NaN     NaN      NaN\n#> c:e          0.06818        NaN     NaN      NaN\n#> d:e         -0.27148        NaN     NaN      NaN\n#> b:f          1.47168        NaN     NaN      NaN\n#> c:f         -0.50005        NaN     NaN      NaN\n#> d:f          0.56888        NaN     NaN      NaN\n#> e:f         -0.90304        NaN     NaN      NaN\n#> b:c:d       -0.10050        NaN     NaN      NaN\n#> b:c:e       -1.27961        NaN     NaN      NaN\n#> b:d:e       -0.48266        NaN     NaN      NaN\n#> c:d:e        1.56388        NaN     NaN      NaN\n#> b:c:f             NA         NA      NA       NA\n#> b:d:f             NA         NA      NA       NA\n#> c:d:f             NA         NA      NA       NA\n#> b:e:f             NA         NA      NA       NA\n#> c:e:f             NA         NA      NA       NA\n#> d:e:f             NA         NA      NA       NA\n#> b:c:d:e           NA         NA      NA       NA\n#> b:c:d:f           NA         NA      NA       NA\n#> b:c:e:f           NA         NA      NA       NA\n#> b:d:e:f           NA         NA      NA       NA\n#> c:d:e:f           NA         NA      NA       NA\n#> b:c:d:e:f         NA         NA      NA       NA\n#> \n#> Residual standard error: NaN on 0 degrees of freedom\n#> Multiple R-squared:      1,  Adjusted R-squared:    NaN \n#> F-statistic:   NaN on 19 and 0 DF,  p-value: NA"},{"path":"multiple-regression-ii.html","id":"review-concept-slamecka-and-orthogonality","chapter":"15 Multiple Regression II”","heading":"15.5 Review Concept: Slamecka and orthogonality","text":"last lab discussed orthogonal multiple regression. defined orthogonal geometric concept, whereby one dimension orthogonal another perpendicular, connected 90 degree angle. Whenever occurs, possible move along one dimension without also moving along dimensions (e.g., can go back forth X-axis without going anywhere Y-axis, orthogonal dimensions 90 degree relationship). Another way saying variation along one dimension influence variation along another. words, dimensions orthogonal un-correlated .also described Slamecka68 design orthogonal design. chapter 6, see example non-orthogonal multiple regression, research design involved non-orthogonal independent variables. Specifically, IVs correlated , orthogonalBefore discuss design, worth clarifying Slamecka design orthogonal. example, exactly case IVs design uncorrelated ?Remember Slamecka design two IVs, three levels . Original learning 3 levels (2, 4, & 8); interpolated learning 3 levels (0, 4, 8). first glance might seem two IVs highly correlated:reason Slamecka design orthogonal design experimental design, specifically subjects assigned different levels independent variables. Depending subjects assigned, IVs can correlated (confounded) uncorrelated.lab assignment week 1 involved writing table indicating subjects assigned condition. reprinted :Now, Subjects, OL, IL variables orthogonal, must uncorrelated . can check using cor() function, return matrix correlations.Finally, consider moment confounded design look like. example:set confounded design individual subjects assigned basically levels OL IL. can see, now OL IL variables perfectly correlated (orthogonal). confounded design either causes change DV, won’t clear one caused change.Research designs like Slamecka’s69 employ experimental manipulations orthogonal variables , principle, capable making causal claims. Specifically, results showing DV (like recall) clearly changed function levels IVs support causal claim something IVs caused change. conclude manipulation cause change (reject null hypothesis chance causing change), saying variation dependent variable can “explained” terms changes IV.connect back earlier venting explanation, even case statistical explanation thin kind explanation. example, consider researcher idea magnetism. find table bunch metal filings. move rock toward away table, metal filings don’t move. rock isn’t magnetic, doesn’t anything position metal filings. researcher accidentally grabs magnet moves toward table. notice metal filings move magnet moves closer table. appears like magic. researcher systematically measure metal filing displacement function magnet distance, produces clear results IV (magnet distance) appears cause metal filings move. publish paper titled effect weird rock moving metal filings table.made example highlight two levels explanation, statistical theoretical. statistical level position magnet “explains” variance displacement metal filings. However, level explanation says nothing cause works, explanation magnetism. statistical level explanation produces phenomena effect (effect magnets moving metal filings) still needs explained (e.g., theory magnetism). , although take manipulations terms whether explain measurements, theoretical level manipulations never explain anything. Instead produce phenomena require theoretical (account causes work produce phenomena) statistical explanation.","code":"\nOL <- c(2,4,8)\nIL <- c(0,4,8)\n\ncor(OL,IL)\n#> [1] 0.9819805\nlibrary(tibble)\n\nslamecka_design <- tribble(\n  ~Subjects, ~OL, ~IL,\n  #--|--|----\n  1, 2, 0,\n  1, 4, 4,\n  1, 8, 8,\n  2, 4, 0,\n  2, 8, 4,\n  2, 2, 8,\n  3, 8, 0,\n  3, 2, 4,\n  3, 4, 8,\n  4, 2, 4,\n  4, 4, 0,\n  4, 8, 8,\n  5, 4, 4,\n  5, 2, 8,\n  5, 8, 0,\n  6, 8, 4,\n  6, 4, 8,\n  6, 2, 0,\n  7, 2, 8,\n  7, 4, 0,\n  7, 8, 4,\n  8, 4, 8,\n  8, 2, 4,\n  8, 8, 0,\n  9, 8, 8,\n  9, 4, 4,\n  9, 2, 0\n)\ncor(slamecka_design)\n#>          Subjects OL IL\n#> Subjects        1  0  0\n#> OL              0  1  0\n#> IL              0  0  1\nslamecka_confounded <- tribble(\n  ~Subjects, ~OL, ~IL,\n  #--|--|----\n  1, 2, 0,\n  1, 4, 4,\n  1, 8, 8,\n  2, 4, 4,\n  2, 8, 8,\n  2, 2, 0,\n  3, 8, 8,\n  3, 2, 0,\n  3, 4, 4,\n  4, 2, 0,\n  4, 4, 4,\n  4, 8, 8,\n  5, 4, 4,\n  5, 2, 0,\n  5, 8, 8,\n  6, 8, 8,\n  6, 4, 4,\n  6, 2, 0,\n  7, 2, 0,\n  7, 4, 4,\n  7, 8, 8,\n  8, 4, 4,\n  8, 2, 0,\n  8, 8, 8,\n  9, 8, 8,\n  9, 4, 4,\n  9, 2, 0\n)\ncor(slamecka_confounded)\n#>          Subjects        OL        IL\n#> Subjects        1 0.0000000 0.0000000\n#> OL              0 1.0000000 0.9819805\n#> IL              0 0.9819805 1.0000000"},{"path":"multiple-regression-ii.html","id":"concept-ii-semi-partial-correlation","chapter":"15 Multiple Regression II”","heading":"15.6 Concept II: Semi-partial Correlation","text":"tried briefly develop idea explanation hard, even using best class tools like orthogonal experimental designs. example, even really powerful manipulation like magnet can move metal, just running experiment doesn’t explain magnetism works, part requires theory magnetism.non-experimental designs prospect explanation even remote opinion extremely difficult. example, correlational research, typical strategy take many measurements see correlates . strategy can find patterns data, saw first concept section possible “explain” variance completely random vectors .Chapter 6, Abdi, Edelman, Dowling, & Valentin70 contrasts orthogonal multiple regression suitable experimental designs non-orthogonal multiple regression typical correlation research. cases research design multiple variables “explain” variance dependent measure. measures human behavior complex multiply determined, natural consider idea different variables may may causal influence measure, cause change, variable may cause different amounts change. result, multiple variables, researchers might interested figuring , variable, much causes change measure.case experimental research orthogonal designs, determination action causal variables done independently manipulating variable interest, measuring whether manipulation effect measure. case non-orthogonal correlational research possible make causal claims. However, possible add variables interest regression, raise many questions possible causes. Let’s consider issues looking textbook example.","code":""},{"path":"multiple-regression-ii.html","id":"age-speech-rate-and-memory","chapter":"15 Multiple Regression II”","heading":"15.6.1 Age, speech rate, and memory","text":"Abdi, Edelman, Dowling, & Valentin71 discusses research design Hulme, C., Thomson, N., Muir, C., & Lawrence, .72 interested whether variables age speech rate involve memory ability. basic claim older children can talk faster better memory abilities compared younger children talk slower, perhaps faster speech rate help one rehearse information.possible randomly assign people different ages different speech rate abilities, kind research can employ experimental design orthogonal variables. result, “Independent Variables” Age (X) speech rate (T) independent , confounded correlated one another. example, children age also talk faster, age variable positively correlated talking faster.Example data textbook :can run multiple linear regression ask question, much Age (X) Speech Rate (T) explain variance memory ability (Y). fake date, see \\(R^2\\) nearly one, taken togetehr, X T seem ‘explain’ almost variation Y.can also look correlations \\(R^2\\) values variables.see X explains .64 variation Y, T explains .97 variation Y. Notice adds .97+.64 = 1.61, greater 1. words explaining 100% variation, nonsensical. happen?assumption X T correlated , can see correlation matrix true, correlation \\(r=.75\\). result, consider idea three parts puzzle. unique potential influence X, unique potential influence T, shared part X T. words, part age speech rate variables measuring underlying thing (call shared part), also measuring unique things.\\(R^2\\) X Y .64, includes unique part X shared part T. \\(R^2\\) T Y .97 includes unique part T, shared part X. add two together, add shared part twice, causes total \\(R^2\\) greater 1. Using technique semi-partial correlation, possible re-express two confounded variables three sources variation, unique parts , shared part.Semi-partial correlation involves process “de-correlation”, “taking line data”. Let’s take closer look de-correlation process using semi-partial correlation.","code":"\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata <- tibble(X = c(4,4,7,7,10,10),\n               T = c(1,2,2,4,3,6),\n               Y = c(14,23,30,50,39,67))\n# Predict Y as a function of X and T\n(overall_model <- summary(lm(Y~X+T, data=data)))\n#> \n#> Call:\n#> lm(formula = Y ~ X + T, data = data)\n#> \n#> Residuals:\n#>      1      2      3      4      5      6 \n#> -1.167 -1.667  2.333  3.333 -1.167 -1.667 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)    1.667      3.598   0.463  0.67470   \n#> X              1.000      0.725   1.379  0.26162   \n#> T              9.500      1.087   8.736  0.00316 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.877 on 3 degrees of freedom\n#> Multiple R-squared:  0.9866, Adjusted R-squared:  0.9776 \n#> F-statistic: 110.1 on 2 and 3 DF,  p-value: 0.001559\ncor(data)\n#>           X         T         Y\n#> X 1.0000000 0.7500000 0.8027961\n#> T 0.7500000 1.0000000 0.9889517\n#> Y 0.8027961 0.9889517 1.0000000\ncor(data)^2\n#>           X         T         Y\n#> X 1.0000000 0.5625000 0.6444815\n#> T 0.5625000 1.0000000 0.9780254\n#> Y 0.6444815 0.9780254 1.0000000"},{"path":"multiple-regression-ii.html","id":"the-residuals-are-de-correlated","chapter":"15 Multiple Regression II”","heading":"15.6.2 The residuals are de-correlated","text":"Consider much X (Age) explains variation Y (memory ability). conduct regression, look two components Y data expressed terms X. parts Y fall regression line (predicted parts), parts Y line (residual error).Now, data table contains three versions Y, original Y variable decomposed predicted part (regression line part), residual part. Let’s plot run regression line .Notice residuals flat line. process decomposing Y one part regression line, residual part reflecting deviation error line, process de-correlation. Specifically, residual part X part correlate Y. words, residuals pattern looks like correlation subtracted . subtract linear correlation, leftover part definition correlation left, residuals forced statistically independent dependent measure.can show thing using predictor variable T. predict Y T, can plot residuals show correlation removed.","code":"\n# Predict Y as a function of X\n\nlm.x <- lm(Y~X, data=data)\n\ndata <- data %>%\n  mutate(X_residuals = residuals(lm.x),\n         X_predicted_Y = predict(lm.x))\n\nknitr::kable(data)\n\nA <- ggplot(data, aes(y=Y, x=X))+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=FALSE)\n\nB <- ggplot(data, aes(y=X_predicted_Y, x=X))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nC <- ggplot(data, aes(y=X_residuals, x=X))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nlibrary(patchwork)\n\nA+B+C\n\n# Predict Y as a function of T\n\nlm.t <- lm(Y~T, data=data)\n\ndata <- data %>%\n  mutate(T_residuals = residuals(lm.t),\n         T_predicted_Y = predict(lm.t))\nD <- ggplot(data, aes(y=Y, x=T))+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=FALSE)\n\nE <- ggplot(data, aes(y=T_predicted_Y, x=T))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nF <- ggplot(data, aes(y=T_residuals, x=T))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\nD+E+F"},{"path":"multiple-regression-ii.html","id":"semi-partial-correlation","chapter":"15 Multiple Regression II”","heading":"15.6.3 Semi-partial correlation","text":"technique semi-partial correlation relies decorrelation process just reviewed. idea predictor variables X T unique variance, also shared variance. want separate three sources variance \\(X_{\\text{unique}}\\), \\(T_{\\text{unique}}\\), \\(XT_{\\text{shared}}\\).","code":""},{"path":"multiple-regression-ii.html","id":"finding-the-unique-part-of-x","chapter":"15 Multiple Regression II”","heading":"15.6.4 Finding the unique part of X","text":"part X unique X correlated T? Let’s consider X dependent variable, regress T onto predictor variable.Now, know residuals part X linearly correlated T.words, values \\(X_{\\text{unique}}\\) \\(XT_{\\text{shared}}\\) part removed. Remember, original question figure much X alone explained Y. can computing \\(R^2\\) vector residuals, Y dependent measure.","code":"\n## Semi-partial correlation\n# Part uniquely explained by X\nlm.xt <- lm(X~T, data=data)\nresiduals(lm.xt)\n#>      1      2      3      4      5      6 \n#> -0.750 -1.875  1.125 -1.125  3.000 -0.375\ncor(residuals(lm.xt),data$Y)^2\n#> [1] 0.008528111"},{"path":"multiple-regression-ii.html","id":"finding-the-unique-part-of-t","chapter":"15 Multiple Regression II”","heading":"15.6.5 Finding the unique part of T","text":"repeat process find \\(T_{\\text{unique}}\\), except now regress X predictor onto T dependent variable.","code":"\n# Part uniquely explained by T\nlm.tx <- lm(T~X, data=data)\nresiduals(lm.tx)\n#>    1    2    3    4    5    6 \n#> -0.5  0.5 -1.0  1.0 -1.5  1.5\ncor(residuals(lm.tx),data$Y)^2\n#> [1] 0.342072"},{"path":"multiple-regression-ii.html","id":"the-shared-part","chapter":"15 Multiple Regression II”","heading":"15.6.6 The shared part","text":"estimate much variance shared part X T explain can bit algebra. take total multiple \\(R^2\\) original model lm(Y~X+T), subtract \\(X_{\\text{unique}}\\) \\(T_{\\text{unique}}\\) total.","code":"\n# Part common to X and T\noverall_model$r.squared - cor(residuals(lm.xt),data$Y)^2 - cor(residuals(lm.tx),data$Y)^2\n#> [1] 0.6359534"},{"path":"multiple-regression-ii.html","id":"what-has-been-explained","chapter":"15 Multiple Regression II”","heading":"15.6.7 What has been explained?","text":"Semi-partial correlation useful demonstration idea partitioning sources variance unique shared portions. However, returned question explained, may questions first place.unique part Age (X) explained small part variation memory ability (.0085). Age variable clear measure first place–proxy variable many things change development. Speech rate specific age, explains less variance (.34) shared part age speech rate (.63). shared part actually referring ? knows? ’s latent construct sense related age speech rate. , design correlational, clear things cause memory ability causing latent construct change vice-versa. Explanation hard, non-orthogonal designs can easily raise questions answer. Partly reason, stick examining experimental designs remainder course.","code":""},{"path":"multiple-regression-ii.html","id":"practical-i-semi-partial-correlation-with-ppcor","chapter":"15 Multiple Regression II”","heading":"15.7 Practical I: semi-partial correlation with ppcor","text":"ppcor package function computing semi-partial correlations. can see spcor() function returns values textbook example.Note spcor() returns correlations (\\(r\\) values). Square produce coefficients determination (\\(R^2\\)).","code":"\nlibrary(ppcor)\n\ndata <- tibble(X = c(4,4,7,7,10,10),\n               T = c(1,2,2,4,3,6),\n               Y = c(14,23,30,50,39,67))\n\nspcor(data, method = \"pearson\")\n#> $estimate\n#>             X          T         Y\n#> X  1.00000000 -0.2963241 0.4120552\n#> T -0.07367089  1.0000000 0.6488088\n#> Y  0.09234777  0.5848692 1.0000000\n#> \n#> $p.value\n#>           X         T         Y\n#> X 0.0000000 0.6283051 0.4906046\n#> T 0.9062842 0.0000000 0.2362282\n#> Y 0.8825865 0.3002769 0.0000000\n#> \n#> $statistic\n#>            X          T         Y\n#> X  0.0000000 -0.5373837 0.7832889\n#> T -0.1279494  0.0000000 1.4767956\n#> Y  0.1606375  1.2489073 0.0000000\n#> \n#> $n\n#> [1] 6\n#> \n#> $gp\n#> [1] 1\n#> \n#> $method\n#> [1] \"pearson\"\nspcor(data, method = \"pearson\")$estimate^2\n#>             X          T         Y\n#> X 1.000000000 0.08780798 0.1697895\n#> T 0.005427400 1.00000000 0.4209528\n#> Y 0.008528111 0.34207202 1.0000000"},{"path":"multiple-regression-ii.html","id":"lab-3-generalization-assignment-1","chapter":"15 Multiple Regression II”","heading":"15.8 Lab 3 Generalization Assignment","text":"","code":""},{"path":"multiple-regression-ii.html","id":"instructions-14","chapter":"15 Multiple Regression II”","heading":"15.8.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab3.Rmd”Use Lab3.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.Submit github repository link Lab 3 blackboard.","code":""},{"path":"multiple-regression-ii.html","id":"problems-13","chapter":"15 Multiple Regression II”","heading":"15.8.2 Problems","text":"problem week develop little bit content (tutorial material) added lab. (6 points)mentioned lab, ran time little bit week, didn’t get chance provide lots examples semi-partial correlation R. also didn’t cover everything Chapter 6, example, didn’t talk difference partial semi-partial correlation. whole, assuming many ways tutorial content lab improved.purpose lab try create something principle added content lab. example, maybe think another way explain semi-partial correlation, concept orthogonality, like provide different example computing semi-partial correlation.fairly open lab assignment. task develop little tutorial example inserted lab. ’m going give guidelines long . probably shouldn’t long entire lab, probably shouldn’t short 1 line code one sentence. ’m thinking paragraph two explanation, code snippet two illustrate example. Imagine trying explain concepts practical tips, see come .submit assignment please let know OK putting example course website lab. totally OK want . OK , add new section lab called “Student contributed examples”; , recognized contributing author lab. Note also, sharing lab creative commons license (CC SA 4.0), basically means people can copy, edit, re-use material wish, credit content creators . choose example displayed, content also license. can discuss license detail next week questions.Good luck!","code":""},{"path":"multiple-regression-ii.html","id":"references-3","chapter":"15 Multiple Regression II”","heading":"15.9 References","text":"","code":""},{"path":"anova.html","id":"anova","chapter":"16 ANOVA","heading":"16 ANOVA","text":"","code":""},{"path":"anova.html","id":"readings-5","chapter":"16 ANOVA","heading":"16.1 Readings","text":"Chapters 7 8 .73 can also check chapter ANOVA74 https://crumplab.github.io/statistics/anova.html.","code":""},{"path":"anova.html","id":"overview-17","chapter":"16 ANOVA","heading":"16.2 Overview","text":"next three weeks discuss one-factor Analysis Variance (ANOVA). can think ANOVA extension t-test idea, can applied designs independent variable (IV) two levels, even complicated designs involving multiple IVs (possibly many levels ).begin one-factor ANOVA designs single independent variable two levels. see shortly, use ANOVA design two levels, result t-test. However, unlike t-test, can also use ANOVA designs two levels.lab three major concept sections help firm ANOVA concepts, practical section using aov function compute ANOVAs R.Let’s consider pictures first, move onto lab:","code":""},{"path":"anova.html","id":"grand-mean","chapter":"16 ANOVA","heading":"16.2.1 Grand mean","text":"","code":""},{"path":"anova.html","id":"within-group-variation","chapter":"16 ANOVA","heading":"16.2.2 Within group variation","text":"","code":""},{"path":"anova.html","id":"between-group-variation","chapter":"16 ANOVA","heading":"16.2.3 Between group variation","text":"","code":""},{"path":"anova.html","id":"f-ratio","chapter":"16 ANOVA","heading":"16.2.4 F Ratio","text":"","code":""},{"path":"anova.html","id":"practical-1-by-hand-anova-using-r","chapter":"16 ANOVA","heading":"16.3 Practical 1: By hand ANOVA using R","text":"","code":"\nlibrary(tibble)\nromeo_juliet <- tibble(subjects = 1:20,\n                       Group = rep(c(\"No Context\",\n                                 \"Context Before\",\n                                 \"Context After\",\n                                 \"Partial Context\"), each = 5),\n                       Comprehension = c(3,3,2,4,3,\n                                         5,9,8,4,9,\n                                         2,4,5,4,1,\n                                         5,4,3,5,4\n                                   )\n                          )\n\nromeo_juliet$Group <- factor(romeo_juliet$Group,\n                             levels = c(\"No Context\",\n                                 \"Context Before\",\n                                 \"Context After\",\n                                 \"Partial Context\"))\n\nknitr::kable(romeo_juliet)"},{"path":"anova.html","id":"grand-mean-and-total-sums-of-squares","chapter":"16 ANOVA","heading":"16.3.1 Grand Mean and total sums of squares","text":"","code":"\nlibrary(dplyr)\n# get grand mean\ngrand_mean <- mean(romeo_juliet$Comprehension)\n\n# get squared deviations from grand mean\nSS_total_table <- romeo_juliet %>%\n  mutate(grand_mean = mean(romeo_juliet$Comprehension)) %>%\n  mutate(deviations = Comprehension - grand_mean,\n         sq_deviations = (Comprehension - grand_mean)^2)\n\n#sum them\nSS_total <- sum(SS_total_table$sq_deviations)"},{"path":"anova.html","id":"ss-between","chapter":"16 ANOVA","heading":"16.3.2 SS Between","text":"","code":"\n# get group means\ngroup_means <- romeo_juliet %>%\n  group_by(Group) %>%\n  summarize(mean_Comprehension = mean(Comprehension),.groups = 'drop')\n\n# get squared deviations between grand mean and group means\nSS_between_table <- romeo_juliet %>%\n  mutate(grand_mean = mean(romeo_juliet$Comprehension),\n         group_means = rep(group_means$mean_Comprehension, each = 5)) %>%\n  mutate(deviations = group_means - grand_mean,\n         sq_deviations = (group_means - grand_mean)^2)\n\nSS_between <- sum(SS_between_table$sq_deviations)"},{"path":"anova.html","id":"ss-within","chapter":"16 ANOVA","heading":"16.3.3 SS Within","text":"Check SS total = SS + SS Within:","code":"\n# get group means\ngroup_means <- romeo_juliet %>%\n  group_by(Group) %>%\n  summarize(mean_Comprehension = mean(Comprehension), .groups = 'drop')\n\n# get squared deviations between group means and original data points\nSS_within_table <- romeo_juliet %>%\n  mutate(group_means = rep(group_means$mean_Comprehension, each = 5)) %>%\n  mutate(deviations = group_means - Comprehension,\n         sq_deviations = (group_means - Comprehension)^2)\n\nSS_within <- sum(SS_within_table$sq_deviations)\nSS_total\n#> [1] 88.55\n\nSS_between+SS_within\n#> [1] 88.55\n\nSS_total == SS_between+SS_within\n#> [1] TRUE"},{"path":"anova.html","id":"f-is-a-ratio-of-variances","chapter":"16 ANOVA","heading":"16.3.4 F is a ratio of variances","text":"","code":"\n# Between groups variance\ndfb <- 4-1 \nMS_Between <- SS_between/dfb\n\n# Within groups variance\ndfw <- 20-4\nMS_Within <- SS_within/dfw\n\n# compute F, a ratio of variances\nF_ratio <- MS_Between/MS_Within"},{"path":"anova.html","id":"alternative-example-using-matrix","chapter":"16 ANOVA","heading":"16.3.5 Alternative example using matrix","text":"","code":"\n# represent data in a wide-format matrix\n\nmatrix_data <- matrix(c(3,3,2,4,3,\n                        5,9,8,4,9,\n                        2,4,5,4,1,\n                        5,4,3,5,4),\n                      ncol=4,\n                      nrow=5)\n\ncolnames(matrix_data) <- c(\"No Context\",\n                                 \"Context Before\",\n                                 \"Context After\",\n                                 \"Partial Context\")\n\n## Sums of Squares\n\nSS_total <- sum( (matrix_data - mean(matrix_data))^2 )\nSS_between <- sum( (colMeans(matrix_data) - mean(matrix_data))^2 )*5\nSS_within <- sum( (colMeans(matrix_data) - t(matrix_data))^2 )\n\n## Mean squared errors\n\ndfb <- 4-1 \nMS_Between <- SS_between/dfb\n\ndfw <- 20-4\nMS_Within <- SS_within/dfw\n\n# compute F, a ratio of variances\nF_ratio <- MS_Between/MS_Within"},{"path":"anova.html","id":"using-the-built-in-aov","chapter":"16 ANOVA","heading":"16.3.6 Using the built-in aov","text":"Note, go detail aov` practical section.","code":"\nromeo_juliet$Comprehension <- sample(romeo_juliet$Comprehension)\n\nanova.out <- aov(Comprehension ~ Group, data = romeo_juliet)\nsummary(anova.out)\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> Group        3  10.15   3.383    0.69  0.571\n#> Residuals   16  78.40   4.900"},{"path":"anova.html","id":"concept-1-understanding-the-pieces-of-the-anova-table","chapter":"16 ANOVA","heading":"16.4 Concept 1: Understanding the pieces of the ANOVA table","text":"pieces ANOVA table (degrees freedom, Sums Squares, Mean squared error, F-value) computed directly data. Algebraically, Mean squared errors sum squares divided respective degrees freedoms. , F-value ratio: Mean squared error group () divided mean squared error residuals (Within). p-value come data, case comes F-distribution. concept section followed section creating simulated F-distribution. prepare us concept, first focus pieces ANOVA table, examine piece behave context experimental designs. Specifically, ask term behave Null hypothesis true well isn’t.textbook “Romeo & Juliet” example, example data 4 groups, 5 observations per condition. empirical question whether group manipulation caused changes means. ANOVA applied data set large F-value found. F-value unlikely produced null-hypothesis, hypothesis rejected. continue basic example, examine slightly abstract perspective. ask terms ANOVA design behave Null true, Null true.","code":""},{"path":"anova.html","id":"ss-total-1","chapter":"16 ANOVA","heading":"16.4.1 SS Total","text":"null TRUE, individual subject means groups, assumed randomly sampled normal distribution. example, one random sample 20 subject means design. use unit normal distribution throughout.\\(\\text{SS}_\\text{Total}\\) sum squared deviations grand mean (mean data) data point. Assuming sampling unit normal distribution, \\(\\text{SS}_\\text{Total}\\) behave Null hypothesis true?can compute \\(\\text{SS}_\\text{Total}\\) simulated data ., just \\(\\text{SS}_\\text{Total}\\) happens look like one random sample 20 values normal distribution. null, every time sample 20 values expect slightly different values \\(\\text{SS}_\\text{Total}\\). Let’s generate sampling distribution \\(\\text{SS}_\\text{Total}\\). expect \\(\\text{SS}_\\text{Total}\\) take value distribution Null TRUE.happen \\(\\text{SS}_\\text{Total}\\) Null hypothesis false? infinity ways null hypothesis false, let’s simulate one see happen \\(\\text{SS}_\\text{Total}\\).Specifically, let’s imagine manipulation works cause first group shifted mean two standard deviations compared groups.distribution \\(\\text{SS}_\\text{Total}\\) includes smaller range values Null TRUE, compared distribution \\(\\text{SS}_\\text{Total}\\) one alternative scenario. big concept manipulation causes changes means, increase overall variance data relative grand mean. Thus, \\(\\text{SS}_\\text{Total}\\) tend larger null false, compared true.","code":"\nsim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\nSS_total <- sum( (mean(sim_data) - sim_data)^2 )\nSS_total\n#> [1] 18.78812\nSS_total_distribution <- c()\nfor(i in 1:1000){\nsim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\nSS_total <- sum( (mean(sim_data) - sim_data)^2 )\nSS_total_distribution[i] <- SS_total\n}\nhist(SS_total_distribution)\nmean(SS_total_distribution)\n#> [1] 19.10178\nSS_total_distribution_alt <- c()\nfor(i in 1:1000){\nsim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\nsim_data[,1] <- sim_data[,1]+2 #add effect for group 1\nSS_total <- sum( (mean(sim_data) - sim_data)^2 )\nSS_total_distribution_alt[i] <- SS_total\n}\nhist(SS_total_distribution_alt)\nlibrary(ggplot2)\nSS_total_data <- data.frame(SS_total = c(SS_total_distribution,\n                                         SS_total_distribution_alt),\n                            type = rep(c(\"Null\",\"Alternative\"), each=1000))\n\nggplot(SS_total_data, aes(x=SS_total, group=type, fill=type))+\n  geom_histogram(position=\"dodge\")"},{"path":"anova.html","id":"ss-between-1","chapter":"16 ANOVA","heading":"16.4.2 SS Between","text":"\\(\\text{SS}_\\text{}\\) treats score group mean, sums squared deviations group mean grand mean. According null, group means (average), variance random sampling. Thus, can simulate sampling distribution \\(\\text{SS}_\\text{}\\) expect null.Let’s also create distribution \\(\\text{SS}_\\text{}\\) happen first group two standard deviation shift mean.’s clear plotting distributions together \\(\\text{SS}_\\text{}\\) larger numbers variation means gets larger.","code":"\nSS_between_distribution <- c()\nfor(i in 1:1000){\n  sim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\n  SS_between <- sum( (mean(sim_data) - colMeans(sim_data))^2 )*5\n  SS_between_distribution[i] <- SS_between\n}\nSS_between_distribution_alt <- c()\nfor(i in 1:1000){\n  sim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\n  sim_data[,1] <- sim_data[,1]+2 #add effect for group 1\n  SS_between <- sum( (mean(sim_data) - colMeans(sim_data))^2 )*5\n  SS_between_distribution_alt[i] <- SS_between\n}\nSS_between_data <- data.frame(SS_between = c(SS_between_distribution,\n                                         SS_between_distribution_alt),\n                            type = rep(c(\"Null\",\"Alternative\"), each=1000))\n\nggplot(SS_between_data, aes(x=SS_between, group=type, fill=type))+\n  geom_histogram(position=\"dodge\")"},{"path":"anova.html","id":"ss-within-1","chapter":"16 ANOVA","heading":"16.4.3 SS Within","text":"\\(\\text{SS}_\\text{Within}\\) sums squared deviations score group mean. can simulate sampling distribution \\(\\text{SS}_\\text{Within}\\) expect null.Let’s also create distribution \\(\\text{SS}_\\text{Within}\\) happen first group two standard deviation shift mean.Remember, created alternative distribution violating null shift mean first group. assume manipulation effect variances. Thus, \\(\\text{SS}_\\text{Within}\\) null true false (mean shifts).","code":"\nSS_Within_distribution <- c()\nfor(i in 1:1000){\n  sim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\n  SS_Within <- sum( (colMeans(sim_data) - t(sim_data))^2 )\n  SS_Within_distribution[i] <- SS_Within\n}\nSS_Within_distribution_alt <- c()\nfor(i in 1:1000){\n  sim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\n  sim_data[,1] <- sim_data[,1]+2 #add effect for group 1\n  SS_Within <- sum( (colMeans(sim_data) - t(sim_data))^2)\n  SS_Within_distribution_alt[i] <- SS_Within\n}\nSS_Within_data <- data.frame(SS_Within = c(SS_Within_distribution,\n                                         SS_Within_distribution_alt),\n                            type = rep(c(\"Null\",\"Alternative\"), each=1000))\n\nggplot(SS_Within_data, aes(x=SS_Within, group=type, fill=type))+\n  geom_histogram(position=\"dodge\")"},{"path":"anova.html","id":"ms-between","chapter":"16 ANOVA","heading":"16.4.4 MS Between","text":"\\(\\text{MS}_\\text{}\\) variance, remember variances average sums squares. case, “estimating” variance due effect manipulation, divide degrees freedom \\(\\text{SS}_\\text{}/df_B\\).four group means, estimating respect grand mean, number means free vary 4-1 = 3.can plot distributions \\(\\text{MS}_\\text{}\\) null true, false (using alternative example). can re-use \\(\\text{SS}_\\text{}\\) distributions simply divide values \\(df_B\\).","code":"\nMS_between_data <- data.frame(MS_between = c(SS_between_distribution/3,\n                                         SS_between_distribution_alt/3),\n                            type = rep(c(\"Null\",\"Alternative\"), each=1000))\n\nggplot(MS_between_data, aes(x=MS_between, group=type, fill=type))+\n  geom_histogram(position=\"dodge\")"},{"path":"anova.html","id":"ms-within","chapter":"16 ANOVA","heading":"16.4.5 MS Within","text":"\\(\\text{MS}_\\text{Within}\\) represents average variation within group. ANOVA assumes manipulations cause shifts means, perspective, \\(\\text{MS}_\\text{Within}\\) represents variance due manipulation.“estimating” variance divide degrees freedom \\(\\text{SS}_\\text{Within}/df_W\\).20 subject means, estimating respect four group means, number means free vary 20-4 = 16.can plot distributions \\(\\text{MS}_\\text{Within}\\) expect null true, false (using alternative example). , can re-use \\(\\text{SS}_\\text{Within}\\) distributions simply divide values \\(df_W\\).","code":"\nMS_Within_data <- data.frame(MS_Within = c(SS_Within_distribution/16,\n                                         SS_Within_distribution_alt/16),\n                            type = rep(c(\"Null\",\"Alternative\"), each=1000))\n\nggplot(MS_Within_data, aes(x=MS_Within, group=type, fill=type))+\n  geom_histogram(position=\"dodge\")"},{"path":"anova.html","id":"f","chapter":"16 ANOVA","heading":"16.4.6 F","text":"last component ANOVA table \\(F\\), ratio variances \\(\\text{MS}_\\text{} / \\text{MS}_\\text{Within}\\).Let’s re-simulate F-distribution expect null., re-simulate F-distribution expect alternative (group one mean shift two standard deviations)., plot F-distributions.","code":"\nF_distribution <- c()\nfor(i in 1:1000){\n  sim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\n  SS_between <- sum( (mean(sim_data) - colMeans(sim_data))^2 )*5\n  SS_Within <- sum( (colMeans(sim_data) - t(sim_data))^2 )\n  sim_F <- (SS_between/3) / (SS_Within/16)\n  F_distribution[i] <- sim_F\n}\nF_distribution_alt <- c()\nfor(i in 1:1000){\n  sim_data <- matrix(rnorm(20,0,1),ncol=4,nrow=5)\n  sim_data[,1] <- sim_data[,1]+2 #add effect for group 1\n  SS_between <- sum( (mean(sim_data) - colMeans(sim_data))^2 )*5\n  SS_Within <- sum( (colMeans(sim_data) - t(sim_data))^2 )\n  sim_F <- (SS_between/3) / (SS_Within/16)\n  F_distribution_alt[i] <- sim_F\n}\nF_data <- data.frame(F = c(F_distribution,\n                           F_distribution_alt),\n                           type = rep(c(\"Null\",\"Alternative\"),\n                           each=1000))\n\nggplot(F_data, aes(x=F, group=type, fill=type))+\n  geom_histogram(position=\"dodge\")"},{"path":"anova.html","id":"concept-2-simulating-the-f-distribution","chapter":"16 ANOVA","heading":"16.5 Concept 2: Simulating the F-distribution","text":"p-value come ? seen aov function work computing degrees freedom, sums squares, mean squares F-value directly provided data frame. Remember, F-value essentially descriptive statistic.\nsummary function shows F-value associated fairly small p-value (.00278). aov function looks p-value using F-distribution function.However, F distribution come ? F distribution just another sampling distribution. hypothetical distribution F values obtained null hypothesis situation. can obtain F-distribution using monte-carlo simulation. Let’s see get clsoe.","code":"\n?pf\npf(7.227,3,16, lower.tail = FALSE)\n#> [1] 0.002782149\n# replace the example data with random numbers from a unit normal\nromeo_juliet$Comprehension <- rnorm(20,0,1)\naov.out <- aov(Comprehension ~ Group, data = romeo_juliet)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\n\n# do the above a bunch of times\nsave_F_values <- length(10000)\nfor(i in 1:10000){\nromeo_juliet$Comprehension <- rnorm(20,0,1)\naov.out <- aov(Comprehension ~ Group, data = romeo_juliet)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\nsave_F_values[i] <- simulated_F\n}\n\n# look at the F distribution\nhist(save_F_values)\n\n# get the p-value associated with 7.22\nlength(save_F_values[save_F_values>7.22])/length(save_F_values)\n#> [1] 0.003\n\n# do the above 10000 times\nsave_F_values <- length(10000)\nfor(i in 1:10000){\nromeo_juliet$Comprehension <- rnorm(20,0,1)\naov.out <- aov(Comprehension ~ Group, data = romeo_juliet)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\nsave_F_values[i] <- simulated_F\n}\n\n# look at the F distribution\nhist(save_F_values)\n\n# get the p-value associated with 7.22\nlength(save_F_values[save_F_values>7.22])/length(save_F_values)\n#> [1] 0.0022"},{"path":"anova.html","id":"practical-2-one-way-anova-with-aov-function","chapter":"16 ANOVA","heading":"16.6 Practical 2: One-way ANOVA with aov function","text":"use aov() function many times throughout remainder course perform various ANOVAs. brief practical section show bells whistles useful conducting ANOVAs R.continue use example data textbook:","code":"\nromeo_juliet <- tibble(subjects = 1:20,\n                       Group = rep(c(\"No Context\",\n                                 \"Context Before\",\n                                 \"Context After\",\n                                 \"Partial Context\"), each = 5),\n                       Comprehension = c(3,3,2,4,3,\n                                         5,9,8,4,9,\n                                         2,4,5,4,1,\n                                         5,4,3,5,4\n                                   )\n                          )\n\nromeo_juliet$Group <- factor(romeo_juliet$Group,\n                             levels = c(\"No Context\",\n                                 \"Context Before\",\n                                 \"Context After\",\n                                 \"Partial Context\"))"},{"path":"anova.html","id":"aov","chapter":"16 ANOVA","heading":"16.6.1 aov()","text":"aov() function two main inputs, formula (Comprehension ~ Group), data frame (data = romeo_juliet). dataframe must organized long-format.column name dependent variable (Comprehension) placed left side ~, column name factor (independent variable) placed right (Group). formula Comprehension ~ Group can read , “analyze means Comprehension function levels Group”.aov() function produces list object containing pieces ANOVA model. Printing console usually give information looking :","code":"\nanova.out <- aov(Comprehension ~ Group, data = romeo_juliet)\nanova.out\n#> Call:\n#>    aov(formula = Comprehension ~ Group, data = romeo_juliet)\n#> \n#> Terms:\n#>                 Group Residuals\n#> Sum of Squares  50.95     37.60\n#> Deg. of Freedom     3        16\n#> \n#> Residual standard error: 1.532971\n#> Estimated effects may be unbalanced"},{"path":"anova.html","id":"summary-1","chapter":"16 ANOVA","heading":"16.6.2 summary()","text":"ANOVA table can printed console using summary(). summary() function takes aov() model ’s input, returns ANOVA table output.also write :","code":"\nsummary(anova.out)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(aov(Comprehension ~ Group, data = romeo_juliet))\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova.html","id":"model.tables","chapter":"16 ANOVA","heading":"16.6.3 model.tables()","text":"Another helpful function model.table(), extracts means level analysis (aov() object), prints console.","code":"\nmodel.tables(anova.out)\n#> Tables of effects\n#> \n#>  Group \n#> Group\n#>      No Context  Context Before   Context After Partial Context \n#>           -1.35            2.65           -1.15           -0.15"},{"path":"anova.html","id":"extracting-model-terms","chapter":"16 ANOVA","heading":"16.6.4 Extracting model terms","text":"practice, might use aov(), summary(), model.tables() quickly print ANOVA look . example, let’s wanted run ANOVA example data look R, :Great, wanted access specific values ANOV table. can extract values finding correct indices list objects generated aov() summary().use strategy help insert specific values R markdown document.","code":"\nanova.out <- aov(Comprehension ~ Group, data = romeo_juliet)\nsummary(anova.out)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.tables(anova.out)\n#> Tables of effects\n#> \n#>  Group \n#> Group\n#>      No Context  Context Before   Context After Partial Context \n#>           -1.35            2.65           -1.15           -0.15\nmy_summary <- summary(anova.out)\nmy_summary[[1]]$Df\n#> [1]  3 16\nmy_summary[[1]]$`Sum Sq`\n#> [1] 50.95 37.60\nmy_summary[[1]]$`Mean Sq`\n#> [1] 16.98333  2.35000\nmy_summary[[1]]$`F value`\n#> [1] 7.22695      NA\nmy_summary[[1]]$`Pr(>F)`\n#> [1] 0.002782234          NA"},{"path":"anova.html","id":"reporting-with-papaja","chapter":"16 ANOVA","heading":"16.6.5 Reporting with papaja","text":"papaja package many useful functions allow extract pieces ANOVA want report document, results section.Check reporting options using apa_print(), inputting ANOVA object.give example, current APA style reporting F-test something like following, F(dfb, dfb) = F-value, MSE = MS_within, p = p-value, maybe partial-eta squared value. information available papaja::apa_print(anova.)$full_result$Group, insert inline chunk, get \\(F(3, 16) = 7.23\\), \\(p = .003\\), \\(\\hat{\\eta}^2_G = .575\\), 90% CI \\([.219, .729]\\).can also created tables formatted APA style. case print ANOVA table document slightly nicer way console print .\nTable 16.1: \n","code":"\nlibrary(papaja)\n\npapaja::apa_print(anova.out)\n#> $estimate\n#> $estimate$Group\n#> [1] \"$\\\\hat{\\\\eta}^2_G = .575$, 90\\\\% CI $[.219, .729]$\"\n#> \n#> \n#> $statistic\n#> $statistic$Group\n#> [1] \"$F(3, 16) = 7.23$, $p = .003$\"\n#> \n#> \n#> $full_result\n#> $full_result$Group\n#> [1] \"$F(3, 16) = 7.23$, $p = .003$, $\\\\hat{\\\\eta}^2_G = .575$, 90\\\\% CI $[.219, .729]$\"\n#> \n#> \n#> $table\n#> A data.frame with 7 labelled columns:\n#> \n#>    term estimate     conf.int statistic df df.residual p.value\n#> 1 Group     .575 [.219, .729]      7.23  3          16    .003\n#> \n#> term     : Effect \n#> estimate : $\\\\hat{\\\\eta}^2_G$ \n#> conf.int : 90\\\\% CI \n#> statistic: $F$ \n#> df       : $\\\\mathit{df}$ \n#> ... (2 more labels)\n#> attr(,\"class\")\n#> [1] \"apa_results\" \"list\"\n\npapaja::apa_print(anova.out)$full_result$Group\n#> [1] \"$F(3, 16) = 7.23$, $p = .003$, $\\\\hat{\\\\eta}^2_G = .575$, 90\\\\% CI $[.219, .729]$\"\npapaja::apa_table(apa_print(anova.out)$table)"},{"path":"anova.html","id":"lab-4-generalization-assignment-1","chapter":"16 ANOVA","heading":"16.7 Lab 4 Generalization Assignment","text":"","code":""},{"path":"anova.html","id":"instructions-15","chapter":"16 ANOVA","heading":"16.7.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab4.Rmd”Use Lab4.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 4 blackboard.","code":""},{"path":"anova.html","id":"problems-14","chapter":"16 ANOVA","heading":"16.7.2 Problems","text":"Consider following example data -subjects experiment two groups, B:Use R conduct t.test ANOVA data. use R prove results analyses . example, prove p-values , prove F-value T-value related. (3 points)Look lab ANOVA wrote undergraduate statistics OER lab manual https://crumplab.github.io/statisticsLab/lab-8-one-way-anova.html. lab shows example obtaining data published paper psych science one-factor ANOVA used part analysis. Load data, conduct ANOVA, report ggplot means, use papaja help write short results section reporting ANOVA result. (3 points).","code":"\n\nexample_data <- tibble(Group = rep(c(\"A\",\"B\"), each = 5),\n                       DV = c(2,4,3,5,4,7,6,5,6,7))"},{"path":"anova-and-randomization.html","id":"anova-and-randomization","chapter":"17 ANOVA and Randomization","heading":"17 ANOVA and Randomization","text":"","code":""},{"path":"anova-and-randomization.html","id":"reading-8","chapter":"17 ANOVA and Randomization","heading":"17.1 Reading","text":"Chapters 9 10 .75","code":""},{"path":"anova-and-randomization.html","id":"overview-18","chapter":"17 ANOVA and Randomization","heading":"17.2 Overview","text":"Simulating Null Randomization testAlternative HypothesesThroughout course reiterate ANOVA regression fundamentally analysis. example, readings textbook week show ANOVA linear regression perspective. purpose lab develop another similarity, ANOVA randomization/permutation tests (remember discussed last semester lab 6).\nplan lab walk conceptual issues involved detecting causal influences experiments one independent variable multiple levels. see ANOVA one approach detecting causal influences. lab attempt expose assumptions made ANOVA, also show Randomization tests employed non-parametric alternative (test doesn’t depend assumptions inherent ANOVA).","code":""},{"path":"anova-and-randomization.html","id":"how-can-you-know-if-your-experiment-works","chapter":"17 ANOVA and Randomization","heading":"17.3 How can you know if your experiment works?","text":"planning run -subjects experiment one independent variable multiple levels (let’s say four). experimenter, interested whether manipulation (IV) causes change measurement (DV). assign subjects randomly levels IV, take measurements subject performance levels. , look means level.aspect manipulation causally changes means measurement (DV), expect find differences means levels. However, randomly assigning different subjects different levels, also know find differences means result error introduced random assignment process. Thus, consider role chance producing differences measurements part inferential process determining whether experiment causal influence.","code":""},{"path":"anova-and-randomization.html","id":"constructing-a-null-hypothesis","chapter":"17 ANOVA and Randomization","heading":"17.4 Constructing a null-hypothesis","text":"Last lab walked details pieces ANOVA, including F-ratio computed directly data. Remember, formula F :\\(F = \\frac{MS_{}}{MS_{within}}\\)\\(MS_{}\\) variance representing average differences levels independent variable. number grow larger differences means across levels grows larger. \\(MS_{within}\\) variance representing average differences among subjects inside level. assume manipulation change random differences subjects, variance roughly regardless whether manipulation causes change means.\\(F\\)-ratio summary statistic data. \\(F\\) tend get larger 1 experimental manipulation actually causes difference means levels. numerator \\(MS_{}\\), tend much larger denominator \\(MS_{within}\\). time, \\(F\\) can take range values just random chance . , first step process statistical inference determine kinds values F can take result random assignment process. involves constructing sampling distribution \\(F\\) null-hypothesis differences means.","code":""},{"path":"anova-and-randomization.html","id":"f-distribution","chapter":"17 ANOVA and Randomization","heading":"17.4.1 F distribution","text":"mentioned last class, base R provide distribution functions \\(F\\), df(), pf(), qf(), rf().example, can sample random deviates F-distribution:, can compute probability obtaining F-values, given associated p-values.distribution functions follow formal analytic solutions F-distribution, requires advanced calculus. wikipedia page F-distribution shows math involved https://en.wikipedia.org/wiki/F-distribution. Although math advanced beyond scope lab, concept behind math advanced, can approximate distributions R using monte-carlo simulation.","code":"\nsome_F_values <- rf(1000,3,16)\nhist(some_F_values)\npf(4, 3, 16, lower.tail = FALSE)\n#> [1] 0.02658133"},{"path":"anova-and-randomization.html","id":"simulating-a-null-f-distribution-in-r","chapter":"17 ANOVA and Randomization","heading":"17.5 Simulating a null F-distribution in R","text":"simulate null F-distribution R, consider overarching point experimental design. Let’s say want really good job controlling error introduced randomly assigning participants groups. , run actual experiment, instead run thousands control experiments. control experiments, randomly assign subjects different groups measure dependent variable. However, conduct manipulation . case impossible manipulation anything wasn’t one. However, still assigned subjects randomly different groups. result, can measure F-ratios control experiments. Sometimes random chance larger smaller F-ratios. variation F-ratios control experiments result random assignment process fact dependent measure inherent variability. careful experimenter, interested knowing control distribution F-values looks like run experiment. words, null-distribution record happens across ultimate set control experiments (manipulation nothing). estimate distribution, run experiment ask whether think result found happened control scenario (e.g., something chance , manipulation).Let’s quick check see results simulating like results get analytic F-distributions. simulated distribution converge analytic one number iterations monte-carlo simulation increased.","code":"\nlibrary(tibble)\n\n# construct a dataframe to represent sampling random subjects into each group of the design\n\nlevels <- 4\nn_per_level <- 5\n\nrandom_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = rnorm(levels*n_per_level, 0, 1)\n                      )\n\n# compute the ANOVA and extractd the F-value\naov.out <- aov(DV ~ IV, data = random_data)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\n\n\n# repeat the above many times to compute the F-distribution\nsave_F_values <- length(1000)\nfor(i in 1:1000){\nrandom_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = rnorm(levels*n_per_level, 0, 1)\n                      )\naov.out <- aov(DV ~ IV, data = random_data)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\nsave_F_values[i] <- simulated_F\n}\nlibrary(ggplot2)\n\nF_comparison <- tibble(type = rep(c(\"analytic\",\"simulated\"), each = 1000), \n                        F_value = c(rf(1000,3,16),save_F_values))\n\nggplot(F_comparison, aes(x=F_value))+\n  geom_histogram()+\n  facet_wrap(~type)"},{"path":"anova-and-randomization.html","id":"anova-assumptions","chapter":"17 ANOVA and Randomization","heading":"17.5.1 ANOVA assumptions","text":"major purpose running simulations find ways inspect underlying assumptions ANOVA. can see simulation produced F-distribution one obtained analytic solution F-distribution.basic assumption simulation, also appear ANOVA, score subject (used compute F-ratio) randomly sampled normal distribution. simulation, randomly sampled numbers subject unit normal distribution mean = 0, standard deviation = 1.first point make parameters simulated normal distribution influence F-distribution. Remember, normal distribution overall shape matter mean standard deviation . result, always get F-distribution assume samples coming normal distribution. illustrate point, let’s re-run simulation, take values normal distribution mean = 50, standard deviation = 25.common term describe assumption ..d, stands independent identically distributed. , assume score data sampled independently (randomly) scores, score sampled identical distribution scores. two simulated F-distributions , implemented iid assumption times. one case, sampled values normal mean = 0 , sd = 1, sampled scores normal mean = 50, sd = 25. aside, worth considering whether assumption ever true real world. example, run control experiment sample people different groups, manipulation beyond assigning people different groups, confident mean measurement person sampling iid normal distribution?Even though practice routinely violate assumptions ANOVA, also case F-distributions fairly resistant violations assumptions. example, let’s assume individual scores sampled bimodal distribution, obviously normal. result F-distribution roughly similar previous ones based normal distribution.","code":"\nsave_new_F_values <- length(1000)\nfor(i in 1:1000){\nrandom_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = rnorm(levels*n_per_level, 50, 25)\n                      )\naov.out <- aov(DV ~ IV, data = random_data)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\nsave_new_F_values[i] <- simulated_F\n}\n\nF_comparison <- tibble(type = rep(c(\"analytic\",\"sim_0_1\",\"sim_50_25\"), each = 1000), \n                        F_value = c(rf(1000,3,16),save_F_values, save_new_F_values))\n\nggplot(F_comparison, aes(x=F_value))+\n  geom_histogram()+\n  facet_wrap(~type)\n# an example bi-modal sample\nhist(sample(c(rnorm(levels*n_per_level/2,0,1),rnorm(levels*n_per_level/2,5,1))))\n\nsave_bimodal_F_values <- length(1000)\nfor(i in 1:1000){\n\n  random_data <- tibble(subjects = 1:(levels*n_per_level),\n                        IV = as.factor(rep(1:levels, each = n_per_level)),\n                        DV = sample(c(rnorm(levels*n_per_level/2,0,1),\n                                      rnorm(levels*n_per_level/2,5,1)))\n                        )\n  aov.out <- aov(DV ~ IV, data = random_data)\n  simulated_F <- summary(aov.out)[[1]]$`F value`[1]\n  save_bimodal_F_values[i] <- simulated_F\n}\n\nF_comparison <- tibble(type = rep(c(\"analytic\",\"sim_0_1\",\"sim_50_25\", \"sim_bimodal\"), each = 1000), \n                        F_value = c(rf(1000,3,16),save_F_values, save_new_F_values,save_bimodal_F_values))\n\nggplot(F_comparison, aes(x=F_value))+\n  geom_histogram(bins=100)+\n  facet_wrap(~type)"},{"path":"anova-and-randomization.html","id":"p-values","chapter":"17 ANOVA and Randomization","heading":"17.5.2 p-values","text":"Let’s consider p-values moment. graph shows several simulations “ultimate control experiment” design. \\(F-values\\) values can obtained chance alone. Remember, introduce randomness assigning different people randomly different groups, assume inherent variability measurement also unpredictable.Let’s conventional moment ask critical value F, alpha .05. value F 95% F values smaller critical value.can use qf function compute critical value precisely:Let’s see close simulations got critical value:Blurring eyes bit, see control experiments manipulation nothing produce F values smaller 3.2 3.4ish 95% time. Looking distributions, , can see never produce values much larger 10.conduct real experiment, might want observe F value larger 3.2ish. found F-value larger 10, pretty confident null-distribution cause F-value. Instead, seem plausible experimental manipulation actually caused change means measured variable.","code":"\nqf(.95,3,16)\n#> [1] 3.238872\nlibrary(dplyr)\n\n## 1000 deviates samples from true F distribution\nFs <- F_comparison[F_comparison$type==\"analytic\",]$F_value\nsorted_Fs <- sort(Fs)\nsorted_Fs[950]\n#> [1] 3.325027\n\n## 1000 deviates samples from normal sim_0_1\nFs <- F_comparison[F_comparison$type==\"sim_0_1\",]$F_value\nsorted_Fs <- sort(Fs)\nsorted_Fs[950]\n#> [1] 3.103811\n\n## 1000 deviates samples from normal sim_50_25\nFs <- F_comparison[F_comparison$type==\"sim_50_25\",]$F_value\nsorted_Fs <- sort(Fs)\nsorted_Fs[950]\n#> [1] 3.20533\n\n## 1000 deviates samples from bimodal sim\nFs <- F_comparison[F_comparison$type==\"sim_bimodal\",]$F_value\nsorted_Fs <- sort(Fs)\nsorted_Fs[950]\n#> [1] 3.818101"},{"path":"anova-and-randomization.html","id":"randomization-test","chapter":"17 ANOVA and Randomization","heading":"17.6 Randomization test","text":"seen true F-distribution well approximated simulation distributions, F-distribution robust violations assumptions ANOVA. example, changed base distribution bi-modal distribution, found pretty similar overall F-distribution ones obtained assuming normal distribution.generally, can construct useful null distributions one-factor design depend distributional assumptions ANOVA. example, conduct randomization test. randomization test creates null-distribution controls random assignment process. example, clear experiment involves randomly assigning people different groups, assignment happened just one many possible assignments groups. example, assigned tall people one group, average height people one group, shorter people another group, really want claim manipulation like listening different kinds music, caused people different heights? , instead say assignment process caused people specific heights assigned specific groups. general question , given people measuring different, kinds differences groups get just assigning people randomly different groups?randomization test determines answer question taking observed data, reshuffling across groups many times , time computing test statistic like \\(F\\). sImagine data one group larger mean others. just made numbers , definetly come normal distribution. numbers around 9, numbers group 1 happen 10.Even though violating assumptions ANOVA, can still compute ANOVA table data. , get \\(F\\) value 7.407. p-value F-value assumes data randomly sampled normal distribution, .Instead using provided p-value, can conduct randomization test. involves shuffling data, computing F, repeating several times create sampling distribution \\(F\\). distribution specific numbers dataset. asking, kind \\(F\\) values obtained numbers person randomly assigned different groups.can see, distribution possible F values obtained randomly reshuffling data isn’t terribly different analytic F distribution based normal distribution.Using F distribution Randomization test, can compute p-value observed F-value find similar result.","code":"\nlevels <- 4\nn_per_level <- 5\n\nsome_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = c(11,12,11,11,12,\n                             10,8,10,9,10,\n                             8,9,10,10,10,\n                             10,8,10,9,10\n                             )\n                      )\naov.out <- aov(DV ~ IV, data = some_data)\nsummary(aov.out)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> IV           3   15.0   5.000   7.407 0.00249 **\n#> Residuals   16   10.8   0.675                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsave_F_values <- length(1000)\nfor(i in 1:1000){\n  some_data <- tibble(subjects = 1:(levels*n_per_level),\n                        IV = as.factor(rep(1:levels, each = n_per_level)),\n                        DV = sample(c(11,12,11,11,12,\n                               10,8,10,9,10,\n                               8,9,10,10,10,\n                               10,8,10,9,10\n                               ))\n                        )\n  aov.out <- aov(DV ~ IV, data =some_data)\n  simulated_F <- summary(aov.out)[[1]]$`F value`[1]\n  save_F_values[i] <- simulated_F\n}\n\nF_comparison <- tibble(type = rep(c(\"analytic\",\"randomization\"), each = 1000), \n                        F_value = c(rf(1000,3,16),save_F_values))\n\nggplot(F_comparison, aes(x=F_value))+\n  geom_histogram()+\n  facet_wrap(~type)\nlength(save_F_values[save_F_values>7.407])/1000\n#> [1] 0.004"},{"path":"anova-and-randomization.html","id":"the-alternative-hypotheses","chapter":"17 ANOVA and Randomization","heading":"17.7 The alternative hypotheses","text":"section showed example simulating null-hypothesis design one independent variable multiple levels. basic set-pretty straightforward. randomly sample numbers distribution different groups. Thus, null hypothesis every group mean average , mean distribution came .want simulate situation experimental manipulation presumed actually something measurement? manipulation causes change measurement, definition must violating iid assumption. longer case scores sampled different groups taken randomly identical distributions. Instead, think manipulation effectively causing multiple distributions exist. one level causes large increase mean, think scores group coming distribution shifted mean, relative scores groups.","code":""},{"path":"anova-and-randomization.html","id":"simulating-alternative-hypotheses","chapter":"17 ANOVA and Randomization","heading":"17.7.1 Simulating alternative hypotheses","text":"null-hypothesis easy simulate iid assumption. However, alternative hypotheses general difficult infinity .Let’s begin simulating one specific alternative hypothesis. method just declare distributions data sampled group. scenario null, possible alternative.Let’s imagine measuring performance standardized test, mean = 100, sd = 25. four groups manipulate kind music people listen take test. Let’s say reason listening 1970s sitcom music causally changes test performance, increases mean group 1 one whole standard deviation. Let’s declare exact situation R.tibble represents random values four groups sampled two different distributions. Group one gets rnorm(n,125,25), three groups get rnorm(n,100,25).general, can specify alternative hypothesis want. declare properties distribution(s) generating scores group.can even generate F-distributions based alternative specified. example, kind F-values expect get set assumptions?alternative F-distribution spread , tends higher values null. , still good deal overlap. example, critical value F based null isHow many times F higher 3.23 alternative distribution?pretty low number, means true effect detected often even actually . words, design low power detect detect effect.","code":"\n\nlevels <- 4\nn_per_level <- 5\n\nalternative_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = c(rnorm(n_per_level, 125, 25),\n                             rnorm(n_per_level, 100, 25),\n                             rnorm(n_per_level, 100, 25),\n                             rnorm(n_per_level, 100, 25)\n                             )\n                      )\n# repeat the above many times to compute the F-distribution\nsave_altF_values <- length(1000)\nfor(i in 1:1000){\nalternative_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = c(rnorm(n_per_level, 125, 25),\n                             rnorm(n_per_level, 100, 25),\n                             rnorm(n_per_level, 100, 25),\n                             rnorm(n_per_level, 100, 25)\n                             )\n                      )\naov.out <- aov(DV ~ IV, data = alternative_data)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\nsave_altF_values[i] <- simulated_F\n}\n\nF_comparison <- tibble(type = rep(c(\"null\",\"alternative\"), each = 1000), \n                        F_value = c(rf(1000,3,16),save_altF_values))\n\nggplot(F_comparison, aes(x=F_value))+\n  geom_histogram(bins=100)+\n  facet_wrap(~type, nrow=2)\nqf(.95,3,16)\n#> [1] 3.238872\nlength(save_altF_values[save_altF_values > 3.23])\n#> [1] 266"},{"path":"anova-and-randomization.html","id":"number-of-subjects-needed","chapter":"17 ANOVA and Randomization","heading":"17.7.2 Number of subjects needed","text":"programmed one group mean shifted whole standard deviation. large difference, reliably detected design large amount variability stemming small n. Consider happens increase number subjects, say 5 50 group:scenario \\(F\\) values alternative distribution much higher null distribution, reliably . critical value F slightly different now increasing n changed degrees freedom. new critical value :Now, can ask many experiments 1000 surpassed critical value?case, design sensitive, 100% experiments yielded F-value larger critical value. cost-benefit perspective, increasing n 50 group might waste resources. example, least amount subjects need power around 95%?One solution fidget n find 95%…example, wiggled n_per_level getting 95% experiments passing critical value, landed n = 25 per group. Note, simulation little noisy conducting 1000 simulations, get precise stable estimate increasing number simulations.Instead wiggling n hand, run set simulations different n, plot curve power-curve:","code":"\nn_per_level <- 50\n\n# repeat the above many times to compute the F-distribution\nsave_altF_values <- length(1000)\nfor(i in 1:1000){\nalternative_data <- tibble(subjects = 1:(levels*n_per_level),\n                      IV = as.factor(rep(1:levels, each = n_per_level)),\n                      DV = c(rnorm(n_per_level, 125, 25),\n                             rnorm(n_per_level, 100, 25),\n                             rnorm(n_per_level, 100, 25),\n                             rnorm(n_per_level, 100, 25)\n                             )\n                      )\naov.out <- aov(DV ~ IV, data = alternative_data)\nsimulated_F <- summary(aov.out)[[1]]$`F value`[1]\nsave_altF_values[i] <- simulated_F\n}\n\nF_comparison <- tibble(type = rep(c(\"null\",\"alternative\"), each = 1000), \n                        F_value = c(rf(1000,3,96),save_altF_values))\n\nggplot(F_comparison, aes(x=F_value))+\n  geom_histogram(bins=100)+\n  facet_wrap(~type, nrow=2)\nqf(.95,3,200-4)\n#> [1] 2.650677\nlength(save_altF_values[save_altF_values > 2.65])\n#> [1] 1000\n\nn_per_level <- 25\n\n# repeat the above many times to compute the F-distribution\nsave_altF_values <- length(1000)\nfor(i in 1:1000){\n  alternative_data <- tibble(subjects = 1:(levels*n_per_level),\n                        IV = as.factor(rep(1:levels, each = n_per_level)),\n                        DV = c(rnorm(n_per_level, 125, 25),\n                               rnorm(n_per_level, 100, 25),\n                               rnorm(n_per_level, 100, 25),\n                               rnorm(n_per_level, 100, 25)\n                               )\n                        )\n  aov.out <- aov(DV ~ IV, data = alternative_data)\n  simulated_F <- summary(aov.out)[[1]]$`F value`[1]\n  save_altF_values[i] <- simulated_F\n}\n\nlength(save_altF_values[save_altF_values > qf(.95,3,(n_per_level*4)-4)])/1000\n#> [1] 0.964\n\nnum_subjects <- c(5,10,15,20,25,30)\nsim_power <- length(length(num_subjects))\n\nfor(n in 1:length(num_subjects)){\n\n  n_per_level <- num_subjects[n]\n  \n  # repeat the above many times to compute the F-distribution\n  save_altF_values <- length(1000)\n  for(i in 1:1000){\n    alternative_data <- tibble(subjects = 1:(levels*n_per_level),\n                          IV = as.factor(rep(1:levels, each = n_per_level)),\n                          DV = c(rnorm(n_per_level, 125, 25),\n                                 rnorm(n_per_level, 100, 25),\n                                 rnorm(n_per_level, 100, 25),\n                                 rnorm(n_per_level, 100, 25)\n                                 )\n                          )\n    aov.out <- aov(DV ~ IV, data = alternative_data)\n    simulated_F <- summary(aov.out)[[1]]$`F value`[1]\n    save_altF_values[i] <- simulated_F\n  }\n  \n  power <- length(save_altF_values[save_altF_values > qf(.95,3,(n_per_level*4)-4)])/1000\n  sim_power[n] <- power\n}\n\npower_curve <- tibble(num_subjects,\n                      sim_power)\n\nknitr::kable(power_curve)\n\nggplot(power_curve, aes(x=num_subjects,\n                        y=sim_power))+\n  geom_point()+\n  geom_line()"},{"path":"anova-and-randomization.html","id":"simulated-power-analysis","chapter":"17 ANOVA and Randomization","heading":"17.7.3 Simulated power analysis","text":"Last semester discussed power analysis simple two-group designs along concept effect-size. designs, used Cohen’s D measure effect-size, expresses mean difference groups terms standard deviation units. several ways generalize ideas one-factor ANOVA, two groups.Notice example, didn’t discuss general measure effect-size power-analysis. Instead, declared one group different mean groups, also set size difference one standard deviation. approach useful allows , researcher, declare specifically think might happen experiment, conduct simulation based power analysis determine sufficient sample-size detect effects specified. less line Lenth, R. V.76, advocates thoughtful tailored (research question hand) approach problem power analysis sample-size planning.Let’s consider one example simulated power-analysis general kind question. Assume researcher studying performance standardized test normally distributed. interested manipulations help people learn material tested, subsequently improve test scores. going run experiment 4 groups, group get different learning interventions. don’t know anything whether learning interventions anything. idea “effect” test scores. Can power analysis , even idea happen?…example, consider four different interventions cause mean test performance go amount…, since groups independent, potentially find combination possible ways four group means different!case, let’s say interested practical matters like whether actually use particular learning intervention help students. stage research trying different learning interventions see work. trying four different ones time. good practical question , kind learning intervention worth using? learning intervention changes test performance .00000001 standard deviation worth discovering? Perhaps purposes training students. size difference worth ?Let’s say interested sdetecting evidence manipulations caused test performance go around .2 standard deviations average. ’m pulling number air meaningful, perhaps ’s big enough shift interesting. many subjects needed reasonably powered design, let’s say power = 80%?can implement situation changing mean value group. case, sample mean parameter another normal distribution mean = 0 sd = .2. results data group sampled different normal distributions average mean differences .2 standard deviations direction.","code":"\nnum_subjects <- c(10, 50, 75, 100, 125, 150, 175, 200)\nsim_power <- length(length(num_subjects))\n\nfor(n in 1:length(num_subjects)){\n\n  n_per_level <- num_subjects[n]\n  \n  # repeat the above many times to compute the F-distribution\n  save_altF_values <- length(1000)\n  for(i in 1:1000){\n    alternative_data <- tibble(subjects = 1:(levels*n_per_level),\n                          IV = as.factor(rep(1:levels, each = n_per_level)),\n                          DV = c(rnorm(n_per_level, rnorm(1,0,.2), 1),\n                                 rnorm(n_per_level, rnorm(1,0,.2), 1),\n                                 rnorm(n_per_level, rnorm(1,0,.2), 1),\n                                 rnorm(n_per_level, rnorm(1,0,.2), 1)\n                                 )\n                          )\n    aov.out <- aov(DV ~ IV, data = alternative_data)\n    simulated_F <- summary(aov.out)[[1]]$`F value`[1]\n    save_altF_values[i] <- simulated_F\n  }\n  \n  power <- length(save_altF_values[save_altF_values > qf(.95,3,(n_per_level*4)-4)])/1000\n  sim_power[n] <- power\n}\n\npower_curve <- tibble(num_subjects,\n                      sim_power)\n\nknitr::kable(power_curve)\n\nggplot(power_curve, aes(x=num_subjects,\n                        y=sim_power))+\n  geom_point()+\n  geom_line()"},{"path":"anova-and-randomization.html","id":"lab-5-generalization-assignment-1","chapter":"17 ANOVA and Randomization","heading":"17.8 Lab 5 Generalization Assignment","text":"","code":""},{"path":"anova-and-randomization.html","id":"instructions-16","chapter":"17 ANOVA and Randomization","heading":"17.8.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab5.Rmd”Use Lab5.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 5 blackboard.","code":""},{"path":"anova-and-randomization.html","id":"problems-15","chapter":"17 ANOVA and Randomization","heading":"17.8.2 Problems","text":"context making decisions whether observed data consistent inconsistent Null hypothesis, possible make errors. questions ask create simulated data containing patterns lead correct incorrect decisions role null hypothesis.Consider design 3 groups, 10 people per group. Assume dependent variable assumed normally distributed, use unit normal distributions mean = 0, sd = 1 simulations.Create simulated data design produced null hypothesis, results \\(F\\) value smaller critical value \\(F\\) design (assume alpha = .05). Report ANOVA, show ggplot means simulated data. Furthermore, display individual data points top means. reject null hypothesis situation, incorrect correct rejecting null? (3 points)Create simulated data design produced null hypothesis, results \\(F\\) value smaller critical value \\(F\\) design (assume alpha = .05). Report ANOVA, show ggplot means simulated data. Furthermore, display individual data points top means. reject null hypothesis situation, incorrect correct rejecting null? (3 points)Create simulated data design produced null hypothesis, results \\(F\\) value larger critical value \\(F\\) design (assume alpha = .05). Report ANOVA, show ggplot means simulated data. Furthermore, display individual data points top means. reject null hypothesis situation, incorrect correct rejecting null? (3 points)Create simulated data design produced null hypothesis, results \\(F\\) value larger critical value \\(F\\) design (assume alpha = .05). Report ANOVA, show ggplot means simulated data. Furthermore, display individual data points top means. reject null hypothesis situation, incorrect correct rejecting null? (3 points)Bonus QuestionIn lab saw F-distribution robust violations assumptions ANOVA. example, simulation null based bi-modal distribution similar true F distribution. bonus question, show can “break” F-distribution. Specifically, can run simulation samples numbers non-normal distribution produce different looking F-distribution? (3 points)","code":""},{"path":"anova-and-randomization.html","id":"references-4","chapter":"17 ANOVA and Randomization","heading":"17.9 References","text":"","code":""},{"path":"linear-contrasts.html","id":"linear-contrasts","chapter":"18 Linear Contrasts","heading":"18 Linear Contrasts","text":"","code":""},{"path":"linear-contrasts.html","id":"readings-6","chapter":"18 Linear Contrasts","heading":"18.1 Readings","text":"Chapters 12, 13, 14 .77","code":""},{"path":"linear-contrasts.html","id":"overview-19","chapter":"18 Linear Contrasts","heading":"18.2 Overview","text":"","code":""},{"path":"linear-contrasts.html","id":"practical-i-orthogonal-constrasts","chapter":"18 Linear Contrasts","heading":"18.3 Practical I: Orthogonal Constrasts","text":"start practical example orthogonal contrasts R. following R code shows reproduce example 12.4 Abdi, Edelman, Dowling, & Valentin78. video briefly reviews example discusses details implementing orthogonal contrasts R.\n(#tab:example_table)\nsidenote, Abdi, Edelman, Dowling, & Valentin79 points use complete set orthogonal contrasts, omnibus F-value average F-values contrasts.","code":"\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(dplyr)\noptions(dplyr.summarise.inform = FALSE)\n\n# create the example data\nsmith_example <- tribble(\n  ~Same, ~Different, ~Imagery, ~Photo, ~Placebo,\n  #--|--|--|--|----\n  25,11,14,25,8,\n  26,21,15,15,20,\n  17,9,29,23,10,\n  15,6,10,21,7,\n  14,7,12,18,15,\n  17,14,22,24,7,\n  14,12,14,14,1,\n  20,4,20,27,17,\n  11,7,22,12,11,\n  21,19,12,11,4\n) %>% \n  pivot_longer(cols = everything(),\n               names_to = \"IV\",\n               values_to = \"DV\") %>%\n  mutate(IV = factor(IV,levels = c(\"Same\", \n                                    \"Different\", \n                                    \"Imagery\", \n                                    \"Photo\", \n                                    \"Placebo\")))\n\n# Note: R automatically orders Factor levels alphabetically\n# we later define contrasts between groups using\n# the the order from the textbook\n# so we need to explicitly declare the order of levels\n\n# run the omnibus test\naov.out <- aov(DV~IV, smith_example)\nsummary(aov.out)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> IV           4    700     175   5.469 0.00112 **\n#> Residuals   45   1440      32                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# check the existing contrasts for your factor\ncontrasts(smith_example$IV)\n#>           Different Imagery Photo Placebo\n#> Same              0       0     0       0\n#> Different         1       0     0       0\n#> Imagery           0       1     0       0\n#> Photo             0       0     1       0\n#> Placebo           0       0     0       1\n\n# define your new set of contrasts\n\nc1 <- c(2,-3,2,2,-3)\nc2 <- c(2,0,-1,-1,0)\nc3 <- c(0,0,+1,-1,0)\nc4 <- c(0,+1,0,0,-1)\n\n# combine them into the columns of a matrix\nmy_contrasts <- cbind(c1, c2, c3, c4)\n\n# assign them to the contrast property for your IV\ncontrasts(smith_example$IV) <- my_contrasts\n\n# rerun ANOVA\naov.out <- aov(DV~IV, smith_example)\nsummary(aov.out)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> IV           4    700     175   5.469 0.00112 **\n#> Residuals   45   1440      32                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# use summary.aov and split to print out contrast info\n# define your contrast labels that will get printed to the table\n\n(full_summary <- summary.aov(aov.out,\n                             split=list(IV=list(\"(1+3+4) vs (2+5)\"=1, \n                                                \"(1) vs (3+4)\" = 2, \n                                                \"(3) vs (4)\"= 3,\n                                                \"(2) vs (5)\"= 4)\n                                        )\n                             )\n  )\n#>                        Df Sum Sq Mean Sq F value   Pr(>F)    \n#> IV                      4    700     175   5.469  0.00112 ** \n#>   IV: (1+3+4) vs (2+5)  1    675     675  21.094 3.52e-05 ***\n#>   IV: (1) vs (3+4)      1      0       0   0.000  1.00000    \n#>   IV: (3) vs (4)        1     20      20   0.625  0.43334    \n#>   IV: (2) vs (5)        1      5       5   0.156  0.69450    \n#> Residuals              45   1440      32                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(papaja)\napa_table(apa_print(full_summary)$table)\nfull_summary[[1]]$`F value`[1]\n#>         \n#> 5.46875\nmean(full_summary[[1]]$`F value`[2:5])\n#> [1] 5.46875"},{"path":"linear-contrasts.html","id":"concept-1-understanding-orthogonal-contrasts","chapter":"18 Linear Contrasts","heading":"18.4 Concept 1: Understanding orthogonal contrasts","text":"figure shows group means different grand mean. top right panel shows group mean can considered unique source difference. bottom three panels right show set three orthogonal contrasts, alternative way breaking sources differences.similar pattern group means, along grand mean, deviations, sum squares.groups different, independent influence grand mean, can think group mean ’s unique effect (represented four different colored lines top right panel ).Even though four presumed influences, estimating basis grand mean, one degree freedom lost. , three group means number, last one fixed order set produce specific grand mean .define linear contrasts, defining make-specific means terms combinations set means. , default ANOVA assumption mean unique, can thought basic set orthogonal contrasts.enter group names means dataframe, declare IV factor, can see basic set orthogonal contrasts.Let’s walk answer rhetorical questions. question group mean , “estimate mean?”. First , assume group means deviations grand mean. group mean expressed sum grand mean deviation.Let’s start D. values data use estimate mean D? values column D show contrast weights. saying take 100% deviation D (4), 0% means.goes C B. estimate means, grand mean unique deviationsSo far say mean sum grand mean unique deviations grand mean, get original mean back B, C, D:Notice contrast weight . last mean left. Remember degrees freedom issue. can define three means terms grand mean, last one fixed. one value can , order grand mean 7.","code":"\nknitr::include_graphics(\"imgs/LinearContrasts/LinearContrasts.004.jpeg\")\n\ngroup_means <- c(4,3,10,11)\n(grand_mean <- mean(group_means))\n#> [1] 7\n(differences <- group_means-grand_mean)\n#> [1] -3 -4  3  4\n(squared_differences <- differences^2)\n#> [1]  9 16  9 16\n(sum_squares <- sum(squared_differences))\n#> [1] 50\nfake_data <- tibble(IV = factor(c(\"A\",\"B\",\"C\",\"D\")),\n                    DV = c(4,3,10,11))\n\ncontrasts(fake_data$IV)\n#>   B C D\n#> A 0 0 0\n#> B 1 0 0\n#> C 0 1 0\n#> D 0 0 1\ncontrasts(fake_data$IV)[,'D']\n#> A B C D \n#> 0 0 0 1\n\ncontrasts(fake_data$IV)[,'D'] * differences\n#> A B C D \n#> 0 0 0 4\n\n# the formula for D\ngrand_mean + (1 * differences[4])\n#> [1] 11\ncontrasts(fake_data$IV)\n#>   B C D\n#> A 0 0 0\n#> B 1 0 0\n#> C 0 1 0\n#> D 0 0 1\n\ncontrasts(fake_data$IV) * differences\n#>    B C D\n#> A  0 0 0\n#> B -4 0 0\n#> C  0 3 0\n#> D  0 0 4\ngrand_mean*contrasts(fake_data$IV) + contrasts(fake_data$IV) * differences\n#>   B  C  D\n#> A 0  0  0\n#> B 3  0  0\n#> C 0 10  0\n#> D 0  0 11\ngrand_mean\n#> [1] 7\n\n# A has to be four, if the others are 3, 10, and 11)\nmean(c(4, 3, 10, 11))\n#> [1] 7"},{"path":"linear-contrasts.html","id":"other-orthogonal-contrasts","chapter":"18 Linear Contrasts","heading":"18.4.1 Other orthogonal contrasts","text":"Just keep picture close hand, previous section showed top right panel, showing idea mean unique source, can described terms linear contrasts.set means can described set orthogonal linear contrasts. example, consider set three contrasts figure.formula textbook computing Sums Squares contrasts :\\(\\frac{S(\\sum{C_a M_a})^2}{\\sum{C_a^2}}\\)compute SSs contrast, can see add total SS (50) example . true set orthogonal linear contrasts.","code":"\nknitr::include_graphics(\"imgs/LinearContrasts/LinearContrasts.004.jpeg\")\nc1 <- c(-1,-1,1,1)\nc2 <- c(1,-1,0,0)\nc3 <- c(0,0,-1,1)\n\nmy_contrasts <- cbind(c1,c2,c3)\n\ncontrasts(fake_data$IV) <- my_contrasts\ncontrasts(fake_data$IV)\n#>   c1 c2 c3\n#> A -1  1  0\n#> B -1 -1  0\n#> C  1  0 -1\n#> D  1  0  1\n\n# check they are orthogonal\ncor(contrasts(fake_data$IV))\n#>    c1 c2 c3\n#> c1  1  0  0\n#> c2  0  1  0\n#> c3  0  0  1\n# multiple contrast weights by group means\ncontrasts(fake_data$IV) * group_means\n#>   c1 c2  c3\n#> A -4  4   0\n#> B -3 -3   0\n#> C 10  0 -10\n#> D 11  0  11\n\n# Find the sums for each column\ncolSums(contrasts(fake_data$IV) * group_means)\n#> c1 c2 c3 \n#> 14  1  1\n\n# Square the sums\ncolSums(contrasts(fake_data$IV) * group_means)^2\n#>  c1  c2  c3 \n#> 196   1   1\n\n# divide by the SS for the contrast weights\n(colSums(contrasts(fake_data$IV) * group_means)^2)/ colSums(contrasts(fake_data$IV)^2)\n#>   c1   c2   c3 \n#> 49.0  0.5  0.5"},{"path":"linear-contrasts.html","id":"explanation-part-2","chapter":"18 Linear Contrasts","heading":"18.4.2 Explanation part 2","text":"addendum “explanation”, gave low score explanatory value. thought better, came …’s thing orthogonal linear contrasts: add right way, guaranteed get original means back.Let’s see process action:example, means :set three linear contrasts :claim add contrasts right way, can get back original pattern means.Let’s try adding, remember start grand means. , question , “can add combination linear contrasts way can get original pattern means back?”.First, note need contrasts . example, now way can add just first linear contrast somehow get original means back…example matter many times add linear contrast grand means, can never get pattern original means…also B, C always D. won’t capture extra nuances, need two contrasts.Check , add three first contrast, 1 second third contrasts. ’m getting something close pattern original means. , ’s clear exactly much add. claim combination produce original means, ? “coefficients” telling much contrast add?fiddle hand try different coefficients…, connect issue concept linear regression. Effectively, trying explain pattern means (4,3,10,11), terms multiple linear regression three different linear contrasts. question weights ?Ok, just used multiple linear regression find much contrast need add together reproduce original pattern means. coefficients 3, 1, , 1, just need change 3.5, .5, , .5:Although don’t time lab dive issue. always true set orthogonal linear contrasts. can always decompose recompose set means weighted contributions linear contrasts. leave define different orthogonal linear contrasts set means, show can added produce original set means (sounds like good generalization problem).one last demonstration. point set means can described combination set orthogonal contrasts. made one example set orthogonal linear contrasts four group situation, able show set contrasts can reproduce set means.’ve assembled code previous examples. using previously defined set orthogonal contrasts. Now, able put numbers want group means (DV), find set contrasts can perfectly explain .","code":"\nfake_data$DV\n#> [1]  4  3 10 11\ncontrasts(fake_data$IV)\n#>   c1 c2 c3\n#> A -1  1  0\n#> B -1 -1  0\n#> C  1  0 -1\n#> D  1  0  1\ngrand_means <- c(7,7,7,7)\ngrand_means\n#> [1] 7 7 7 7\ngrand_means + contrasts(fake_data$IV)[,1]\n#> A B C D \n#> 6 6 8 8\ngrand_means + contrasts(fake_data$IV)[,1]*2\n#> A B C D \n#> 5 5 9 9\ngrand_means + contrasts(fake_data$IV)[,1]*3\n#>  A  B  C  D \n#>  4  4 10 10\ngrand_means+\n(contrasts(fake_data$IV)[,1]*3.5)+\n(contrasts(fake_data$IV)[,2]*.5)+\n(contrasts(fake_data$IV)[,3]*.5)  \n#>  A  B  C  D \n#>  4  3 10 11\nfake_data_2 <- fake_data\nfake_data_2 <- cbind(fake_data,contrasts(fake_data$IV))\n\nlm(DV ~ c1 + c2 + c3, data = fake_data_2 )\n#> \n#> Call:\n#> lm(formula = DV ~ c1 + c2 + c3, data = fake_data_2)\n#> \n#> Coefficients:\n#> (Intercept)           c1           c2           c3  \n#>         7.0          3.5          0.5          0.5\nsummary(lm(DV ~ c1 + c2 + c3, data = fake_data_2 ))\n#> \n#> Call:\n#> lm(formula = DV ~ c1 + c2 + c3, data = fake_data_2)\n#> \n#> Residuals:\n#> ALL 4 residuals are 0: no residual degrees of freedom!\n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)      7.0        NaN     NaN      NaN\n#> c1               3.5        NaN     NaN      NaN\n#> c2               0.5        NaN     NaN      NaN\n#> c3               0.5        NaN     NaN      NaN\n#> \n#> Residual standard error: NaN on 0 degrees of freedom\n#> Multiple R-squared:      1,  Adjusted R-squared:    NaN \n#> F-statistic:   NaN on 3 and 0 DF,  p-value: NA\ngrand_means+\n(contrasts(fake_data$IV)[,1]*3.5)+\n(contrasts(fake_data$IV)[,2]*.5)+\n(contrasts(fake_data$IV)[,3]*.5)  \n#>  A  B  C  D \n#>  4  3 10 11\n# you should be able to change any of the DV numbers\n# and always find a combination of contrasts\n# that perfectly explains the data\n\nfake_data <- tibble(IV = factor(c(\"A\",\"B\",\"C\",\"D\")),\n                    DV = c(43,22,53,104))\n\nc1 <- c(-1,-1,1,1)\nc2 <- c(1,-1,0,0)\nc3 <- c(0,0,-1,1)\nmy_contrasts <- cbind(c1,c2,c3)\n\ncontrasts(fake_data$IV) <- my_contrasts\n\nfake_data_2 <- cbind(fake_data,contrasts(fake_data$IV))\n\nlm(DV ~ c1 + c2 + c3, data = fake_data_2 )\n#> \n#> Call:\n#> lm(formula = DV ~ c1 + c2 + c3, data = fake_data_2)\n#> \n#> Coefficients:\n#> (Intercept)           c1           c2           c3  \n#>        55.5         23.0         10.5         25.5\nsummary(lm(DV ~ c1 + c2 + c3, data = fake_data_2 ))\n#> \n#> Call:\n#> lm(formula = DV ~ c1 + c2 + c3, data = fake_data_2)\n#> \n#> Residuals:\n#> ALL 4 residuals are 0: no residual degrees of freedom!\n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)     55.5        NaN     NaN      NaN\n#> c1              23.0        NaN     NaN      NaN\n#> c2              10.5        NaN     NaN      NaN\n#> c3              25.5        NaN     NaN      NaN\n#> \n#> Residual standard error: NaN on 0 degrees of freedom\n#> Multiple R-squared:      1,  Adjusted R-squared:    NaN \n#> F-statistic:   NaN on 3 and 0 DF,  p-value: NA"},{"path":"linear-contrasts.html","id":"concept-2-family-wise-error-rate","chapter":"18 Linear Contrasts","heading":"18.5 Concept 2: Family-wise error rate","text":"Orthogonal linear contrasts show single omnibus ANOVA can broken set contrasts. useful especially contrasts designed examine patterns interest.time, conducting linear contrasts like conducting multiple independent significance tests, conduct tests, increase likelihood making inferential error (e.g., rejecting null even effect isn’t real).section supplement textbook section 12.3.2, describing Monte Carlo simulation illustrate difference alphas:\\(\\alpha(PC)\\) alpha per comparison: probability making type error single test\\(\\alpha(PC)\\) alpha per comparison: probability making type error single test\\(\\alpha(PF)\\), alpha per family comparisons, probability making least one type error across whole set related tests.\\(\\alpha(PF)\\), alpha per family comparisons, probability making least one type error across whole set related tests.","code":""},{"path":"linear-contrasts.html","id":"monte-carlo-simulation-of-two-alphas","chapter":"18 Linear Contrasts","heading":"18.5.1 Monte-Carlo simulation of two alphas","text":"textbook describes monte-carlo simulation one-factor design 6 groups. 100 observations per group, sampled normal distribution.simulation two things:Conduct omnibus ANOVA compute F. p-value smaller .05, “reject null”, count type error.Conduct omnibus ANOVA compute F. p-value smaller .05, “reject null”, count type error.conduct 5 linear orthogonal contrasts, compute F, p-values smaller .05, “reject” , count type errors.conduct 5 linear orthogonal contrasts, compute F, p-values smaller .05, “reject” , count type errors.textbook example, 10,000 simulations. code contains basic components need perform simulation.conduct simulation want count type errors. Specifically, let’s count type errors omnibus test, well linear contrasts. simulation similar one reported Table 12.1 textbook.terms organizing code, adopt strategy saving necessary data simulation dataframe can analyzed produce summary table.Let’s use dplyr count many type errors occurred omnibus contrast based F-tests.type error rate omnibus test pretty close .05:Similarly, conducted 50000 total linear contrasts. Looking individual contrasts, type error rate also close .05:However, family-wise type error rate larger. conducted 10,000 simulated experiments, asked question, probability rejecting null linear contrasts significant, find:","code":"\n# example dataframe to simulate null\nsim_data <- tibble(DV = rnorm(6*100,0,1),\n                   IV = factor(rep(1:6, each = 100)))\n\n# example orthogonal linear contrasts\nc1 <- c(1,-1,0,0,0,0)\nc2 <- c(0,0,1,-1,0,0)\nc3 <- c(0,0,0,0,1,-1)\nc4 <- c(-1,-1,2,2,-1,-1)\nc5 <- c(1,1,0,0,-1,-1)\n\n# create contrast matrix\north_contrasts <- cbind(c1,c2,c3,c4,c5)\n\n# check contrasts are orthogonal\ncor(orth_contrasts)\n#>    c1 c2 c3 c4 c5\n#> c1  1  0  0  0  0\n#> c2  0  1  0  0  0\n#> c3  0  0  1  0  0\n#> c4  0  0  0  1  0\n#> c5  0  0  0  0  1\n\n# assign new contrasts to IV\ncontrasts(sim_data$IV) <- orth_contrasts\n\n# run ANOVA\nsummary.aov(aov(DV~IV, sim_data), split=list(IV=list(\"c1\"=1, \n                                                \"c2\" = 2, \n                                                \"c3\"= 3,\n                                                \"c4\"= 4,\n                                                \"c5\" = 5)\n                                        ))\n#>              Df Sum Sq Mean Sq F value Pr(>F)  \n#> IV            5    8.1   1.617   1.491 0.1908  \n#>   IV: c1      1    5.6   5.649   5.209 0.0228 *\n#>   IV: c2      1    0.1   0.146   0.134 0.7141  \n#>   IV: c3      1    0.1   0.057   0.053 0.8185  \n#>   IV: c4      1    1.1   1.091   1.006 0.3163  \n#>   IV: c5      1    1.1   1.141   1.053 0.3053  \n#> Residuals   594  644.1   1.084                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#create a tibble to store all the results\nall_sim_data <- tibble()\n\n# conduct simulation\nfor(i in 1:10000){\n\nsim_data <- tibble(DV = rnorm(6*100,0,1),\n                   IV = factor(rep(1:6, each = 100)))\n\ncontrasts(sim_data$IV) <- orth_contrasts\n\nsim_output <- summary.aov(aov(DV~IV, sim_data), split=list(IV=list(\"c1\"=1, \n                                                \"c2\" = 2, \n                                                \"c3\"= 3,\n                                                \"c4\"= 4,\n                                                \"c5\" = 5)\n                                        ))\n\n#save current results in a tibble\nsim_results <- tibble(type = c(\"omnibus\",rep(\"contrast\",5)),\n                      p_values = sim_output[[1]]$`Pr(>F)`[1:6],\n                      sim_num = rep(i,6)\n                      )\n\n# add current results to the tibble storing all the results\nall_sim_data <- rbind(all_sim_data,sim_results)\n}\n# analyze the sim data\ntype_I_errors <- all_sim_data %>%\n  mutate(type_I = p_values < .05) %>%\n  group_by(type, sim_num) %>%\n  summarize(counts = sum(type_I)) %>%\n  group_by(type,counts) %>%\n  summarize(type_I_frequency = sum(counts))\n\nknitr::kable(type_I_errors)\ntype_I_errors %>%\n  filter(type == 'omnibus',\n         counts == 1) %>%\n  pull(type_I_frequency)/10000\n#> [1] 0.0473\ntype_I_errors %>%\n  filter(type == 'contrast',\n         counts > 0) %>%\n  pull(type_I_frequency) %>%\n  sum()/50000\n#> [1] 0.04884\ntype_I_errors %>%\n  filter(type == 'contrast',\n         counts > 0) %>%\n  pull(type_I_frequency) %>%\n  sum()/10000\n#> [1] 0.2442"},{"path":"linear-contrasts.html","id":"concept-3-correcting-for-multiple-comparisons","chapter":"18 Linear Contrasts","heading":"18.6 Concept 3: Correcting for multiple comparisons","text":"textbook describes several methods “correcting” p-values multiple comparisons protect family-wise type error rate. corrections include Sidak, Bonferroni, Boole, Dunn. going purposefully leave concept section mostly blank; , discuss class.blunt perspective, think generally useful report many tests conducted, along uncorrected p-values . experience, many different people different perspectives done. corrections can done uncorrected p-values, reader preference, supplied necessary information corrections think necessary.","code":""},{"path":"linear-contrasts.html","id":"practical-ii-non-orthogonal-contrasts","chapter":"18 Linear Contrasts","heading":"18.7 Practical II: Non-orthogonal Contrasts","text":"practical section brief. purpose show accomplish textbook examples chapter 13 R.","code":""},{"path":"linear-contrasts.html","id":"pretend-non-orthogonal-contrasts-are-orthogonal","chapter":"18 Linear Contrasts","heading":"18.7.1 13.2.3 Pretend non-orthogonal contrasts are orthogonal","text":"example, sample data “romeo juliet” example supplied, along four linear contrasts testing different research hypotheses. textbook shows examples computing F statistics contrast.can R , minor difficulties.","code":"\nromeo_juliet <- tibble(subjects = 1:20,\n                       Group = rep(c(\"Context Before\",\n                                 \"Partial Context\",\n                                 \"Context After\",\n                                 \"Without context\"), each = 5),\n                       Comprehension = c(5,9,8,4,9,\n                                         5,4,3,5,4,\n                                         2,4,5,4,1,\n                                         3,3,2,4,3\n                                   )\n                          )\n\nromeo_juliet$Group <- factor(romeo_juliet$Group,\n                             levels = c(\"Context Before\",\n                                 \"Partial Context\",\n                                 \"Context After\",\n                                 \"Without context\")\n                             )\n\n# define non-orthogonal contrasts\n\nc1 <- c(1,1,1,-3)\nc2 <- c(0,0,1,-1)\nc3 <- c(3,-1,-1,-1)\nc4 <- c(1,-1,0,0)\n\nnew_contrasts <- cbind(c1,c2,c3,c4)\ncor(new_contrasts)\n#>           c1        c2        c3        c4\n#> c1 1.0000000 0.8164966 0.3333333 0.0000000\n#> c2 0.8164966 1.0000000 0.0000000 0.0000000\n#> c3 0.3333333 0.0000000 1.0000000 0.8164966\n#> c4 0.0000000 0.0000000 0.8164966 1.0000000\n\ncontrasts(romeo_juliet$Group) <- new_contrasts\n\n# note, by default, correctly computes the first contrast, but not the rest...\n\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"c1\"=1, \"c2\" = 2, \"c3\"= 3, \"c4\" = 4)))\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#>   Group: c1  1  12.15   12.15   5.170 0.03710 * \n#>   Group: c2  1  19.20   19.20   8.170 0.01138 * \n#>   Group: c3  1  19.60   19.60   8.340 0.01070 * \n#>   Group: c4  1                                  \n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# you could enter them one at a time:\n\ncontrasts(romeo_juliet$Group) <- c1\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"c1\"=1)))\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#>   Group: c1  1  12.15   12.15   5.170 0.03710 * \n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncontrasts(romeo_juliet$Group) <- c2\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"c2\"=1)))\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#>   Group: c2  1   0.10    0.10   0.043 0.83917   \n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncontrasts(romeo_juliet$Group) <- c3\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"c3\"=1)))\n#>             Df Sum Sq Mean Sq F value   Pr(>F)    \n#> Group        3  50.95   16.98   7.227 0.002782 ** \n#>   Group: c3  1  46.82   46.82  19.922 0.000392 ***\n#> Residuals   16  37.60    2.35                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncontrasts(romeo_juliet$Group) <- c4\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"c4\"=1)))\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group        3  50.95   16.98   7.227 0.00278 **\n#>   Group: c4  1  19.60   19.60   8.340 0.01070 * \n#> Residuals   16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-contrasts.html","id":"multiple-regression-of-non-orthogonal-contrasts","chapter":"18 Linear Contrasts","heading":"18.7.2 13.3.3 Multiple regression of non-orthogonal contrasts","text":"Next jump last example. use “romeo juliet” data, given three non-orthogonal contrasts (table 13.5).example, textbook looks F-values contrasts depend whether take “traditional” approach, consider role semi-partial correlation (modern approach).","code":"\nromeo_juliet <- tibble(subjects = 1:20,\n                       Group = rep(c(\"Context Before\",\n                                 \"Partial Context\",\n                                 \"Context After\",\n                                 \"Without context\"), each = 5),\n                       Comprehension = c(5,9,8,4,9,\n                                         5,4,3,5,4,\n                                         2,4,5,4,1,\n                                         3,3,2,4,3\n                                   )\n                          )\n\nromeo_juliet$Group <- factor(romeo_juliet$Group,\n                             levels = c(\"Context Before\",\n                                 \"Partial Context\",\n                                 \"Context After\",\n                                 \"Without context\")\n                             )\n\n# traditional approach\n\n# define contrasts\n\nc1 <- c(3,-1,-1,-1)\nc2 <- c(1,1,-1,-1)\nc3 <- c(1,-1,1,-1)\n\n# run individual ANOVAs for each contrast\n\ncontrasts(romeo_juliet$Group) <- c1\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"contrast\"=1)))\n#>                   Df Sum Sq Mean Sq F value   Pr(>F)    \n#> Group              3  50.95   16.98   7.227 0.002782 ** \n#>   Group: contrast  1  46.82   46.82  19.922 0.000392 ***\n#> Residuals         16  37.60    2.35                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncontrasts(romeo_juliet$Group) <- c2\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"contrast\"=1)))\n#>                   Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group              3  50.95   16.98   7.227 0.00278 **\n#>   Group: contrast  1  31.25   31.25  13.298 0.00217 **\n#> Residuals         16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncontrasts(romeo_juliet$Group) <- c3\nsummary.aov(aov(Comprehension~Group, romeo_juliet), split=list(Group=list(\"contrast\"=1)))\n#>                   Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Group              3  50.95   16.98   7.227 0.00278 **\n#>   Group: contrast  1  11.25   11.25   4.787 0.04386 * \n#> Residuals         16  37.60    2.35                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# the modern approach\n\n# add contrasts as factors in a multiple linear regression\n\nromeo_juliet <- romeo_juliet %>%\n  mutate(c1 = rep(c(3,-1,-1,-1),each=5),\n         c2 = rep(c(1,1,-1,-1),each=5),\n         c3 = rep(c(1,-1,1,-1),each=5)\n         )\n\n# conduct the multiple linear regression\nsummary(lm(Comprehension ~ c1 + c2 + c3 , romeo_juliet))\n#> \n#> Call:\n#> lm(formula = Comprehension ~ c1 + c2 + c3, data = romeo_juliet)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -3.00  -1.05   0.00   0.85   2.00 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   4.3500     0.3428  12.690 9.11e-10 ***\n#> c1            0.6500     0.3428   1.896   0.0761 .  \n#> c2            0.6000     0.4848   1.238   0.2337    \n#> c3            0.1000     0.4848   0.206   0.8392    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.533 on 16 degrees of freedom\n#> Multiple R-squared:  0.5754, Adjusted R-squared:  0.4958 \n#> F-statistic: 7.227 on 3 and 16 DF,  p-value: 0.002782\n\n# note the t^2 values are the same as the F's from the textbook\n\n# semi-partial correlations\n\nlibrary(ppcor)\nspcor(romeo_juliet[,3:6])$estimate^2\n#>               Comprehension         c1         c2          c3\n#> Comprehension    1.00000000 0.09542631 0.04065500 0.001129305\n#> c1               0.06116540 1.00000000 0.08315764 0.166459175\n#> c2               0.04368932 0.13942024 1.00000000 0.145925742\n#> c3               0.00132626 0.30499118 0.15947322 1.000000000"},{"path":"linear-contrasts.html","id":"lab-6-generalization-assignment-1","chapter":"18 Linear Contrasts","heading":"18.8 Lab 6 Generalization Assignment","text":"","code":""},{"path":"linear-contrasts.html","id":"instructions-17","chapter":"18 Linear Contrasts","heading":"18.8.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab6.Rmd”Use Lab6.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 6 blackboard.","code":""},{"path":"linear-contrasts.html","id":"problems-16","chapter":"18 Linear Contrasts","heading":"18.8.2 Problems","text":"Section 12.3.3 textbook refers : problem replications meaningless experiment: ‘alpha captain’s age’. issue run ineffectual experiment enough times can always find significant result chance. textbook mentions repeat experiment 20 times, guaranteed find significant result .64 probability, probability .92 repeat experiment 50 times.Make use rbinom() function show can reproduce probabilities. (1 point)Make use rbinom() function show can reproduce probabilities. (1 point)ineffectual experiment conducted 20 times, four groups, experimenter accept significant result orthogonal linear contrasts, probability finding significant result ? (1 point)ineffectual experiment conducted 20 times, four groups, experimenter accept significant result orthogonal linear contrasts, probability finding significant result ? (1 point)next two questions draw connection technique yet discussed called p-curve analysis.80 P-curve analysis sometimes used purposes meta-analyses determine whether “good” evidence effect literature.Consider researcher publishes study showing significant effect, p <. 05; , reality researcher makes type error, manipulation cause difference. many researchers replicated study, kind p-values find? Use R create sampling distribution p-values expected situation. shape distribution ? (2 points)Consider researcher publishes study showing significant effect, p <. 05; , reality researcher makes type error, manipulation cause difference. many researchers replicated study, kind p-values find? Use R create sampling distribution p-values expected situation. shape distribution ? (2 points)Now assume published result reflects true effect. Specifically, let’s imagine study two groups (-subjects), 20 subjects group. Assume scores subjects sampled normal distribution, group larger mean group B .5 standard deviations (e.g., Cohen’s d = .5). many researchers replicated study, kind p-values find? Use R create sampling distribution p-values expected situation. shape distribution ? (2 points)Now assume published result reflects true effect. Specifically, let’s imagine study two groups (-subjects), 20 subjects group. Assume scores subjects sampled normal distribution, group larger mean group B .5 standard deviations (e.g., Cohen’s d = .5). many researchers replicated study, kind p-values find? Use R create sampling distribution p-values expected situation. shape distribution ? (2 points)Bonus QuestionsSame #3, except now assume design four groups (-subjects). Assume group mean .5 standard deviations larger groups B, C, D. Use R create sampling distribution p-values expected linear contrast evaluating research hypothesis > B = C = D. (1 point)#3, except now assume design four groups (-subjects). Assume group mean .5 standard deviations larger groups B, C, D. Use R create sampling distribution p-values expected linear contrast evaluating research hypothesis > B = C = D. (1 point)Consider one-factor subjects ANOVA four groups. Run two simulations null-hypothesis, one omnibus test, one specific linear contrast mentioned > B = C = D. probability rejecting type error (rejecting null alpha < .05) omnibus test versus specific contrast? (1 point)Consider one-factor subjects ANOVA four groups. Run two simulations null-hypothesis, one omnibus test, one specific linear contrast mentioned > B = C = D. probability rejecting type error (rejecting null alpha < .05) omnibus test versus specific contrast? (1 point)","code":""},{"path":"linear-contrasts.html","id":"references-5","chapter":"18 Linear Contrasts","heading":"18.9 References","text":"","code":""},{"path":"factorial-anova-1.html","id":"factorial-anova-1","chapter":"19 Factorial ANOVA","heading":"19 Factorial ANOVA","text":"","code":""},{"path":"factorial-anova-1.html","id":"reading-9","chapter":"19 Factorial ANOVA","heading":"19.1 Reading","text":"Chapter 16 Abdi, Edelman, Dowling, & Valentin81. See also Chapters 9 10 Crump, Navarro, & Suzuki82 factorial designs.","code":""},{"path":"factorial-anova-1.html","id":"overview-20","chapter":"19 Factorial ANOVA","heading":"19.2 Overview","text":"lab includes practical conceptual introduction factorial ANOVAs R. practical sections show use aov() function compute ANOVAs designs multiple independent variables, shows compute textbook examples R. conceptual sections make use R tool illustrate ideas main effects statistical interactions.","code":""},{"path":"factorial-anova-1.html","id":"practical-1-factorial-anovas-using-aov","chapter":"19 Factorial ANOVA","heading":"19.3 Practical 1: Factorial ANOVAs using aov()","text":"aov() function used one-factor ANOVAs also conduct ANOVAs mutiple factors. new requirements :long dataframe columns factor (independent variable) measurements (dependent variable)long dataframe columns factor (independent variable) measurements (dependent variable)formula instructing aov() function compute intended ANOVAA formula instructing aov() function compute intended ANOVA","code":""},{"path":"factorial-anova-1.html","id":"example-long-data-frame","chapter":"19 Factorial ANOVA","heading":"19.3.1 Example long data frame","text":"Designs multiple independent variables can become fairly complicated. example uses 2x2 design. two independent variables two levels . design crossed, fully factorial, level one IV paired every level IV.","code":"\nlibrary(tibble)\n\n# set number of subjects per cell\nn <- 10\nfactorial_data <- tibble(A = factor(rep(c(\"L1\",\"L2\"), each = n)),\n                         B = factor(rep(c(\"L1\",\"L2\"), n)),\n                         DV = rnorm(n*2,0,1))"},{"path":"factorial-anova-1.html","id":"example-anova-formula","chapter":"19 Factorial ANOVA","heading":"19.3.2 Example ANOVA formula","text":"formula aov() similar . general syntax DV_column_name ~ IV1_column_name * IV2_column_name. important new element * symbol. instructs aov() function compute possible main effects interactions.example replaced variable names names respective columns factorial_data tibble.Briefly, factorial design three IVs, another * formula along name third IV. example, DV_name ~ IV1_name * IV2_name * IV3_name.","code":"\naov_out <- aov(DV ~ A*B, data = factorial_data)\nsummary(aov_out)\n#>             Df Sum Sq Mean Sq F value Pr(>F)  \n#> A            1  0.132  0.1317   0.273 0.6088  \n#> B            1  0.592  0.5925   1.226 0.2845  \n#> A:B          1  1.904  1.9044   3.941 0.0645 .\n#> Residuals   16  7.731  0.4832                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.tables(aov_out, type = \"means\")\n#> Tables of means\n#> Grand mean\n#>           \n#> -0.114805 \n#> \n#>  A \n#> A\n#>       L1       L2 \n#> -0.19595 -0.03366 \n#> \n#>  B \n#> B\n#>       L1       L2 \n#> -0.28692  0.05731 \n#> \n#>  A:B \n#>     B\n#> A    L1      L2     \n#>   L1 -0.6766  0.2847\n#>   L2  0.1028 -0.1701"},{"path":"factorial-anova-1.html","id":"three-factor-example","chapter":"19 Factorial ANOVA","heading":"19.3.3 Three factor example","text":"exercise, consider modify example code create simulated random data design three IVs, two levels. defined 2x2x2 design. ’ve provided example code .","code":"\nn <- 12\nfactorial_data <- tibble(A = factor(rep(c(\"L1\",\"L2\"), each = n)),\n                         B = factor(rep(rep(c(\"L1\",\"L2\"), each = n/2),2)),\n                         C = factor(rep(c(\"L1\",\"L2\"), n)),\n                         DV = rnorm(n*2,0,1))\n\nsummary(aov(DV ~ A*B*C, data = factorial_data))\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> A            1  0.117   0.117   0.066  0.800\n#> B            1  2.483   2.483   1.405  0.253\n#> C            1  0.433   0.433   0.245  0.627\n#> A:B          1  0.044   0.044   0.025  0.876\n#> A:C          1  4.362   4.362   2.468  0.136\n#> B:C          1  0.498   0.498   0.282  0.603\n#> A:B:C        1  5.170   5.170   2.925  0.107\n#> Residuals   16 28.275   1.767"},{"path":"factorial-anova-1.html","id":"example-with-ggplot","chapter":"19 Factorial ANOVA","heading":"19.3.4 Example with ggplot","text":"analyzing real data may want accomplish number analysis tasks, preserving data long-form, looking means table, looking means data points graphs, printing ANOVA summary tables. following example code shows steps one snippet.","code":"\n#load libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# simulated data (pretend running an experiment)\nn <- 10\nfactorial_data <- tibble(A = factor(rep(c(\"A1\",\"A2\"), each = n)),\n                         B = factor(rep(c(\"B1\",\"B2\"), n)),\n                         DV = rnorm(n*2,0,1))\n\n# Look at the means in a table\nfactorial_data %>%\n  group_by(A,B) %>%\n  summarise(mean_DV = mean(DV))\n#> # A tibble: 4 × 3\n#> # Groups:   A [2]\n#>   A     B     mean_DV\n#>   <fct> <fct>   <dbl>\n#> 1 A1    B1     -0.133\n#> 2 A1    B2      0.659\n#> 3 A2    B1     -0.128\n#> 4 A2    B2      0.333\n\n# look at the means in a plot\nfactorial_data %>%\n  ggplot(aes(y=DV, x=A, group = B,fill=B))+\n   geom_bar(stat=\"summary\", fun = \"mean\", position=\"dodge\") +\n   geom_point(position = position_dodge(width=0.5))\n\n# look at plots of the main effects and interaction\n\nA <- factorial_data %>%\n  group_by(A) %>%\n  summarise(mean_DV = mean(DV)) %>%\n  ggplot(aes(y=mean_DV, x=A))+\n   geom_bar(stat=\"identity\", position=\"dodge\") + \n   ggtitle(\"Main effect A\")\n\nB <- factorial_data %>%\n  group_by(B) %>%\n  summarise(mean_DV = mean(DV)) %>%\n  ggplot(aes(y=mean_DV, x=B))+\n   geom_bar(stat=\"identity\", position=\"dodge\")+ \n   ggtitle(\"Main effect B\")\n\nAB <- factorial_data %>%\n  group_by(A,B) %>%\n  summarise(mean_DV = mean(DV)) %>%\n  ggplot(aes(y=mean_DV, x=A, fill=B))+\n   geom_bar(stat=\"identity\", position=\"dodge\")+ \n   ggtitle(\"AxB Interaction\")\n\n# patchwork formula\n(A+B)/AB\n\n# ANOVA table\n# print to console\naov_out <- aov(DV ~ A*B, data = factorial_data)\nsummary(aov_out)\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> A            1  0.130  0.1296   0.094  0.763\n#> B            1  1.960  1.9605   1.428  0.249\n#> A:B          1  0.137  0.1367   0.100  0.756\n#> Residuals   16 21.959  1.3724\n\n# ANOVA means\n# print to console\nmodel.tables(aov_out, type = \"means\")\n#> Tables of means\n#> Grand mean\n#>           \n#> 0.1826051 \n#> \n#>  A \n#> A\n#>      A1      A2 \n#> 0.26311 0.10210 \n#> \n#>  B \n#> B\n#>      B1      B2 \n#> -0.1305  0.4957 \n#> \n#>  A:B \n#>     B\n#> A    B1      B2     \n#>   A1 -0.1326  0.6589\n#>   A2 -0.1283  0.3325"},{"path":"factorial-anova-1.html","id":"practical-2-textbook-examples","chapter":"19 Factorial ANOVA","heading":"19.4 Practical 2: Textbook examples","text":"","code":""},{"path":"factorial-anova-1.html","id":"model-i-fixed-effects","chapter":"19 Factorial ANOVA","heading":"19.4.1 Model I Fixed effects","text":"default aov() function computes model sums squares fixed effects. use aov() function compute “cute cued recall” example section 16.7. Represent data table 16.3, generate plot similar figure 16.4, compute ANOVA table page 306.","code":"\na1b1 <- c(11,9,7,11,12,7,12,11,10,10)\na1b2 <- c(12,12,7,9,9,10,12,10,7,12)\na2b1 <- c(13,18,19,13,8,15,13,9,8,14)\na2b2 <- c(13,21,20,15,17,14,13,14,16,7)\na3b1 <- c(17,20,22,13,21,16,23,19,20,19)\na3b2 <- c(32,31,27,30,29,30,33,25,25,28)\n\nrecall_data <- tibble(words_recalled = c(a1b1,a1b2,\n                                         a2b1,a2b2,\n                                         a3b1,a3b2),\n                      A = rep(c(\"12 words\",\n                                \"24 words\",\n                                \"48 words\"), each = 20),\n                      B = rep(rep(c(\"Free recall\",\n                                \"Cued Recall\"), each = 10),3)\n                      )\n\nggplot(recall_data, aes(x=A, y=words_recalled, group = B, linetype=B))+\n  geom_point(stat=\"summary\", fun=\"mean\")+\n  geom_line(stat=\"summary\", fun=\"mean\")\n\naov_out <- aov(words_recalled ~ A*B, data = recall_data)\n\nsummary(aov_out)\n#>             Df Sum Sq Mean Sq F value   Pr(>F)    \n#> A            2   2080    1040  115.56  < 2e-16 ***\n#> B            1    240     240   26.67 3.58e-06 ***\n#> A:B          2    280     140   15.56 4.62e-06 ***\n#> Residuals   54    486       9                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel.tables(aov_out, type=\"means\")\n#> Tables of means\n#> Grand mean\n#>    \n#> 16 \n#> \n#>  A \n#> A\n#> 12 words 24 words 48 words \n#>       10       14       24 \n#> \n#>  B \n#> B\n#> Cued Recall Free recall \n#>          18          14 \n#> \n#>  A:B \n#>           B\n#> A          Cued Recall Free recall\n#>   12 words 10          10         \n#>   24 words 15          13         \n#>   48 words 29          19"},{"path":"factorial-anova-1.html","id":"model-ii-random-effects","chapter":"19 Factorial ANOVA","heading":"19.4.2 Model II Random effects","text":"factors random, F-ratios computed different manner (see textbook). example covers section 16.8.4 textbook, involving example data two random factors.Note, use Anova() function car package, provides way specify type sums squares.Finally, example data provide identical ANOVA tables regardless whether model II sums squares used. MSE interaction term residual error term example data. , even though looks like getting answer aov() function, computing model model II.","code":"\nA1 <- c(127,121,117,109,107,101,98,94,97,89)\nA2 <- c(117,109,113,113,108,104,95,93,96,92)\nA3 <- c(111,111,111,101,99,91,95,89,89,83)\nA4 <- c(108,100,100,92,92,90,87,77,89,85)\n\nrandom_data <- tibble(scores = c(A1,A2,A3,A4),\n                      A = factor(rep(1:4,each = 10)),\n                      B = factor(rep(rep(1:5,each=2),4))\n                      )\n\naov.lm <- lm(formula = scores ~ A*B, data = random_data)\ncar::Anova(aov.lm, type = 2)\n#> Anova Table (Type II tests)\n#> \n#> Response: scores\n#>           Sum Sq Df F value    Pr(>F)    \n#> A           1200  3      20 3.102e-06 ***\n#> B           3200  4      40 2.836e-09 ***\n#> A:B          240 12       1    0.4827    \n#> Residuals    400 20                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\naov_out <- aov(scores ~ A*B, data = random_data)\nsummary(aov_out)\n#>             Df Sum Sq Mean Sq F value   Pr(>F)    \n#> A            3   1200     400      20 3.10e-06 ***\n#> B            4   3200     800      40 2.84e-09 ***\n#> A:B         12    240      20       1    0.483    \n#> Residuals   20    400      20                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"factorial-anova-1.html","id":"model-iii-mixed-models-one-fixedone-random","chapter":"19 Factorial ANOVA","heading":"19.4.3 Model III Mixed models (one fixed/one random)","text":"textbook briefly mentions model III, give worked example. now, ’m going leave section blank; however, mixed models fairly common Psychology plan return section brief examples.","code":""},{"path":"factorial-anova-1.html","id":"conceptual-1-factorial-anova-and-family-wise-error","chapter":"19 Factorial ANOVA","heading":"19.5 Conceptual 1: Factorial ANOVA and family-wise error","text":"Factorial designs include multiple independent variables interactions. number independent variables increase, number independent tests also increase. example, 2x2 design, three independent tests: one main effect, one interaction.consider family-wise error rate issue factorial ANOVAs. conduct simulation null model 10,000 2x2 ANOVAs. simulation record p-value main effect interaction. set alpha criterion p < .05. question : 10,000 simulated experiments, many type 1 errors made?Remember, one-factor ANOVA, definition number type errors made null alpha criterion, 5% case. also true 2x2 factorial ANOVA?proportion total number simulated experiments made type 1 error?look type error rates separately main effect interaction find?conclusion Factorial ANOVAs protected family-wise type error rate. increase number IVs, increase likelihood finding least one “significant” effect among main effects interactions.Finally, ’s quick alternative simulation using rbinom(). set number simulations 10000, size 3 (representing three independent tests), set probability getting 1 .05. Last, counted many results value greater 0 (representing type error), divided 10000 (number simulations) estimate family-wise error rate.","code":"\n# set up tibble to save simulation values\nsave_sim <- tibble()\n\n# loop to conduct i number of simulations\nfor(i in 1:10000){\n  \n  #simulate null data for a 2x2\n  n <- 10\n  factorial_data <- tibble(A = factor(rep(c(\"L1\",\"L2\"), each = n)),\n                           B = factor(rep(c(\"L1\",\"L2\"), n)),\n                           DV = rnorm(n*2,0,1))\n  # compute ANOVA\n  output <- summary(aov(DV~A*B, data=factorial_data))\n  \n  #save p-values for each effect\n  sim_tibble <- tibble(p_vals = output[[1]]$`Pr(>F)`[1:3],\n                       effect = c(\"A\",\"B\",\"AxB\"),\n                       sim = rep(i,3))\n  \n  #add the saved values to the overall tibble\n  save_sim <-rbind(save_sim,sim_tibble)\n}\ntype_I_errors <- save_sim %>%\n  filter(p_vals < .05) %>%\n  group_by(sim) %>%\n  count()\n  \ndim(type_I_errors)[1]/10000\n#> [1] 0.1413\nsave_sim %>%\n  group_by(effect) %>%\n  summarise(type_I_error = length(p_vals[p_vals < .05])/10000)\n#> # A tibble: 3 × 2\n#>   effect type_I_error\n#>   <chr>         <dbl>\n#> 1 A            0.0524\n#> 2 AxB          0.0537\n#> 3 B            0.0491\na <- rbinom(10000,3,.05)\nlength(a[a>0])/10000\n#> [1] 0.1355"},{"path":"factorial-anova-1.html","id":"lab-7-generalization-assignment-1","chapter":"19 Factorial ANOVA","heading":"19.6 Lab 7 Generalization Assignment","text":"","code":""},{"path":"factorial-anova-1.html","id":"instructions-18","chapter":"19 Factorial ANOVA","heading":"19.6.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab7.Rmd”Use Lab7.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 7 blackboard.","code":""},{"path":"factorial-anova-1.html","id":"problems-17","chapter":"19 Factorial ANOVA","heading":"19.6.2 Problems","text":"four problems worth 3 points. Choose two four. complete two, receive bonus points.Explain concept main effects interactions example using R. example, include definition main effects interactions figure depicting main effects interaction along explanation patterns . major point problem engage task developing explanation concepts 1) helpful understand concepts, 2) helpful others understand concepts. (3 points)Explain concept main effects interactions example using R. example, include definition main effects interactions figure depicting main effects interaction along explanation patterns . major point problem engage task developing explanation concepts 1) helpful understand concepts, 2) helpful others understand concepts. (3 points)Complete 2x2 factorial lab found https://crumplab.github.io/statisticsLab/lab-10-factorial-anova.html, section 10.4.8. specifically, task follow lab exercise load data, transform data long-format, conduct 2x2 subjects ANOVA, write short results section reporting main effects interaction. (3 points)Complete 2x2 factorial lab found https://crumplab.github.io/statisticsLab/lab-10-factorial-anova.html, section 10.4.8. specifically, task follow lab exercise load data, transform data long-format, conduct 2x2 subjects ANOVA, write short results section reporting main effects interaction. (3 points)chapter 10 Crump, Navarro, & Suzuki83, discussion patterns main effects interactions can occur 2x2 design, represents perhaps simplest factorial design. 8 possible outcomes discussed https://crumplab.github.io/statistics/--factorial-designs.html#looking--main-effects--interactions. Examples 8 outcomes shown two figures, one bar graphs, one line graphs. Reproduce either figures using ggplot2. (3 points)chapter 10 Crump, Navarro, & Suzuki83, discussion patterns main effects interactions can occur 2x2 design, represents perhaps simplest factorial design. 8 possible outcomes discussed https://crumplab.github.io/statistics/--factorial-designs.html#looking--main-effects--interactions. Examples 8 outcomes shown two figures, one bar graphs, one line graphs. Reproduce either figures using ggplot2. (3 points)conceptual section lab used R simulation find family-wise type error rate simple factorial design 2 independent variables. Use R simulation find family-wise type error rate factorial design 3 independent variables. (3 points)conceptual section lab used R simulation find family-wise type error rate simple factorial design 2 independent variables. Use R simulation find family-wise type error rate factorial design 3 independent variables. (3 points)Show 2x2 factorial ANOVA can accomplished using one-factor ANOVA three linear contrasts. (3 points)Show 2x2 factorial ANOVA can accomplished using one-factor ANOVA three linear contrasts. (3 points)","code":""},{"path":"factorial-anova-1.html","id":"references-6","chapter":"19 Factorial ANOVA","heading":"19.7 References","text":"","code":""},{"path":"factorial-ii.html","id":"factorial-ii","chapter":"20 Factorial II","heading":"20 Factorial II","text":"","code":""},{"path":"factorial-ii.html","id":"readings-7","chapter":"20 Factorial II","heading":"20.1 Readings","text":"Chapter 17.84","code":""},{"path":"factorial-ii.html","id":"overview-21","chapter":"20 Factorial II","heading":"20.2 Overview","text":"lab supplements chapter factorial designs contrast analyses discusses statistical techniques interrogating specific patterns among means factorial designs.Fundamental ideas strong understanding concepts main effect interactions. current lab build score model concept use R examine properties main effects interactions greater detail.","code":""},{"path":"factorial-ii.html","id":"concept-i-generating-main-effect-and-interactions","chapter":"20 Factorial II","heading":"20.3 Concept I: Generating main effect and interactions","text":"section use R generate predicted patterns main effects interactions design two independent variables.following code chunk shows score model fixed design (model ) can coded R produce pattern main effects interactions, along graphical display patterns.","code":"\n#load libraries\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# define 2-factor score model\ngrand_mean <- 50\nA <- c(0,5,10,15,20,25,50)\nB <- c(0,5,-15)\nAB <- rep(0,length(A)*length(B))\n\n# create design table\nmodel_data <- tibble()\nfor(i in 1:length(A)){\n  for(j in 1:length(B)){\n    IVA <- i \n    IVB <- j\n    DV <- grand_mean+A[i]+B[j]+AB[(i-1)*length(B)+j]\n    sc_GM <- grand_mean\n    sc_A <- A[i]\n    sc_B <- B[j]\n    sc_AB <- AB[(i-1)*length(B)+j] \n    row_entry <- tibble(IVA,IVB,DV,\n                        sc_GM,sc_A,sc_B,sc_AB)\n    model_data <- rbind(model_data,row_entry)\n  }\n}\n\nknitr::kable(model_data)\n\n# generate plots\nbar_graph <- ggplot(model_data, \n                    aes(y=DV,\n                        x=as.factor(IVA),\n                        fill=as.factor(IVB)))+\n  geom_bar(stat='identity', position='dodge')\n\nline_graph <- ggplot(model_data, \n                     aes(y=DV,\n                         x=IVA,\n                         linetype=as.factor(IVB)))+\n  geom_line()+\n  geom_point()\n\n(bar_graph/line_graph)"},{"path":"factorial-ii.html","id":"one-or-two-main-effects-and-no-interaction","chapter":"20 Factorial II","heading":"20.3.1 One or two main effects and no interaction","text":"important recognize just design uses multiple independent variables, necessarily case interactions occur.example, consider following toy example. conduct experiment, first IV three levels (0,5,10), indicating much money participants paid complete experiment. second IV also three levels (0,5,10), indicating much bonus money participants paid complete experiment. DV much money total participant given experimenter end experiment.Consider represent experiment using code . First, program expected means level first second IV. Second, program interaction term.","code":"\n# define 2-factor score model\ngrand_mean <- 10\nA <- c(-5,0,5)\nB <- c(-5,0,5)\nAB <- rep(0,length(A)*length(B))\n\n# create design table\nmodel_data <- tibble()\nfor(i in 1:length(A)){\n  for(j in 1:length(B)){\n    IVA <- i \n    IVB <- j\n    DV <- grand_mean+A[i]+B[j]+AB[(i-1)*length(B)+j]\n    sc_GM <- grand_mean\n    sc_A <- A[i]\n    sc_B <- B[j]\n    sc_AB <- AB[(i-1)*length(B)+j] \n    row_entry <- tibble(IVA,IVB,DV,\n                        sc_GM,sc_A,sc_B,sc_AB)\n    model_data <- rbind(model_data,row_entry)\n  }\n}\n\nknitr::kable(model_data)\n\n# generate plots\nbar_graph <- ggplot(model_data, \n                    aes(y=DV,\n                        x=as.factor(IVA),\n                        fill=as.factor(IVB)))+\n  geom_bar(stat='identity', position='dodge')\n\nline_graph <- ggplot(model_data, \n                     aes(y=DV,\n                         x=IVA,\n                         linetype=as.factor(IVB)))+\n  geom_line()+\n  geom_point()\n\n(bar_graph/line_graph)"},{"path":"factorial-ii.html","id":"two-main-effects-and-an-interaction","chapter":"20 Factorial II","heading":"20.3.2 Two main effects and an interaction","text":"Let’s consider example classic demonstration visual search. visual search task subjects shown visual display asked find target among distractors.example, task find T, report whether rotated left right. example displays shown .figure depicts 3x2 design, three levels set-size (10,20, 30), two levels popout (popout vs color popout),classic main effect visual search involves set-size. general, takes longer finder target number distractors (set-size) increases.However, possible modulate set-size effect. example, set-size effect pronounced color popout condition. However, color popout condition, target just “pops” , easy find. case, number distractors (set-size) small possibly effect search time.Let’s implement ideas produce expected patterns main effects interactions design. assume takes 500ms find target, set size effect changes time 10ms per distractor.","code":"\nknitr::include_graphics('imgs/Factorial/visual_search.png')\n\n# define 2-factor score model\ngrand_mean <- 500\nA <- c(-100,0,100)\nB <- c(100,-100)\nAB <- c(0,0,0,-100,0,-200)\n\n# create design table\nmodel_data <- tibble()\nfor(i in 1:length(A)){\n  for(j in 1:length(B)){\n    IVA <- i \n    IVB <- j\n    DV <- grand_mean+A[i]+B[j]+AB[(i-1)*length(B)+j]\n    sc_GM <- grand_mean\n    sc_A <- A[i]\n    sc_B <- B[j]\n    sc_AB <- AB[(i-1)*length(B)+j] \n    row_entry <- tibble(IVA,IVB,DV,\n                        sc_GM,sc_A,sc_B,sc_AB)\n    model_data <- rbind(model_data,row_entry)\n  }\n}\n\nknitr::kable(model_data)\n\n# generate plots\nbar_graph <- ggplot(model_data, \n                    aes(y=DV,\n                        x=as.factor(IVA),\n                        fill=as.factor(IVB)))+\n  geom_bar(stat='identity', position='dodge')\n\nline_graph <- ggplot(model_data, \n                     aes(y=DV,\n                         x=IVA,\n                         linetype=as.factor(IVB)))+\n  geom_line()+\n  geom_point()\n\n(bar_graph/line_graph)"},{"path":"factorial-ii.html","id":"concept-ii-simulated-power-analysis","chapter":"20 Factorial II","heading":"20.4 Concept II: Simulated power analysis","text":"following code block gives example simulating data 2x2 design can used estimate power (proportion experiments returning significant result) main effect interaction.","code":"\n# N per group\nN <- 200\n\nA_pvalue <- c()\nB_pvalue <- c()\nAB_pvalue <- c()\nfor(i in 1:1000){\n  IVA <- rep(rep(c(\"1\",\"2\"), each=2),N)\n  IVB <- rep(rep(c(\"1\",\"2\"), 2),N)\n  DV <- c(replicate(N,c(rnorm(1,0,1), # means A1B1\n                        rnorm(1,0,1), # means A1B2\n                        rnorm(1,.2,1), # means A2B1\n                        rnorm(1,.2,1)  # means A2B2\n          )))\n  sim_df <- data.frame(IVA,IVB,DV)\n  \n  aov_results <- summary(aov(DV~IVA*IVB, sim_df))\n  A_pvalue[i]<-aov_results[[1]]$`Pr(>F)`[1]\n  B_pvalue[i]<-aov_results[[1]]$`Pr(>F)`[2]\n  AB_pvalue[i]<-aov_results[[1]]$`Pr(>F)`[3]\n}\n\nlength(A_pvalue[A_pvalue<0.05])/1000\n#> [1] 0.785\nlength(B_pvalue[B_pvalue<0.05])/1000\n#> [1] 0.037\nlength(AB_pvalue[AB_pvalue<0.05])/1000\n#> [1] 0.054"},{"path":"factorial-ii.html","id":"lab-8-generalization-assignment-1","chapter":"20 Factorial II","heading":"20.5 Lab 8 Generalization Assignment","text":"Note: technical glitch video, zoomed close RStudio window. screencast visible, sometimes working code view. always, .Rmd files solutions github repository course, can check .","code":""},{"path":"factorial-ii.html","id":"instructions-19","chapter":"20 Factorial II","heading":"20.5.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab8.Rmd”Use Lab8.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 8 blackboard.","code":""},{"path":"factorial-ii.html","id":"problems-18","chapter":"20 Factorial II","heading":"20.5.2 Problems","text":"Consider 2x2 design. Assume DV measured normal distribution mean 0, standard deviation 1. Assume main effect causes total shift .5 standard deviations mean levels. Assume level 1 B control, expect measure standard effect . Assume level 2 B experimental factor intended reduce effect .25 standard deviations.. create ggplot2 figure depicts expected results design (2 points)Conduct simulation-based power analyses answer questions.B. many subjects needed detect main effect power = .8? (2 points)C. many subjects needed detect interaction effect power = .8? (2 points)Bonus point question:B1. Create power curve showing power interaction effect example influenced number subjects. Choose range N 25 800 (per cell) run simulation-based power analysis increments 25 subjects. plot results using ggplot2 (2 points).","code":""},{"path":"factorial-ii.html","id":"references-7","chapter":"20 Factorial II","heading":"20.6 References","text":"","code":""},{"path":"repeated-measures-anova.html","id":"repeated-measures-anova","chapter":"21 Repeated Measures ANOVA","heading":"21 Repeated Measures ANOVA","text":"","code":""},{"path":"repeated-measures-anova.html","id":"reading-10","chapter":"21 Repeated Measures ANOVA","heading":"21.1 Reading","text":"Chapters 18 19 Abdi, Edelman, Dowling, & Valentin85. Also see, additional examples discussion simple repeated measures Crump, Navarro, & Suzuki86, https://crumplab.github.io/statistics/repeated-measures-anova.html.","code":""},{"path":"repeated-measures-anova.html","id":"practical-1-how-to-run-a-repeated-measures-anova-in-r","chapter":"21 Repeated Measures ANOVA","heading":"21.2 Practical 1: How to run a repeated measures ANOVA in R","text":"practical section contains three textbook examples conducting repeated measures ANOVAs R. first two examples single factor designs, involving single repeated measure. third example involves 2x2 design, factors repeated measures.examples use aov() function used -subjects designs. major difference involves specifying error term appropriately formula.","code":""},{"path":"repeated-measures-anova.html","id":"single-factor-example-1","chapter":"21 Repeated Measures ANOVA","heading":"21.2.1 Single factor Example 1","text":"example 18.13 Abdi, Edelman, Dowling, & Valentin87.","code":"\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(dplyr)\n\ne1 <- tribble(~Subject, ~Drug_A, ~Placebo, ~Drug_B,\n        \"s1\", 124, 108, 104,\n        \"s2\", 105, 107, 100,\n        \"s3\",107,90,100,\n        \"s4\",109,89,93,\n        \"s5\",94,105,89,\n        \"s6\",121,71,84) \n\n# convert to long\ne1 <- pivot_longer(e1,\n                   cols = !Subject,\n                   names_to = \"IV\",\n                   values_to = \"Latency\")\n\n# Convert IVs to factors\ne1 <- e1 %>%\n  mutate(Subject = as.factor(Subject),\n         IV = as.factor(IV))\n\n# conduct ANOVA, note the addition of the error term in the formula\naov_out <- aov(Latency ~ IV + Error(Subject), data=e1)\nsummary(aov_out)\n#> \n#> Error: Subject\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> Residuals  5    750     150               \n#> \n#> Error: Within\n#>           Df Sum Sq Mean Sq F value Pr(>F)  \n#> IV         2    900     450    3.75 0.0609 .\n#> Residuals 10   1200     120                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"repeated-measures-anova.html","id":"formula-syntax-for-repeated-measures-anova","chapter":"21 Repeated Measures ANOVA","heading":"21.2.2 Formula syntax for repeated measures ANOVA","text":"Now seen example conducting repeated measures ANOVA aov(), let’s closely inspect formula syntax.general syntax repeated measures ANOVA one factor. replace DV column name dependent variable dataframe; replace IV column name independent variable dataframe; , replacee Subject column name subject variable dataframe.Compare syntax -subjects ANOVA one factor, notice subjects version include + Error(Subject) component formula.","code":"\naov(DV ~ IV + Error(Subject), data = dataframe)\naov(DV ~ IV, data = dataframe)"},{"path":"repeated-measures-anova.html","id":"single-factor-example-2","chapter":"21 Repeated Measures ANOVA","heading":"21.3 Single Factor Example 2","text":"example 18.14 Abdi, Edelman, Dowling, & Valentin88.","code":"\ne2 <- tribble(~Subject, ~r1, ~r2, ~r3, ~r4, ~r5, ~r6,\n        \"s1\",30,18,21,15,18,12,\n        \"s2\",21,23,16,17,13,12,\n        \"s3\",19,21,13,15,13,9,\n        \"s4\",19,19,16,9,11,10,\n        \"s5\",21,16,12,15,9,11,\n        \"s6\",22,17,14,12,10,9,\n        \"s7\",19,20,17,10,13,5,\n        \"s8\",17,18,11,11,9,12\n        ) \n\n# convert to long\ne2 <- pivot_longer(e2,cols = !Subject,\n                   names_to = \"Rank\",\n                   values_to = \"Recall\")\n\n# Convert IVs to factors\ne2 <- e2 %>%\n  mutate(Subject = as.factor(Subject),\n         Rank = as.factor(Rank))\n\n# conduct ANOVA, note the addition of the error term in the formula\naov_out <- aov(Recall ~ Rank + Error(Subject), data= e2)\nsummary(aov_out)\n#> \n#> Error: Subject\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> Residuals  7    168      24               \n#> \n#> Error: Within\n#>           Df Sum Sq Mean Sq F value   Pr(>F)    \n#> Rank       5    720     144      24 2.09e-10 ***\n#> Residuals 35    210       6                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"repeated-measures-anova.html","id":"two-factor-repeated-measures-example-3","chapter":"21 Repeated Measures ANOVA","heading":"21.4 Two factor repeated measures Example 3","text":"multiple within-subject factors, need specify error terms appropriately. general syntax two factor ANOVA two repeated measures.example 19.5 Abdi, Edelman, Dowling, & Valentin89.","code":"\naov(DV ~ IV1*IV2 + Error(Subjects/IV1*IV2), data= dataframe)\ngodden_baddeley <- tribble(~Subjects,~LearningPlace,~TestingPlace,~Recall,\n        \"s1\",\"On Land\",\"On Land\",34,\n        \"s2\",\"On Land\",\"On Land\",37,\n        \"s3\",\"On Land\",\"On Land\",27,\n        \"s4\",\"On Land\",\"On Land\",43,\n        \"s5\",\"On Land\",\"On Land\",44,\n        \"s1\",\"On Land\",\"Under Sea\",18,\n        \"s2\",\"On Land\",\"Under Sea\",21,\n        \"s3\",\"On Land\",\"Under Sea\",25,\n        \"s4\",\"On Land\",\"Under Sea\",37,\n        \"s5\",\"On Land\",\"Under Sea\",34,\n        \"s1\",\"Under Sea\",\"On Land\",14,\n        \"s2\",\"Under Sea\",\"On Land\",21,\n        \"s3\",\"Under Sea\",\"On Land\",31,\n        \"s4\",\"Under Sea\",\"On Land\",27,\n        \"s5\",\"Under Sea\",\"On Land\",32,\n        \"s1\",\"Under Sea\",\"Under Sea\",22,\n        \"s2\",\"Under Sea\",\"Under Sea\",25,\n        \"s3\",\"Under Sea\",\"Under Sea\",33,\n        \"s4\",\"Under Sea\",\"Under Sea\",33,\n        \"s5\",\"Under Sea\",\"Under Sea\",42\n        )\n\n# convert IVs to factors\ngodden_baddeley <- godden_baddeley %>%\n  mutate(Subjects = as.factor(Subjects),\n         LearningPlace = as.factor(LearningPlace),\n         TestingPlace = as.factor(TestingPlace))\n\n# run ANOVA\naov_out <- aov(Recall ~ LearningPlace*TestingPlace + Error(Subjects/(LearningPlace*TestingPlace)), godden_baddeley)\n\n# print out ANOVA summary table\nsummary(aov_out)\n#> \n#> Error: Subjects\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> Residuals  4    680     170               \n#> \n#> Error: Subjects:LearningPlace\n#>               Df Sum Sq Mean Sq F value Pr(>F)\n#> LearningPlace  1     80      80       2   0.23\n#> Residuals      4    160      40               \n#> \n#> Error: Subjects:TestingPlace\n#>              Df Sum Sq Mean Sq F value Pr(>F)\n#> TestingPlace  1     20      20     2.5  0.189\n#> Residuals     4     32       8               \n#> \n#> Error: Subjects:LearningPlace:TestingPlace\n#>                            Df Sum Sq Mean Sq F value Pr(>F)  \n#> LearningPlace:TestingPlace  1    320     320      20 0.0111 *\n#> Residuals                   4     64      16                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# generate plot of means\nlibrary(ggplot2)\n\nggplot(godden_baddeley, aes(x=TestingPlace,\n                            y=Recall,\n                            shape=LearningPlace,\n                            group=LearningPlace))+\n  geom_point(stat=\"summary\",fun=\"mean\")+\n  geom_line(stat=\"summary\",fun=\"mean\")+\n  theme_classic(base_size=12)"},{"path":"repeated-measures-anova.html","id":"concept-1-sphericity","chapter":"21 Repeated Measures ANOVA","heading":"21.5 Concept 1: Sphericity","text":"","code":"\n## example data set to illustrate sphericity\ntextbook <- tribble(~S, ~a1, ~a2, ~a3, ~a4,\n        \"s1\",76,64,34,26,\n        \"s2\",60,48,46,30,\n        \"s3\",58,34,32,28,\n        \"s4\",46,46,32,28,\n        \"s5\",30,18,36,28\n        ) \n\n## selecting the table of values in each column\ntextbook[,2:5]\n#> # A tibble: 5 × 4\n#>      a1    a2    a3    a4\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1    76    64    34    26\n#> 2    60    48    46    30\n#> 3    58    34    32    28\n#> 4    46    46    32    28\n#> 5    30    18    36    28\n\n## computing the covariation matrix\ncov(textbook[,2:5])\n#>     a1  a2 a3 a4\n#> a1 294 258  8 -8\n#> a2 258 294  8 -8\n#> a3   8   8 34  6\n#> a4  -8  -8  6  2\n\n## ta\ncolMeans(cov(textbook[,2:5]))\n#>  a1  a2  a3  a4 \n#> 138 138  14  -2\n\n## sa-s\ncolMeans(cov(textbook[,2:5])) - mean(cov(textbook[,2:5]))\n#>  a1  a2  a3  a4 \n#>  66  66 -58 -74\n\n## double-centering\n\ncov_matrix <- cov(textbook[,2:5])\ncol_mean_matrix <- cov_matrix*0 + colMeans(cov_matrix)\nrow_mean_matrix <- t(cov_matrix*0 + rowMeans(cov_matrix))\ndc_matrix <- cov_matrix - col_mean_matrix -row_mean_matrix + mean(cov_matrix)\n\n## greenhouse-geisser\nsum(diag(dc_matrix))^2 / ((dim(dc_matrix)[1]-1)*sum(dc_matrix^2))\n#> [1] 0.4459613"},{"path":"repeated-measures-anova.html","id":"sphericity-corrections-in-r","chapter":"21 Repeated Measures ANOVA","heading":"21.5.1 Sphericity corrections in R","text":"aov() function automatically apply corrections sphericity sometimes common statistics programs. However, ANOVA functions , ANOVA function car package. One wrinkle function requires data wide format. exampleNote, adjusted degrees freedom displayed printout. repeated measures ANOVA yielded F(3,12) = 5.3571, p = .0142330.estimate epsilon based Greenhouse Geisser .44596. Notice, p-value different printout (p = .059999). Greenhouse-Geisser estimate epsilon used modify degrees freedom.comparison purposes, ANOVA summary using aov() include corrections.","code":"\ntextbook <- tribble(~S, ~a1, ~a2, ~a3, ~a4,\n        \"s1\",76,64,34,26,\n        \"s2\",60,48,46,30,\n        \"s3\",58,34,32,28,\n        \"s4\",46,46,32,28,\n        \"s5\",30,18,36,28\n        ) \n\n## ANOVA using car\nwide_data <- as.matrix(textbook[1:5,2:5])\naov_model <- lm(wide_data ~1)\nrm_factor <- factor(c('a1','a2','a3','a4'))\nlibrary(car)\nnew_anova <- Anova(aov_model, \n                   idata=data.frame(rm_factor),\n                   idesign = ~rm_factor,\n                   type=\"III\")\nsummary(new_anova,multivariate=FALSE)\n#> \n#> Univariate Type III Repeated-Measures ANOVA Assuming Sphericity\n#> \n#>             Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n#> (Intercept)  32000      1     1152      4 111.1111 0.0004582 ***\n#> rm_factor     1800      3     1344     12   5.3571 0.0142330 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> \n#> Mauchly Tests for Sphericity\n#> \n#>           Test statistic p-value\n#> rm_factor       0.088557 0.27931\n#> \n#> \n#> Greenhouse-Geisser and Huynh-Feldt Corrections\n#>  for Departure from Sphericity\n#> \n#>            GG eps Pr(>F[GG])  \n#> rm_factor 0.44596    0.05999 .\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>              HF eps Pr(>F[HF])\n#> rm_factor 0.5871795 0.04114125\npf(q= 5.3571, df1= 3, df2= 12, lower.tail = FALSE)\n#> [1] 0.01423341\nGG_df1 = (4-1) * .44596\nGG_df2 = (4-1) * (5-1) *.44596\n\nGG_df1\n#> [1] 1.33788\nGG_df2\n#> [1] 5.35152\n\npf(q= 5.3571, df1= GG_df1, df2= GG_df2, lower.tail = FALSE)\n#> [1] 0.05999576\n# compare to ANOVA using aov\n# convert to long\nlong_data <- pivot_longer(textbook,cols = !S,\n                   names_to = \"IV\",\n                   values_to = \"DV\")\n\n# Convert IVs to factors\nlong_data <- long_data %>%\n  mutate(S = as.factor(S),\n         IV = as.factor(IV))\n\n# conduct ANOVA, note the addition of the error term in the formula\naov_out <- aov(DV ~ IV + Error(S), data= long_data)\nsummary(aov_out)\n#> \n#> Error: S\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> Residuals  4   1152     288               \n#> \n#> Error: Within\n#>           Df Sum Sq Mean Sq F value Pr(>F)  \n#> IV         3   1800     600   5.357 0.0142 *\n#> Residuals 12   1344     112                 \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"repeated-measures-anova.html","id":"lab-9-generalization-assignment-1","chapter":"21 Repeated Measures ANOVA","heading":"21.6 Lab 9 Generalization Assignment","text":"","code":""},{"path":"repeated-measures-anova.html","id":"instructions-20","chapter":"21 Repeated Measures ANOVA","heading":"21.6.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab9.Rmd”Use Lab9.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 9 blackboard.","code":""},{"path":"repeated-measures-anova.html","id":"problems-19","chapter":"21 Repeated Measures ANOVA","heading":"21.6.2 Problems","text":"Create R script can generate simulated data following repeated measures design. (2 points). dependent variable assumed come normal distribution mean = 0 standard deviation = 1.B. one repeated measures factor 5 levels (Down1, Down2, Control, Up1, Up2). control group assumed effect. Down1 Down2 levels shift mean 1 2 standard deviations, respectively. Up1 Up2 levels shift mean 1 2 standard deviations, respectively.C. 6 subjects experiment, measured condition. 6 subjects assumed different one another (e.g., different baseline means control condition), influenced IV exact way (e.g., interaction).Run simulation determine proportion experiments return significant result design. Assume effect levels IV increments .1 standard deviation, rather increments 1 design.Run simulation determine proportion experiments return significant result design. Assume effect levels IV increments .1 standard deviation, rather increments 1 design.Demonstrate Godden Baddeley example data textbook (19.5), used 2x2 repeated measures design, can analyzed one-sample t-tests return results. Specifically, show one-sample t-tests main effect interaction. (2 points)Demonstrate Godden Baddeley example data textbook (19.5), used 2x2 repeated measures design, can analyzed one-sample t-tests return results. Specifically, show one-sample t-tests main effect interaction. (2 points)Bonus PointsThese bonus questions sphericity concept, involve modifying example data textbook (used concept sectdion 1). data reprinted convenience:Create line plot showing 5 subjects perform across levels IV. Discuss line plot visually shows sphericity problem data (1 point)Create line plot showing 5 subjects perform across levels IV. Discuss line plot visually shows sphericity problem data (1 point)Modify data remove sphericity problem. Specifically, ensure subjects different another (overall means different), IV effect level subject (interaction). , plot new data discuss graph shows sphericity problem removed. (1 point)Modify data remove sphericity problem. Specifically, ensure subjects different another (overall means different), IV effect level subject (interaction). , plot new data discuss graph shows sphericity problem removed. (1 point)Calculate Greenhouse-Geisser estimate epsilon modified data demonstrate removed sphericity problem. (1 point)Calculate Greenhouse-Geisser estimate epsilon modified data demonstrate removed sphericity problem. (1 point)","code":"\nsphericity <- tribble(~S, ~a1, ~a2, ~a3, ~a4,\n        \"s1\",76,64,34,26,\n        \"s2\",60,48,46,30,\n        \"s3\",58,34,32,28,\n        \"s4\",46,46,32,28,\n        \"s5\",30,18,36,28\n        ) "},{"path":"repeated-measures-anova.html","id":"references-8","chapter":"21 Repeated Measures ANOVA","heading":"21.7 References","text":"","code":""},{"path":"nested-designs.html","id":"nested-designs","chapter":"22 Nested Designs","heading":"22 Nested Designs","text":"","code":""},{"path":"nested-designs.html","id":"reading-11","chapter":"22 Nested Designs","heading":"22.1 Reading","text":"Chapters 20 21 Abdi, Edelman, Dowling, & Valentin90.","code":""},{"path":"nested-designs.html","id":"surprise","chapter":"22 Nested Designs","heading":"22.2 Surprise","text":"lab content yet. Instead, generalization problems demonstrate complete textbook examples Chapter 20 21 R.","code":""},{"path":"nested-designs.html","id":"lab-10-generalization-assignment-1","chapter":"22 Nested Designs","heading":"22.3 Lab 10 Generalization Assignment","text":"","code":""},{"path":"nested-designs.html","id":"instructions-21","chapter":"22 Nested Designs","heading":"22.3.1 Instructions","text":"assignment instructions following:Work inside new R project stats II createdCreate new R Markdown document called “Lab10.Rmd”Use Lab10.Rmd show work attempting solve following generalization problems. Commit work regularly appears Github repository.problem, make note much problem believe can solve independently without help. example, needed watch help video unable solve problem without copying answers, note 0. confident can complete problem scratch completely , note 100. OK 0s 100s anything .Submit github repository link Lab 10 blackboard.","code":""},{"path":"nested-designs.html","id":"problems-20","chapter":"22 Nested Designs","heading":"22.4 Problems","text":"Chapter 20, reproduce bat hat example (20.2) R. code represent data long-form, conduct ANOVA, report ANOVA table. know correctly can reproduce ANOVA table textbook. (3 points)Chapter 20, reproduce bat hat example (20.2) R. code represent data long-form, conduct ANOVA, report ANOVA table. know correctly can reproduce ANOVA table textbook. (3 points)Chapter 21, reproduce phonological similarity example (21.2.1) R. code represent data long-form, conduct ANOVA, report ANOVA table. know correctly can reproduce ANOVA table textbook. (3 points) Note, F-value phonological similarity factor find R may textbook. textbook produces quasi-F, OK .Chapter 21, reproduce phonological similarity example (21.2.1) R. code represent data long-form, conduct ANOVA, report ANOVA table. know correctly can reproduce ANOVA table textbook. (3 points) Note, F-value phonological similarity factor find R may textbook. textbook produces quasi-F, OK .Bonus points:Use R produce missing quasi_F value problem 2.","code":""},{"path":"nested-designs.html","id":"references-9","chapter":"22 Nested Designs","heading":"22.5 References","text":"","code":""},{"path":"wyor.html","id":"wyor","chapter":"23 WYOR","heading":"23 WYOR","text":"","code":""},{"path":"wyor.html","id":"reading-12","chapter":"23 WYOR","heading":"23.1 Reading","text":"Chapter 22 Abdi, Edelman, Dowling, & Valentin91.","code":""},{"path":"wyor.html","id":"overview-22","chapter":"23 WYOR","heading":"23.2 Overview","text":"WYOR refers “Writing statistical Recipes”. goal course explore principles statistical analysis point able use general principles craft statistical analyses tailored designs interest. final lab, practical section discusses general aspects R formula declaring ANOVA linear regression models. can help analyze many designs similar discussed class throughout last two semesters. conceptual section final example simulated statistical analysis, parting thoughts.","code":""},{"path":"wyor.html","id":"practical-i-r-formula","chapter":"23 WYOR","heading":"23.3 Practical I: R formula","text":"Throughout course used lm() aov() functions conduct linear regressions ANOVAs. specific designs, also demonstrated use formula syntax declare design interest. However, present general description formula syntax, can used declare many different kinds designs. practical section provides quick look formula syntax. Also, check blog piece writing formulas R alternate resource, https://conjugateprior.org/2013/01/formulae--r-anova/.","code":""},{"path":"wyor.html","id":"read-the-formula-help-file","chapter":"23 WYOR","heading":"23.3.1 Read the formula() help file","text":"turns help file formula syntax actually pretty helpful. , make sure read .","code":"\n?formula()\n?formula"},{"path":"wyor.html","id":"formula-basics","chapter":"23 WYOR","heading":"23.3.2 Formula basics","text":"normally see formula inside aov lm, follows. column name dependent variable interest (DV) located left, followed tilda (~), followed independent variable(s) interest, pointer data frame.Formulas can also declared outside functions. example, can assign formula named object., can see class object “formula”.Entering name object alone print formula consoleThere also helper functions formula objects allow inspect individual terms formula.assigned formula object, can use object place formula aov() lm().","code":"\naov(DV ~ IV, data = your_data)\nlm(DV ~ IV, data = your_data)\nmy_formula <- DV~IV\nclass(my_formula)\n#> [1] \"formula\"\nmy_formula\n#> DV ~ IV\nterms(my_formula)\n#> DV ~ IV\n#> attr(,\"variables\")\n#> list(DV, IV)\n#> attr(,\"factors\")\n#>    IV\n#> DV  0\n#> IV  1\n#> attr(,\"term.labels\")\n#> [1] \"IV\"\n#> attr(,\"order\")\n#> [1] 1\n#> attr(,\"intercept\")\n#> [1] 1\n#> attr(,\"response\")\n#> [1] 1\n#> attr(,\".Environment\")\n#> <environment: R_GlobalEnv>\nlibrary(tibble)\nsome_data <- tibble(DV=rnorm(20,0,1),\n                    IV=rep(c(\"A\",\"B\"), each=10))\n\nsummary(aov(DV ~ IV, some_data))\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> IV           1   0.00  0.0004       0  0.983\n#> Residuals   18  16.07  0.8926\n\n# is the same as\n\nmy_formula <- DV ~ IV\nsummary(aov(my_formula, some_data))\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> IV           1   0.00  0.0004       0  0.983\n#> Residuals   18  16.07  0.8926"},{"path":"wyor.html","id":"formula-operators","chapter":"23 WYOR","heading":"23.3.3 Formula operators","text":"several formula operators aware , including ~, +, :, *, ^, %%, /, -.tilda ~ operator used separately declare dependent variable “model”, set independent variables used account variation dependent variable.plus + operator used “add” specific terms model. example, one, two, three independent variables, different models contain one, two, three variables.Importantly, + sign adds individual terms nothing . example, factorial design three independent variables (, B, C), several two-way interactions three-way interaction. However, using plus operator, interaction terms added unless explicitly declared.example, can inspect terms formula DV ~ +B+C, see three terms, one independent variable.Interaction terms can declared using :. example, :B specify interaction B. Using + operator, can add individual interaction terms declared using : operator.* operator signifies crossing factors. example, 2x2 design, two levels fully crossed two levels B. design two main effects (B), one interaction term (:B). * operator shortcut include terms (main effects interactions) crossed design, without specify individually.- operator can subtract terms model. example, let’s say design three crossed factors, want include 3-way interaction:%% operator used declare nesting, %%B, term left nested term right. / operator can also used indicate nesting.","code":"\n\n# examples of DV ~ model\n\n# one factor\nDV ~ A\n\n# two factors no interaction\nDV ~ A+B\n\n# three factors, all possible interactions\nDV ~ A*B*C\nDV ~ A\nDV ~ A+B\nDV ~ A+B+C\nattributes(terms(DV ~ A+B+C))$factors\n#>    A B C\n#> DV 0 0 0\n#> A  1 0 0\n#> B  0 1 0\n#> C  0 0 1\n## add the two-interaction between A and B\nDV ~ A + B + C + A:B\n#> DV ~ A + B + C + A:B\n\nattributes(terms(DV ~ A + B + C + A:B))$factors\n#>    A B C A:B\n#> DV 0 0 0   0\n#> A  1 0 0   1\n#> B  0 1 0   1\n#> C  0 0 1   0\n\n## add all two-way interactions\n\nDV ~ A + B + C + A:B + A:C + B:C\n#> DV ~ A + B + C + A:B + A:C + B:C\n\nattributes(terms(DV ~ A + B + C + A:B + A:C + B:C))$factors\n#>    A B C A:B A:C B:C\n#> DV 0 0 0   0   0   0\n#> A  1 0 0   1   1   0\n#> B  0 1 0   1   0   1\n#> C  0 0 1   0   1   1\n\n## add all interactions\n\nDV ~ A + B + C + A:B + A:C + B:C + A:B:C\n#> DV ~ A + B + C + A:B + A:C + B:C + A:B:C\n\nattributes(terms(DV ~ A + B + C + A:B + A:C + B:C + A:B:C))$factors\n#>    A B C A:B A:C B:C A:B:C\n#> DV 0 0 0   0   0   0     0\n#> A  1 0 0   1   1   0     1\n#> B  0 1 0   1   0   1     1\n#> C  0 0 1   0   1   1     1\nDV ~ A*B\n#> DV ~ A * B\nattributes(terms(DV ~ A*B))$factors\n#>    A B A:B\n#> DV 0 0   0\n#> A  1 0   1\n#> B  0 1   1\n\n# is the same as\nDV ~ A + B + A:B\n#> DV ~ A + B + A:B\n\n# 3-factor crossed\n\nDV ~ A*B*C \n#> DV ~ A * B * C\nattributes(terms(DV ~ A*B*C))$factors\n#>    A B C A:B A:C B:C A:B:C\n#> DV 0 0 0   0   0   0     0\n#> A  1 0 0   1   1   0     1\n#> B  0 1 0   1   0   1     1\n#> C  0 0 1   0   1   1     1\n\n#is the same as \n\nDV ~ A + B + C + A:B + A:C + B:C + A:B:C\n#> DV ~ A + B + C + A:B + A:C + B:C + A:B:C\n# omit the three-way interaction\nDV ~ A*B*C - A:B:C\n#> DV ~ A * B * C - A:B:C\nattributes(terms(DV ~ A*B*C - A:B:C))$factors\n#>    A B C A:B A:C B:C\n#> DV 0 0 0   0   0   0\n#> A  1 0 0   1   1   0\n#> B  0 1 0   1   0   1\n#> C  0 0 1   0   1   1\n\n# omit the two-way interactions\nDV ~ A*B*C - A:B - A:C - B:C\n#> DV ~ A * B * C - A:B - A:C - B:C\nDV ~ A + A%in%B\n#> DV ~ A + A %in% B\nattributes(terms(DV ~ A + A%in%B))$factors\n#>    A A:B\n#> DV 0   0\n#> A  1   2\n#> B  0   1\n\nDV ~ A+ A/B\n#> DV ~ A + A/B\nattributes(terms(DV ~ A + A/B))$factors\n#>    A A:B\n#> DV 0   0\n#> A  1   2\n#> B  0   1"},{"path":"wyor.html","id":"error","chapter":"23 WYOR","heading":"23.3.4 Error()","text":"formulas apply -subjects variables, “fixed” effects. also possible include random effects, however assumed designs balanced. Error() term used “specify error strata” (according help file aov). used Error() class designs repeated measures. examples:","code":""},{"path":"wyor.html","id":"one-random-factor","chapter":"23 WYOR","heading":"23.3.4.1 One Random factor","text":"","code":"\nDV ~ Error(A)\n#> DV ~ Error(A)"},{"path":"wyor.html","id":"one-factor-with-repeated-measures","chapter":"23 WYOR","heading":"23.3.4.2 One Factor, with repeated Measures","text":"Note, subject refers column name coding subject variable data frame.","code":"\nDV ~ A + Error(subject)\n#> DV ~ A + Error(subject)"},{"path":"wyor.html","id":"two-factor-both-repeated-measures","chapter":"23 WYOR","heading":"23.3.4.3 Two factor, both repeated measures","text":"","code":"\nDV ~ A*B + Error(subject/(A*B))\n#> DV ~ A * B + Error(subject/(A * B))"},{"path":"wyor.html","id":"two-factor-a-between-fixed-b-repeated-measures","chapter":"23 WYOR","heading":"23.3.4.4 Two factor, A between fixed, B repeated measures","text":"","code":"\nDV ~ A*B + Error(subject/B)\n#> DV ~ A * B + Error(subject/B)"},{"path":"wyor.html","id":"conclusionwhen-in-doubt-reproduce-a-textbook-example","chapter":"23 WYOR","heading":"23.3.5 Conclusion…when in doubt, reproduce a textbook example","text":"Specifying formula base R design interest can challenging, especially designs become complicated. unbalanced designs multiple IVs, combinations fixed random nested repeated measures, work . example, may learn different R packages mixed models lme4 nlme, along slightly different formula syntax use. extent possible, using unfamiliar statistics software packages, can helpful find textbook example can trust (e.g., fully worked example analysis design interest), attempt reproduce example using software way confirm expected behavior.","code":""},{"path":"wyor.html","id":"conceptual-i-simulating-statistics","chapter":"23 WYOR","heading":"23.4 Conceptual I: Simulating statistics","text":"Throughout course conducted several statistical simulations, including simulations power analysis. simulations several purposes. First, give experience general coding R (e.g., run loops stored data simulations). Second, concretely illustrate concepts like random sampling sampling distributions show simulated results converge analytic procedures long run. Last, flexible can used anticipate possible outcomes experimental designs, well develop inferential models interpret possible outcomes.","code":""},{"path":"wyor.html","id":"basic-recipe-for-simulating-designs-in-r","chapter":"23 WYOR","heading":"23.4.1 Basic recipe for simulating designs in R","text":"Declare data frame represents structure design, smallest detail interested . example, data frame include columns dependent variable independent variables design. row individual subject mean.Make explicit assumptions distributions underlying measurements. Populate DV values sampled distributions.null model assumes distribution DV change across conditions/levels IV. Alternate models assume distributions conditions/levels different somehow. get choose kind simulation running, choose specify form differences distributions.populated data frame possible data, can analyse simulated data answer question interest. involve applying standard inferential test, generate test-statistic (e..g, t, F, r, etc), statistic interest (e.g., mean difference).Repeat process randomly generating simulated data, analyzing arrive test-statistics interest. Repeat roughly 10,000 times, save test-statistics every time produce simulated sampling distribution test-statistic.Use simulated sampling distribution inference power analysis design planning.","code":""},{"path":"wyor.html","id":"a-conundrum-concrete-vs.-abstract-test-statistics","chapter":"23 WYOR","heading":"23.4.2 A conundrum: Concrete vs. abstract test-statistics","text":"Many test-statistics, like z, t, F, r, somewhat abstract opaque. example, F-value 3, large small? mean care difference two means? depends factors like degrees freedom etc. Alternatively, sometimes test-statistics concrete. example, told one population mean height 5 ft, another mean height 6ft, mean difference 1ft. familiar feet measure height, 1 ft difference fairly concrete , relative F value, fairly immediate interpret.Using simulation techniques possible evaluate “concrete” statistics. example, consider simple 1 factor design (-subjects). purpose design simply determine difference means group group B.simulation conducts experiment 10,000. simulation null hypothesis, measures randomly drawn normal distribution mean = 0 sd = 1. create several simulated sampling distributions various test-statistics illustrate simulation procedure flexible. , look whether null-hypotheses agree.now created separate null-distributions statistics saved. null-distribution 10,000 values.Consider simulated F-distribution created. Let’s get critical value simulated distribution, compare known critical value F.Now let’s look instead sampling distribution mean difference group B.Let’s convert distribution absolute value mean difference, find critical value (assuming alpha =.05) associated distribution.","code":"\nlibrary(tibble)\nlibrary(dplyr)\n\n# declare simulation paramters\nN <- 10\neffect_size <- 0\nnull_distribution <- tibble()\niterations <- 10000\n\n# run simulation\nfor(i in 1:iterations){\n  \n  # create a random sample of data\n  null_data <- tibble(subjects = as.factor(c(1:(N*2))),\n                      IV = as.factor(rep(c(\"A\",\"B\"), each = N)),\n                      DV = c(rnorm(N,0,1),rnorm(N,effect_size,1))\n                      )\n  \n  # run statistical analyses\n  aov_summary <- summary(aov(DV~IV, data = null_data))\n  \n  SS_IV <- aov_summary[[1]]$`Sum Sq`[1]\n  SS_residuals <- aov_summary[[1]]$`Sum Sq`[2]\n  SS_total <- SS_IV+SS_residuals\n  MS_IV <- aov_summary[[1]]$`Mean Sq`[1]\n  MS_residuals <- aov_summary[[1]]$`Mean Sq`[2]\n  F_val <- aov_summary[[1]]$`F value`[1] \n  \n  means <- null_data %>%\n    group_by(IV) %>%\n    summarize(meanDV = mean(DV),\n              sdDV = sd(DV))\n  \n  mean_difference <- means[means$IV == \"B\",]$meanDV - means[means$IV == \"A\",]$meanDV \n  \n  abs_mean_diff <- abs(mean_difference)\n  \n  cohens_D <- mean_difference / sqrt((sum(means$sdDV)/2))\n  \n  t <- t.test(DV~IV, var.equal=TRUE, null_data)$statistic\n  \n  abs_t <- abs(t)\n  \n  # save all test-statistics\n  sim_vals <- tibble(SS_IV,\n                    SS_residuals,\n                    SS_total,\n                    MS_IV,\n                    MS_residuals,\n                    F_val,\n                    mean_difference,\n                    abs_mean_diff,\n                    cohens_D,\n                    t,\n                    abs_t\n                    )\n  \n  # append saved test-statsistics to the null-distribution tibble\n  null_distribution <- rbind(null_distribution,\n                             sim_vals)\n}\n# write a general function to get critical values from\n# a simulated distribution (vector of values)\n\nget_critical_value <- function(x,alpha){\n  x_sorted <- sort(abs(x))\n  ind <- round(length(x)*(1-alpha))\n  return(x_sorted[ind])\n}\n\n# simulated F critical value\nget_critical_value(null_distribution$F_val,\n                   alpha = .05)\n#> [1] 4.581539\n\n# analytical F critical value\nqf(.95,1,18)\n#> [1] 4.413873\n\nlibrary(ggplot2)\n\nggplot(null_distribution, aes(x=F_val))+\n  geom_histogram(bins=100)+\n  geom_vline(xintercept=get_critical_value(null_distribution$F_val,\n                   alpha = .05))\nggplot(null_distribution, aes(x=mean_difference))+\n  geom_histogram(bins=100)\nget_critical_value(null_distribution$abs_mean_diff,\n                   alpha = .05)\n#> [1] 0.8843424\n\nggplot(null_distribution, aes(x=abs_mean_diff))+\n  geom_histogram(bins=100)+\n  geom_vline(xintercept=get_critical_value(null_distribution$abs_mean_diff,\n                   alpha = .05))"},{"path":"wyor.html","id":"which-null-is-the-true-null","chapter":"23 WYOR","heading":"23.4.3 Which null is the true null?","text":"created several true null distributions. looked values F can produced chance design. well, looked absolute mean differences can produced chance design. found critical values test-statistics. use either purposes “null-hypothesis testing”.However, conundrum alluded earlier different Null distributions don’t necessarilly agree. particular case, agree fairly closely, perfectly.example, use correlation coefficienty quickly look check whether F values related absolute mean differences across simulations.table shows entire correlation matrix. can see large positive correlation F absolute mean difference. words, large differences means produced chance large F values also produced.However, two vectors perfectly correlated, different opinions constitutes type error. issue easily inspected , shows scatterplot simulated F values simulated absolute mean differences, along critical values .red points simulations exceed critical values (significant measures, represent similar conclusions type errors look like), note group less 5% simulations","code":"\nknitr::kable(round(cor(null_distribution),digits=2))\ntest <- null_distribution %>%\n  mutate(significant = case_when(\n    abs_mean_diff > .88 & F_val > 4.4 ~ \"both\",\n    abs_mean_diff > .88 & F_val < 4.4 ~ \"Mean Difference\",\n    abs_mean_diff < .88 & F_val > 4.4 ~ \"F\"\n  ))\n\nggplot(test, aes(x=abs_mean_diff,\n                 y=F_val,\n                 color=significant))+\n  geom_point()+\n  geom_vline(xintercept=get_critical_value(null_distribution$abs_mean_diff,\n                   alpha = .05))+\n  geom_hline(yintercept = get_critical_value(null_distribution$F_val,\n                   alpha = .05))\ntest %>%\n  group_by(significant) %>%\n  summarize(counts= n(),\n            proportion = n()/10000)\n#> # A tibble: 4 × 3\n#>   significant     counts proportion\n#>   <chr>            <int>      <dbl>\n#> 1 both               374     0.0374\n#> 2 F                  168     0.0168\n#> 3 Mean Difference    135     0.0135\n#> 4 <NA>              9323     0.932"},{"path":"wyor.html","id":"references-10","chapter":"23 WYOR","heading":"23.5 References","text":"","code":""},{"path":"semester-project-2021.html","id":"semester-project-2021","chapter":"24 Semester Project 2021","heading":"24 Semester Project 2021","text":"","code":""},{"path":"semester-project-2021.html","id":"overview-write-your-own-lab-chapter","chapter":"24 Semester Project 2021","heading":"24.1 Overview: Write your own lab chapter","text":"basic project semester write lab chapter similar ones writing course. example, lab chapter include minimum, concept section, practical section, generalization problem section. permission, student authored course material compiled web-book.","code":""},{"path":"semester-project-2021.html","id":"motivation-for-the-semester-long-project","chapter":"24 Semester Project 2021","heading":"24.2 Motivation for the semester long project","text":"explain assignment detail, like motivate reasons think assignment worth . See also short opinion piece92 Nature Human Behaviour value building shareable portfolios work.","code":""},{"path":"semester-project-2021.html","id":"dependence-and-independence","chapter":"24 Semester Project 2021","heading":"24.2.1 Dependence and Independence","text":"become adept statistics useful assess abilities independent statistical thinker. weekly lab assignments well-suited assessing independence problems solution videos can use solve problems. , one purpose semester long assignment give larger, open-ended problems solve independently. words, exercise help develop independent statistical thinking abilities.","code":""},{"path":"semester-project-2021.html","id":"learning-by-teaching","chapter":"24 Semester Project 2021","heading":"24.2.2 Learning by teaching","text":"remember taking statistics classes throughout undergraduate graduate school. even used statistics time. , ’ll admit, don’t think really started learning developing statistical thinking abilities teach people statistics. , way course completely backwards. teaching statistics.Even though taught many statistics classes, writing lab manual chapter course, week also learning new things statistics, act producing tutorial content helping improve statistical thinking., think experience learning teaching . , expect process writing lab tutorial hone statistical thinking.","code":""},{"path":"semester-project-2021.html","id":"assignment-as-product","chapter":"24 Semester Project 2021","heading":"24.2.3 Assignment as product","text":"Wouldn’t nice assignments asked complete student actually somehow useful life beyond getting grade course. example, art class make painting part assignment…, get grade, put painting portfolio, show others, put wall, sell , sell prints . work inspire work, can display people show capable . think assignments like , can produce work proud use show people capable ., complete semester long-project, writing chapter book, book shared internet part course material. can use like painting show people evidence kind work can .","code":""},{"path":"semester-project-2021.html","id":"contribute-to-open-educational-resources","chapter":"24 Semester Project 2021","heading":"24.2.4 Contribute to open-educational resources","text":"lab course open-educational resource developing sharing anyone interested using materials. hope materials manual useful students, can potentially improved anyone willing contribute time creating improving content. completing assignment, make contributions author improving course. Note, option contributing course materials; example, want share assignment part course totally OK, sharing requirement assignment.materials licensed creative commons license CC SA 4.0. means people can following:Share: copy redistribute material medium formatShare: copy redistribute material medium formatAdapt: remix, transform, build upon material\npurpose, even commercially.Adapt: remix, transform, build upon material\npurpose, even commercially., also give attribution (must give appropriate credit, provide link license, indicate changes made); , ShareAlike (remix, transform, build upon material, must distribute contributions license original).","code":""},{"path":"semester-project-2021.html","id":"instructions-22","chapter":"24 Semester Project 2021","heading":"24.3 Instructions","text":"semester long project worth total 28 points toward final grade. split 4 units. first 21 points section tutorial, final 7 points participating peer review workshopping others materials.Concept Section (7 points)Practical Section (7 points)Generalization Problem (7 points)Peer review workshopping (7 points)","code":""},{"path":"semester-project-2021.html","id":"examples-2","chapter":"24 Semester Project 2021","heading":"24.3.1 Examples","text":"weekly materials lab course, semester last, serve examples kind content asking generate. labs written three major components, concept section, practical section, generalization problem end. task write content three sections. can write three sections topic, want mix match, can .","code":""},{"path":"semester-project-2021.html","id":"flexibility","chapter":"24 Semester Project 2021","heading":"24.3.2 Flexibility","text":"lot flexibility assignment. want create tutorial materials related content course, helpful (e.g., use opportunity improve skills area want improve). , may choose focus topic covered lab; example, maybe want re-write ANOVA lab, t-test lab last semester. Maybe want write concept section interactions factorial designs, practical section linear regression, generalization problem data-simulation. fine . , perhaps want write tutorial topic didn’t cover class, wish cover; example, different statistical test approach. write tutorial R package (e.g., plots ggplot, manipulate data dplyr, even R package haven’t discussed).","code":""},{"path":"semester-project-2021.html","id":"components","chapter":"24 Semester Project 2021","heading":"24.3.3 Components","text":"guidelines suggestion writing component sections. Overall, use existing lab manual content general guide materials look like. specific length requirements semester long project, long enough accomplish goals writing helpful tutorial. Consider also might want share work later example portfolio, make good something want share others.","code":""},{"path":"semester-project-2021.html","id":"concept-section","chapter":"24 Semester Project 2021","heading":"24.3.3.1 Concept section","text":"Concept sections accomplish two goals:Identify discuss statistical conceptUse R code illustrate implement conceptFor example, important statistical concept central limit theorem, states sampling distribution mean approximately normal long run. write concept section topic, expect discussion ideas implications concepts behind central limit theorem, , R code demonstrates concept. tried wrote topic last semester https://crumplab.github.io/psyc7709Lab/articles/Stats1/Lab8_Normal.html#conceptual--central-limit-theorem-1.","code":""},{"path":"semester-project-2021.html","id":"practical-section","chapter":"24 Semester Project 2021","heading":"24.3.3.2 Practical Section","text":"Practical sections provide working examples accomplish applied goal statistics. example, conduct t-test using R, report data linear regression using papaja. practical examples written components R code snippets help explain practical goal describing tutorial","code":""},{"path":"semester-project-2021.html","id":"generalization-problem-section","chapter":"24 Semester Project 2021","heading":"24.3.3.3 Generalization problem section","text":"completed many generalization problems assigned previous labs. assigned problems way assess statistical thinking skills abilities. idea : understand concepts skills ’ve discussing lab lecture, able solve problems. also another idea: process solving problems help understand concepts skills ’ve discussing lab lecture.’ve attempted come worthwhile problems helpful practice. Now turn. generalization problem following components:description problem solved provides enough guidance reader able understand problem assignedAn example solution problem (e..g, write solution video show problem solved).","code":""},{"path":"semester-project-2021.html","id":"submission","chapter":"24 Semester Project 2021","heading":"24.4 Submission","text":"Write semester long project .Rmd file, submit (link github), blackboard assignment due date.","code":""},{"path":"semester-project-2021.html","id":"important-dates","chapter":"24 Semester Project 2021","heading":"24.5 Important Dates","text":"Th 25th Mar - Propose topics peer review/workshoppingTh 22nd Apr - peer review workshoppingFINAL DUE DATE:semester long project due Monday, May 24th.","code":""},{"path":"semester-project-2021.html","id":"references-11","chapter":"24 Semester Project 2021","heading":"24.6 References","text":"","code":""},{"path":"semester-project-2022.html","id":"semester-project-2022","chapter":"25 Semester Project 2022","heading":"25 Semester Project 2022","text":"","code":""},{"path":"semester-project-2022.html","id":"overview-23","chapter":"25 Semester Project 2022","heading":"25.1 Overview:","text":"developing discussing semester long project throughout semester. , add detail instructions project page.major component using R package pkgdown create website () act portfolio communicating displaying work semester.Last semester completed lab work .Rmd documents, uploaded Github Repository, submitted link repository Blackboard. semester similar, except using R package project structure, saving lab work specific folder called vignettes, compiling work present form website.now, watch getting starting video review process. go details many times class well.","code":""},{"path":"semester-project-2022.html","id":"example-semester-project","chapter":"25 Semester Project 2022","heading":"25.2 Example Semester Project","text":"building semester project throughout semester. code-base can act working example code . semester project located :https://github.com/CrumpLab/SemesterProject7709","code":""},{"path":"semester-project-2022.html","id":"r-basics-tab","chapter":"25 Semester Project 2022","heading":"25.3 R Basics tab","text":"purpose component semester long project demonstrate basic command fundamental R programming skills. Consider litmus test . can confidently provide working code item, also creating resource future self case forget elements R programming work. find struggling produce working code item, identified basic skills practice semester.semi open ended component. provide following list basic R skills. task write working code demonstrating usage skills. demonstrate understand skills. provide example, refer reference code tab course website. , written working code demonstrate basic operations R. One way complete component, make similar reference page showing working code following items.Demonstrate understanding ability use:Different object types, including vectors data.frames. Also, demonstrate knowledge least two object types vectors data.frames.Different object types, including vectors data.frames. Also, demonstrate knowledge least two object types vectors data.frames.Loops. multiple ways iterate R. Demonstrate least two ways, including loop. Show know break loop. Highlight one thing loops R obvious , think might obvious someone else.Loops. multiple ways iterate R. Demonstrate least two ways, including loop. Show know break loop. Highlight one thing loops R obvious , think might obvious someone else.Logic. Show understand logical operations R. Show know use logical operations indexing. example, use logical operations index vector, index data.frame.Logic. Show understand logical operations R. Show know use logical operations indexing. example, use logical operations index vector, index data.frame.Loops logic. Demonstrate can combine loops logic solve problem. example, use loops logic solve fizz buzz problem.Loops logic. Demonstrate can combine loops logic solve problem. example, use loops logic solve fizz buzz problem.Pick least one advanced problem list problems, show can solve R.Pick least one advanced problem list problems, show can solve R.https://crumplab.github.io/programmingforpsych/programming-challenges--learning--fundamentals.htmlFor maximum fun, pick snakes ladders problem. know can .Write function, include R package, show works. FYI, help one. writing new function, putting R folder (R package folder structure). make documentation function (show ). build R package, make R function available instance R. means, can load package library (.rmd writing answers ), can show can use R function right .Write function, include R package, show works. FYI, help one. writing new function, putting R folder (R package folder structure). make documentation function (show ). build R package, make R function available instance R. means, can load package library (.rmd writing answers ), can show can use R function right .one thing R want learn (refresh memory, clarify , whatever), think future self thank making working code can consult later . , wait (course ), thank previous self fine job.one thing R want learn (refresh memory, clarify , whatever), think future self thank making working code can consult later . , wait (course ), thank previous self fine job.","code":""},{"path":"semester-project-2022.html","id":"slide-decks","chapter":"25 Semester Project 2022","heading":"25.4 Slide Decks","text":"possible create slide decks using Rmd documents. Multiple output formats supported including, pdf, html, powerpoint. See page information https://rmarkdown.rstudio.com/lesson-11.html.complete portion semester long project :Create slide deck using one methodsSave slide deck folder called “slides” project folderCreate new tab website, link html slide show. NOTE: make sure final product gets saved docs folder .","code":""},{"path":"references-12.html","id":"references-12","chapter":"References","heading":"References","text":"","code":""}]
